{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2669d8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import math\n",
    "import sys \n",
    "import shutil\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from datetime import datetime\n",
    "from pandas import read_csv\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ShuffleSplit # or StratifiedShuffleSplit\n",
    "from pandas import DataFrame\n",
    "from pandas import DataFrame\n",
    "import sktime\n",
    "from sktime.classification.distance_based import KNeighborsTimeSeriesClassifier\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "from mpl_toolkits import mplot3d\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef988fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data location\n",
    "pathON=\"C:/Users/pierr/Documents_kanop/Shps/Pierre_points_for_louis/Bureau/interpo_ONOFF/ON_data/\"\n",
    "pathOFF=\"C:/Users/pierr/Documents_kanop/Shps/Pierre_points_for_louis/Bureau/interpo_ONOFF/OFF_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d02351a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing in a list the adresses of all the data files in the notebook\n",
    "\n",
    "data_files_ON=[]\n",
    "file_names_ON=[]\n",
    "\n",
    "data_files_OFF=[]\n",
    "file_names_OFF=[]\n",
    "data_files_ALL=[]\n",
    "file_names_ALL=[]\n",
    "\n",
    "#Storing in a list the adresses of all the data files in the notebook\n",
    "for filename in os.listdir(pathOFF):\n",
    "    f = os.path.join(pathOFF,filename)\n",
    "    if os.path.isfile(f):\n",
    "        data_files_OFF.append(f)\n",
    "        file_names_OFF.append(filename)\n",
    "        data_files_ALL.append(f)\n",
    "        file_names_ALL.append(filename)\n",
    "for filename in os.listdir(pathON):\n",
    "    f = os.path.join(pathON,filename)\n",
    "    if os.path.isfile(f):\n",
    "        data_files_ON.append(f)\n",
    "        file_names_ON.append(filename)\n",
    "        data_files_ALL.append(f)\n",
    "        file_names_ALL.append(filename)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6274a0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using data_files with only the 36 binning\n",
    "Filter=True\n",
    "binning=37\n",
    "\n",
    "\n",
    "if Filter==True:\n",
    "    \n",
    "    idx_OFF=[]\n",
    "    idx_ON=[]\n",
    "    dataON=[]\n",
    "    dataOFF=[]\n",
    "    for i in range(len(data_files_OFF)):\n",
    "        dataframe = pd.read_csv(data_files_OFF[i])\n",
    "        lg = len(dataframe)\n",
    "        \n",
    "        if lg==binning:\n",
    "            idx_OFF.append(i)\n",
    "    \n",
    "    for i in range(len(data_files_ON)):\n",
    "        dataframe = pd.read_csv(data_files_ON[i])\n",
    "        lg = len(dataframe)\n",
    "        if lg==binning:\n",
    "            idx_ON.append(i)\n",
    "\n",
    "\n",
    "    for i in range(len(idx_OFF)):\n",
    "\n",
    "        a=idx_OFF[i]\n",
    "        dataOFF.append(data_files_OFF[a])\n",
    "    for i in range(len(idx_ON)):\n",
    "\n",
    "        a=idx_ON[i]\n",
    "        dataON.append(data_files_ON[a])\n",
    "\n",
    "idx = idx_OFF+idx_ON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5aef3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to obtain my custom features\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "def peak_study(array,time,delta_array):\n",
    "    \n",
    "    indices = find_peaks(array)\n",
    "    indices=np.delete(indices,-1)\n",
    "    y=[array[j] for j in indices]\n",
    "    x=[time[j] for j in indices]\n",
    "    y_err=[delta_array[j] for j in indices]\n",
    "    \n",
    "    x=np.hstack(x)\n",
    "    y=np.hstack(y)\n",
    "    y_err=np.hstack(y_err)\n",
    "    \n",
    "    copy_flux=y.copy()\n",
    "    copy_time=x.copy()\n",
    "    a=np.argmax(copy_flux)\n",
    "    ymax1=copy_flux[a]\n",
    "    tmax1=time[a]\n",
    "    \n",
    "    copy_time=np.delete(copy_time,a)\n",
    "    copy_flux=np.delete(copy_flux,a)\n",
    "    a=np.argmax(copy_flux)\n",
    "    ymax2=copy_flux[a]\n",
    "    tmax2=copy_time[a]\n",
    "    \n",
    "    one_to_second_flux=abs(ymax1-ymax2)\n",
    "    one_to_second_time=abs(tmax1-tmax2)\n",
    "\n",
    "    \n",
    "    min_max_peak= abs(max(y)-min(y))\n",
    "    arr=[1,-1]\n",
    "    for i in range(len(y)):\n",
    "        if np.isnan(y[i])==False:\n",
    "            error=y_err[i]\n",
    "            choice=np.random.choice(arr,1)\n",
    "            if choice ==1:\n",
    "                y[i]=y[i]+error\n",
    "            if choice==-1:\n",
    "                y[i]=y[i]-error\n",
    "                \n",
    "    peak_magnitudes=y\n",
    "    nb_peaks=len(y)\n",
    "    delta_energy_arr=[]\n",
    "    delta_time_arr=[]\n",
    "    \n",
    "    for i in range(nb_peaks-1):\n",
    "        \n",
    "        delta = x[i+1]-x[i]\n",
    "        delta_energy=abs(y[i+1]-y[i])\n",
    "        delta_time_arr.append(delta)\n",
    "        delta_energy_arr.append(delta_energy)\n",
    "\n",
    "        \n",
    "    NG_diff_mean_peaks=np.mean(delta_energy_arr)\n",
    "    max_diff_mean_peaks=max(y)-np.mean(y)\n",
    "    min_diff_mean_peaks=min(y)-np.mean(y)\n",
    "    peaks_time_delay=np.mean(delta_time_arr)\n",
    "    std_peaks=np.std(y)\n",
    "    mean_peaks=np.mean(y)\n",
    "    var_peaks=np.var(y)\n",
    "    mean_arr=np.mean(array)\n",
    "    maxi=max(array)\n",
    "    mini=min(array)\n",
    "    amplitude=max(array)-min(array)\n",
    "    std=np.std(array)\n",
    "    return nb_peaks,peak_magnitudes,peaks_time_delay,std_peaks,mean_peaks,NG_diff_mean_peaks,max_diff_mean_peaks,min_diff_mean_peaks,mean_arr,maxi,mini,amplitude,std,one_to_second_flux,one_to_second_time,min_max_peak\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7ed82f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Machine learning section and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a625d7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgON=len(dataON)\n",
    "lgOFF=len(dataOFF)\n",
    "lg=lgON+lgOFF\n",
    "\n",
    "Labels=[]\n",
    "\n",
    "\n",
    "\n",
    "nb_peaks_arr_flux =[]\n",
    "peak_magnitudes_arr_flux =[]\n",
    "peaks_time_delay_arr_flux =[]\n",
    "std_peaks_arr_flux =[]\n",
    "mean_peaks_arr_flux =[]\n",
    "\n",
    "NG_diff_mean_peaks_arr_flux =[]\n",
    "max_diff_mean_peaks_arr_flux =[]\n",
    "min_diff_mean_peaks_arr_flux =[]\n",
    "mean_arr_flux=[]\n",
    "max_arr_flux=[]\n",
    "min_arr_flux=[]\n",
    "amplitude_flux=[]\n",
    "std_flux=[]\n",
    "one_two_flux=[]\n",
    "one_two_time=[]\n",
    "min_max=[]\n",
    "for i in range(len(dataOFF)):\n",
    "    \n",
    "    dataframe=pd.read_csv(dataOFF[i],index_col=[0])\n",
    "    dataframe.columns=['MJD','Flux','Delta Flux','Photon Index','Delta Index','TS','fratio']\n",
    "    \n",
    "    mjd=np.array(dataframe['MJD'])\n",
    "    flux=np.array(dataframe['Flux'])\n",
    "    photon_idx=np.array(dataframe['Photon Index'])\n",
    "    delta_index=np.array(dataframe['Delta Index'])\n",
    "    delta_flux=np.array(dataframe['Delta Flux'])\n",
    "    \n",
    "\n",
    "    nb_peaks,peak_magnitudes,peaks_time_delay,std_peaks,mean_peaks,NG_diff_mean_peaks,max_diff_mean_peaks,min_diff_mean_peaks,mean_arr,maxi,mini,amplitude,std,one_to_second_flux,one_to_second_time,min_max_peak=peak_study(flux,mjd,delta_flux)\n",
    "                                                                                                                                           \n",
    "    nb_peaks_arr_flux.append(nb_peaks)\n",
    "    peak_magnitudes_arr_flux.append(peak_magnitudes)\n",
    "    peaks_time_delay_arr_flux.append(peaks_time_delay)\n",
    "    std_peaks_arr_flux.append(std_peaks)\n",
    "    mean_peaks_arr_flux.append(mean_peaks)\n",
    "\n",
    "    NG_diff_mean_peaks_arr_flux.append(NG_diff_mean_peaks)\n",
    "    max_diff_mean_peaks_arr_flux.append(max_diff_mean_peaks)\n",
    "    min_diff_mean_peaks_arr_flux.append(min_diff_mean_peaks)\n",
    "    \n",
    "    mean_arr_flux.append(mean_arr)\n",
    "    max_arr_flux.append(maxi)\n",
    "    min_arr_flux.append(mini)\n",
    "    amplitude_flux.append(amplitude)\n",
    "    std_flux.append(std)\n",
    "    one_two_flux.append(one_to_second_flux)\n",
    "    one_two_time.append(one_to_second_time)\n",
    "    min_max.append(min_max_peak)\n",
    "for i in range(len(dataON)):\n",
    "    \n",
    "    dataframe=pd.read_csv(dataON[i],index_col=[0])\n",
    "    dataframe.columns=['MJD','Flux','Delta Flux','Photon Index','Delta Index','TS','fratio']\n",
    "    \n",
    "    mjd=np.array(dataframe['MJD'])\n",
    "    flux=np.array(dataframe['Flux'])\n",
    "    photon_idx=np.array(dataframe['Photon Index'])\n",
    "    delta_index=np.array(dataframe['Delta Index'])\n",
    "    delta_flux=np.array(dataframe['Delta Flux'])\n",
    "\n",
    "    nb_peaks,peak_magnitudes,peaks_time_delay,std_peaks,mean_peaks,NG_diff_mean_peaks,max_diff_mean_peaks,min_diff_mean_peaks,mean_arr,maxi,mini,amplitude,std,one_to_second_flux,one_to_second_time,min_max_peak=peak_study(flux,mjd,delta_flux)\n",
    "    nb_peaks_arr_flux.append(nb_peaks)\n",
    "    peak_magnitudes_arr_flux.append(peak_magnitudes)\n",
    "    peaks_time_delay_arr_flux.append(peaks_time_delay)\n",
    "    std_peaks_arr_flux.append(std_peaks)\n",
    "    mean_peaks_arr_flux.append(mean_peaks)\n",
    "\n",
    "    NG_diff_mean_peaks_arr_flux.append(NG_diff_mean_peaks)\n",
    "    max_diff_mean_peaks_arr_flux.append(max_diff_mean_peaks)\n",
    "    min_diff_mean_peaks_arr_flux.append(min_diff_mean_peaks)\n",
    "    \n",
    "    mean_arr_flux.append(mean_arr)\n",
    "    max_arr_flux.append(maxi)\n",
    "    min_arr_flux.append(mini)\n",
    "    amplitude_flux.append(amplitude)\n",
    "    std_flux.append(std)\n",
    "    one_two_flux.append(one_to_second_flux)\n",
    "    one_two_time.append(one_to_second_time)\n",
    "    min_max.append(min_max_peak)\n",
    "#Creating labels\n",
    "for i in range(lgOFF):\n",
    "    \n",
    "    Labels.append(int(0))\n",
    "    \n",
    "for i in range(lgON):\n",
    "    Labels.append(int(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "002d603d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import cesium\n",
    "# from cesium import featurize\n",
    "\n",
    "# flux=[]\n",
    "# mjd=[]\n",
    "\n",
    "# #Construct data matrix\n",
    "# for j in range(len(dataOFF)):\n",
    "\n",
    "#     dataframe=pd.read_csv(dataOFF[j],index_col=[0])\n",
    "#     dataframe.columns=['MJD','Flux','Delta Flux','Photon Index','Delta Index','TS','fratio']\n",
    "\n",
    "#     mjd.append(np.array(dataframe['MJD']))\n",
    "#     flux.append(np.array(dataframe['Flux']))\n",
    "# for j in range(len(dataON)):\n",
    "\n",
    "#     v=j+len(dataOFF)\n",
    "#     dataframe=pd.read_csv(dataON[j],index_col=[0])\n",
    "#     dataframe.columns=['MJD','Flux','Delta Flux','Photon Index','Delta Index','TS','fratio']\n",
    "\n",
    "#     mjd.append(np.array(dataframe['MJD']))\n",
    "#     flux.append(np.array(dataframe['Flux']))\n",
    "\n",
    "# features_to_use = [\n",
    "#                   'max_slope','percent_amplitude','percent_difference_flux_percentile','skew']\n",
    "\n",
    "# CESIUM= featurize.featurize_time_series(times=mjd,\n",
    "#                                               values=flux,\n",
    "#                                               features_to_use=features_to_use)\n",
    "# max_slope=np.hstack(np.array(CESIUM[\"max_slope\"]))\n",
    "# percent_amplitude=np.hstack(np.array(CESIUM[\"percent_amplitude\"]))\n",
    "# percent_difference_flux_percentile=np.hstack(np.array(CESIUM[\"percent_difference_flux_percentile\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6750a0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbfeatures=3\n",
    "\n",
    "data_matrix=np.zeros((nbfeatures,lg))\n",
    "\n",
    "\n",
    "\n",
    "data_matrix[0]=nb_peaks_arr_flux\n",
    "data_matrix[1]=peaks_time_delay_arr_flux\n",
    "data_matrix[2]=std_peaks_arr_flux\n",
    "\n",
    "data_matrix=data_matrix.reshape((lg,nbfeatures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1b81241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data matrix shape:  (1140, 3)\n",
      "Number of OFF time series in data:  1095\n",
      "  \n",
      "Accuracy for ON class:  100.0 %\n",
      "Accuracy for OFF class:  70.56671501784447 %\n",
      "False Positive rate:  0.0 %\n",
      "False Negative rate:  29.43328498215552 %\n",
      "F1 score:  0.8034470374609403\n",
      "   \n",
      "Data matrix shape:  (420, 3)\n",
      "Number of OFF time series in data:  0\n",
      "  \n",
      "Accuracy for ON class:  100.0 %\n",
      "Accuracy for OFF class:  84.74797862129891 %\n",
      "False Positive rate:  0.0 %\n",
      "False Negative rate:  15.252021378701091 %\n",
      "F1 score:  0.8887448684697246\n",
      "   \n"
     ]
    }
   ],
   "source": [
    "from pyts.classification import BOSSVS\n",
    "from pyts.multivariate.classification import MultivariateClassifier\n",
    "import cesium\n",
    "from cesium import featurize\n",
    "from sklearn.model_selection import ShuffleSplit # or StratifiedShuffleSplit\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q_ON= []\n",
    "ON_accuracy=[]\n",
    "OFF_accuracy=[]\n",
    "FPOS=[]\n",
    "FNEG=[]\n",
    "fscore=[]\n",
    "lg=0\n",
    "lgOFF=0\n",
    "lgON=0\n",
    "\n",
    "for i in range(len(Labels)):\n",
    "    lg+=1\n",
    "    if Labels[i]==0:\n",
    "        lgOFF+=1\n",
    "    if Labels[i]==1:\n",
    "        lgON+=1\n",
    "weight_for_0 = (1 / lgOFF) * (lg / 2.0)\n",
    "weight_for_1 = (1 / lgON) * (lg / 2.0)\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "#3D shape of data, multivariate time series\n",
    "\n",
    "\n",
    "\n",
    "print(\"Data matrix shape: \", data_matrix.shape)\n",
    "print(\"Number of OFF time series in data: \",lgOFF)\n",
    "print(\"  \")\n",
    "for ii in range(100):\n",
    "    #3D shape of data, multivariate time series\n",
    "\n",
    "    sss = ShuffleSplit(n_splits=1, test_size=0.2,random_state=ii)\n",
    "    X=data_matrix\n",
    "    y=Labels\n",
    "\n",
    "    sss.get_n_splits(data_matrix, y)\n",
    "    train_index, test_index = next(sss.split(X, y)) \n",
    "\n",
    "    x_train, x_test = X[train_index], X[test_index] \n",
    "\n",
    "    y_train=[]\n",
    "    y_test=[]\n",
    "\n",
    "    for i in range(len(train_index)):\n",
    "        a=train_index[i]\n",
    "        y_train.append(y[a])\n",
    "    for i in range(len(test_index)):\n",
    "\n",
    "        a=test_index[i]\n",
    "        y_test.append(y[a])\n",
    "\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators=1000, max_features=\"auto\",\n",
    "                                          random_state=i,class_weight =class_weight)\n",
    "    Labels=np.hstack(Labels)\n",
    "    model.fit(x_train, y_train)\n",
    "    prediction= model.predict(x_test)\n",
    "    on_score=0\n",
    "    on_nbs=0\n",
    "    off_nbs=0\n",
    "    off_score=0\n",
    "    foff=0\n",
    "    fon=0\n",
    "    for i in range(len(y_test)):\n",
    "        if y_test[i]==1 :\n",
    "            on_nbs+=1\n",
    "            if prediction[i]==1:\n",
    "                on_score+=1\n",
    "                \n",
    "            else : \n",
    "                fon+=1\n",
    "        if y_test[i]==0 :\n",
    "            off_nbs+=1\n",
    "            if prediction[i]==0:\n",
    "                off_score+=1 \n",
    "            else:\n",
    "                foff+=1\n",
    "                a=test_index[i]\n",
    "                Q_ON.append(a)\n",
    "    if on_nbs>0:    \n",
    "        ON_accuracy.append(100*(on_score/on_nbs))\n",
    "        FPOS.append(100*(fon/on_nbs))\n",
    "    if off_nbs>0:\n",
    "        OFF_accuracy.append(100*(off_score/off_nbs))\n",
    "        FNEG.append(100*(foff/off_nbs))\n",
    "    f1= f1_score(y_test,prediction,average='weighted')\n",
    "    fscore.append(f1)\n",
    "\n",
    "print(\"Accuracy for ON class: \",np.mean(ON_accuracy) ,\"%\")\n",
    "print(\"Accuracy for OFF class: \",np.mean(OFF_accuracy) ,\"%\")\n",
    "print(\"False Positive rate: \",np.mean(FPOS) ,\"%\")\n",
    "print(\"False Negative rate: \",np.mean(FNEG) ,\"%\")\n",
    "print(\"F1 score: \",np.mean(fscore))\n",
    "print(\"   \")\n",
    "\n",
    "Q_ON2=[]\n",
    "Q_ON3=[]\n",
    "errOFF=[]\n",
    "gg = np.unique(Q_ON)\n",
    "if len(gg)>0:\n",
    "    for i in range(len(Q_ON)):\n",
    "        aa=Q_ON[i]\n",
    "        Q_ON2.append(file_names_ALL[aa])\n",
    "    gg = np.unique(Q_ON2)    \n",
    "    for i in range(len(gg)):\n",
    "        bb=gg[i]\n",
    "        count = Q_ON2.count(bb)\n",
    "        if count>1:\n",
    "            Q_ON3.append([bb,count])\n",
    "            errOFF.append(bb)\n",
    "            \n",
    "#Storing in a list the adresses of all the data files in the notebook\n",
    "\n",
    "data_file_ON=[]\n",
    "file_names_ON=[]\n",
    "\n",
    "data_files_OFF=[]\n",
    "file_names_OFF=[]\n",
    "\n",
    "#Storing in a list the adresses of all the data files in the notebook\n",
    "for filename in os.listdir(pathOFF):\n",
    "    f = os.path.join(pathOFF,filename)\n",
    "    if os.path.isfile(f) and filename in errOFF :\n",
    "        data_files_OFF.append(f)\n",
    "        file_names_OFF.append(filename)\n",
    "        data_files_ALL.append(f)\n",
    "        file_names_ALL.append(filename)\n",
    "for filename in os.listdir(pathON):\n",
    "    f = os.path.join(pathON,filename)\n",
    "    if os.path.isfile(f) :\n",
    "        data_files_ON.append(f)\n",
    "        file_names_ON.append(filename)\n",
    "        data_files_ALL.append(f)\n",
    "        file_names_ALL.append(filename)     \n",
    "\n",
    "\n",
    "\n",
    "binning=37\n",
    "\n",
    "    \n",
    "idx_OFF=[]\n",
    "idx_ON=[]\n",
    "dataON=[]\n",
    "dataOFF=[]\n",
    "for i in range(len(data_files_OFF)):\n",
    "    dataframe = pd.read_csv(data_files_OFF[i])\n",
    "    lg = len(dataframe)\n",
    "\n",
    "    if lg==binning:\n",
    "        idx_OFF.append(i)\n",
    "\n",
    "for i in range(len(data_files_ON)):\n",
    "    dataframe = pd.read_csv(data_files_ON[i])\n",
    "    lg = len(dataframe)\n",
    "\n",
    "    if lg==binning:\n",
    "        idx_ON.append(i)\n",
    "\n",
    "\n",
    "for i in range(len(idx_OFF)):\n",
    "\n",
    "    a=idx_OFF[i]\n",
    "    dataOFF.append(data_files_OFF[a])\n",
    "for i in range(len(idx_ON)):\n",
    "\n",
    "    a=idx_ON[i]\n",
    "    dataON.append(data_files_ON[a])\n",
    "\n",
    "idx = idx_OFF+idx_ON\n",
    "\n",
    "\n",
    "\n",
    "lgON=len(dataON)\n",
    "lgOFF=len(dataOFF)\n",
    "lg=lgON+lgOFF\n",
    "\n",
    "Labels=[]\n",
    "\n",
    "\n",
    "nb_peaks_arr_flux =[]\n",
    "peak_magnitudes_arr_flux =[]\n",
    "peaks_time_delay_arr_flux =[]\n",
    "std_peaks_arr_flux =[]\n",
    "mean_peaks_arr_flux =[]\n",
    "\n",
    "NG_diff_mean_peaks_arr_flux =[]\n",
    "max_diff_mean_peaks_arr_flux =[]\n",
    "min_diff_mean_peaks_arr_flux =[]\n",
    "mean_arr_flux=[]\n",
    "max_arr_flux=[]\n",
    "min_arr_flux=[]\n",
    "amplitude_flux=[]\n",
    "std_flux=[]\n",
    "one_two_flux=[]\n",
    "one_two_time=[]\n",
    "min_max=[]\n",
    "\n",
    "for i in range(len(dataOFF)):\n",
    "    \n",
    "    dataframe=pd.read_csv(dataOFF[i],index_col=[0])\n",
    "    dataframe.columns=['MJD','Flux','Delta Flux','Photon Index','Delta Index','TS','fratio']\n",
    "    \n",
    "    mjd=np.array(dataframe['MJD'])\n",
    "    flux=np.array(dataframe['Flux'])\n",
    "    photon_idx=np.array(dataframe['Photon Index'])\n",
    "    delta_index=np.array(dataframe['Delta Index'])\n",
    "    delta_flux=np.array(dataframe['Delta Flux'])\n",
    "    \n",
    "\n",
    "    nb_peaks,peak_magnitudes,peaks_time_delay,std_peaks,mean_peaks,NG_diff_mean_peaks,max_diff_mean_peaks,min_diff_mean_peaks,mean_arr,maxi,mini,amplitude,std,one_to_second_flux,one_to_second_time,min_max_peak=peak_study(flux,mjd,delta_flux)\n",
    "                                                                                                                                           \n",
    "    nb_peaks_arr_flux.append(nb_peaks)\n",
    "    peak_magnitudes_arr_flux.append(peak_magnitudes)\n",
    "    peaks_time_delay_arr_flux.append(peaks_time_delay)\n",
    "    std_peaks_arr_flux.append(std_peaks)\n",
    "    mean_peaks_arr_flux.append(mean_peaks)\n",
    "\n",
    "    NG_diff_mean_peaks_arr_flux.append(NG_diff_mean_peaks)\n",
    "    max_diff_mean_peaks_arr_flux.append(max_diff_mean_peaks)\n",
    "    min_diff_mean_peaks_arr_flux.append(min_diff_mean_peaks)\n",
    "    \n",
    "    mean_arr_flux.append(mean_arr)\n",
    "    max_arr_flux.append(maxi)\n",
    "    min_arr_flux.append(mini)\n",
    "    amplitude_flux.append(amplitude)\n",
    "    std_flux.append(std)\n",
    "    one_two_flux.append(one_to_second_flux)\n",
    "    one_two_time.append(one_to_second_time)\n",
    "    min_max.append(min_max_peak)\n",
    "for i in range(len(dataON)):\n",
    "    \n",
    "    dataframe=pd.read_csv(dataON[i],index_col=[0])\n",
    "    dataframe.columns=['MJD','Flux','Delta Flux','Photon Index','Delta Index','TS','fratio']\n",
    "    \n",
    "    mjd=np.array(dataframe['MJD'])\n",
    "    flux=np.array(dataframe['Flux'])\n",
    "    photon_idx=np.array(dataframe['Photon Index'])\n",
    "    delta_index=np.array(dataframe['Delta Index'])\n",
    "    delta_flux=np.array(dataframe['Delta Flux'])\n",
    "\n",
    "    nb_peaks,peak_magnitudes,peaks_time_delay,std_peaks,mean_peaks,NG_diff_mean_peaks,max_diff_mean_peaks,min_diff_mean_peaks,mean_arr,maxi,mini,amplitude,std,one_to_second_flux,one_to_second_time,min_max_peak=peak_study(flux,mjd,delta_flux)\n",
    "    nb_peaks_arr_flux.append(nb_peaks)\n",
    "    peak_magnitudes_arr_flux.append(peak_magnitudes)\n",
    "    peaks_time_delay_arr_flux.append(peaks_time_delay)\n",
    "    std_peaks_arr_flux.append(std_peaks)\n",
    "    mean_peaks_arr_flux.append(mean_peaks)\n",
    "\n",
    "    NG_diff_mean_peaks_arr_flux.append(NG_diff_mean_peaks)\n",
    "    max_diff_mean_peaks_arr_flux.append(max_diff_mean_peaks)\n",
    "    min_diff_mean_peaks_arr_flux.append(min_diff_mean_peaks)\n",
    "    \n",
    "    mean_arr_flux.append(mean_arr)\n",
    "    max_arr_flux.append(maxi)\n",
    "    min_arr_flux.append(mini)\n",
    "    amplitude_flux.append(amplitude)\n",
    "    std_flux.append(std)\n",
    "    one_two_flux.append(one_to_second_flux)\n",
    "    one_two_time.append(one_to_second_time)\n",
    "    min_max.append(min_max_peak)\n",
    "#Creating labels\n",
    "for i in range(lgOFF):\n",
    "    \n",
    "    Labels.append(int(0))\n",
    "    \n",
    "for i in range(lgON):\n",
    "    Labels.append(int(1))\n",
    "\n",
    "\n",
    "lgON=len(dataON)\n",
    "lgOFF=len(dataOFF)\n",
    "lg=lgON+lgOFF\n",
    "weight_for_0 = (1 / lgOFF) * (lg / 2.0)\n",
    "weight_for_1 = (1 / lgON) * (lg / 2.0)\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "\n",
    "\n",
    "nbfeatures=3\n",
    "\n",
    "data_matrix=np.zeros((nbfeatures,lg))\n",
    "\n",
    "\n",
    "\n",
    "data_matrix[0]=nb_peaks_arr_flux\n",
    "data_matrix[1]=peaks_time_delay_arr_flux\n",
    "data_matrix[2]=std_peaks_arr_flux\n",
    "\n",
    "\n",
    "data_matrix=data_matrix.reshape((lg,nbfeatures))\n",
    "\n",
    "\n",
    "Q_ON= []\n",
    "ON_accuracy=[]\n",
    "OFF_accuracy=[]\n",
    "FPOS=[]\n",
    "FNEG=[]\n",
    "fscore=[]\n",
    "lg=0\n",
    "lgOFF=0\n",
    "lgON=0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Data matrix shape: \", data_matrix.shape)\n",
    "print(\"Number of OFF time series in data: \",lgOFF)\n",
    "print(\"  \")\n",
    "for ii in range(100):\n",
    "    #3D shape of data, multivariate time series\n",
    "\n",
    "    sss = ShuffleSplit(n_splits=1, test_size=0.2,random_state=ii)\n",
    "    X=data_matrix\n",
    "    y=Labels\n",
    "\n",
    "    sss.get_n_splits(data_matrix, y)\n",
    "    train_index, test_index = next(sss.split(X, y)) \n",
    "\n",
    "    x_train, x_test = X[train_index], X[test_index] \n",
    "\n",
    "    y_train=[]\n",
    "    y_test=[]\n",
    "\n",
    "    for i in range(len(train_index)):\n",
    "        a=train_index[i]\n",
    "        y_train.append(y[a])\n",
    "    for i in range(len(test_index)):\n",
    "\n",
    "        a=test_index[i]\n",
    "        y_test.append(y[a])\n",
    "\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators=1000, max_features=\"auto\",\n",
    "                                          random_state=i,class_weight =class_weight)\n",
    "    Labels=np.hstack(Labels)\n",
    "    model.fit(x_train, y_train)\n",
    "    prediction= model.predict(x_test)\n",
    "    on_score=0\n",
    "    on_nbs=0\n",
    "    off_nbs=0\n",
    "    off_score=0\n",
    "    foff=0\n",
    "    fon=0\n",
    "    for i in range(len(y_test)):\n",
    "        if y_test[i]==1 :\n",
    "            on_nbs+=1\n",
    "            if prediction[i]==1:\n",
    "                on_score+=1\n",
    "        \n",
    "            else : \n",
    "                fon+=1\n",
    "        if y_test[i]==0 :\n",
    "            off_nbs+=1\n",
    "            if prediction[i]==0:\n",
    "                off_score+=1 \n",
    "            else:\n",
    "                foff+=1\n",
    "                a=test_index[i]\n",
    "                Q_ON.append(a)\n",
    "    if on_nbs>0:    \n",
    "        ON_accuracy.append(100*(on_score/on_nbs))\n",
    "        FPOS.append(100*(fon/on_nbs))\n",
    "    if off_nbs>0:\n",
    "        OFF_accuracy.append(100*(off_score/off_nbs))\n",
    "        FNEG.append(100*(foff/off_nbs))\n",
    "    f1= f1_score(y_test,prediction,average='weighted')\n",
    "    fscore.append(f1)\n",
    "\n",
    "print(\"Accuracy for ON class: \",np.mean(ON_accuracy) ,\"%\")\n",
    "print(\"Accuracy for OFF class: \",np.mean(OFF_accuracy) ,\"%\")\n",
    "print(\"False Positive rate: \",np.mean(FPOS) ,\"%\")\n",
    "print(\"False Negative rate: \",np.mean(FNEG) ,\"%\")\n",
    "print(\"F1 score: \",np.mean(fscore))\n",
    "print(\"   \")\n",
    "\n",
    "\n",
    "gg = np.unique(Q_ON)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
