{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2669d8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import math\n",
    "import sys \n",
    "import shutil\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from datetime import datetime\n",
    "from pandas import read_csv\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ShuffleSplit # or StratifiedShuffleSplit\n",
    "from pandas import DataFrame\n",
    "from pandas import DataFrame\n",
    "import sktime\n",
    "from sktime.classification.distance_based import KNeighborsTimeSeriesClassifier\n",
    "from scipy.signal import find_peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef988fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data location\n",
    "pathON=\"C:/Users/pierr/Documents_kanop/Shps/Pierre_points_for_louis/Bureau/interpo_ONOFF/NONinterpolated/ON_data/\"\n",
    "pathOFF=\"C:/Users/pierr/Documents_kanop/Shps/Pierre_points_for_louis/Bureau/interpo_ONOFF/NONinterpolated/OFF_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d02351a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing in a list the adresses of all the data files in the notebook\n",
    "\n",
    "data_files_ON=[]\n",
    "file_names_ON=[]\n",
    "\n",
    "data_files_OFF=[]\n",
    "file_names_OFF=[]\n",
    "data_files_ALL=[]\n",
    "file_names_ALL=[]\n",
    "\n",
    "#Storing in a list the adresses of all the data files in the notebook\n",
    "for filename in os.listdir(pathOFF):\n",
    "    f = os.path.join(pathOFF,filename)\n",
    "    if os.path.isfile(f):\n",
    "        data_files_OFF.append(f)\n",
    "        file_names_OFF.append(filename)\n",
    "        data_files_ALL.append(f)\n",
    "        file_names_ALL.append(filename)\n",
    "for filename in os.listdir(pathON):\n",
    "    f = os.path.join(pathON,filename)\n",
    "    if os.path.isfile(f):\n",
    "        data_files_ON.append(f)\n",
    "        file_names_ON.append(filename)\n",
    "        data_files_ALL.append(f)\n",
    "        file_names_ALL.append(filename)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6274a0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using data_files with only the 36 binning\n",
    "Filter=True\n",
    "binning=37\n",
    "\n",
    "\n",
    "if Filter==True:\n",
    "    \n",
    "    idx_OFF=[]\n",
    "    idx_ON=[]\n",
    "    dataON=[]\n",
    "    dataOFF=[]\n",
    "    for i in range(len(data_files_OFF)):\n",
    "        dataframe = pd.read_csv(data_files_OFF[i])\n",
    "        lg = len(dataframe)\n",
    "        \n",
    "        if lg==binning:\n",
    "            idx_OFF.append(i)\n",
    "    \n",
    "    for i in range(len(data_files_ON)):\n",
    "        dataframe = pd.read_csv(data_files_ON[i])\n",
    "        lg = len(dataframe)\n",
    "        if lg==binning:\n",
    "            idx_ON.append(i)\n",
    "\n",
    "\n",
    "    for i in range(len(idx_OFF)):\n",
    "\n",
    "        a=idx_OFF[i]\n",
    "        dataOFF.append(data_files_OFF[a])\n",
    "    for i in range(len(idx_ON)):\n",
    "\n",
    "        a=idx_ON[i]\n",
    "        dataON.append(data_files_ON[a])\n",
    "\n",
    "idx = idx_OFF+idx_ON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5aef3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to obtain my custom features\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def peak_study(array,time,delta_array):\n",
    "    \n",
    "    indices = find_peaks(array)\n",
    "    indices=np.delete(indices,-1)\n",
    "    y=[array[j] for j in indices]\n",
    "    x=[time[j] for j in indices]\n",
    "    y_err=[delta_array[j] for j in indices]\n",
    "    \n",
    "    x=np.hstack(x)\n",
    "    y=np.hstack(y)\n",
    "    y_err=np.hstack(y_err)\n",
    "    \n",
    "    copy_flux=y.copy()\n",
    "    copy_time=x.copy()\n",
    "    a=np.argmax(copy_flux)\n",
    "    ymax1=copy_flux[a]\n",
    "    tmax1=time[a]\n",
    "    \n",
    "    copy_time=np.delete(copy_time,a)\n",
    "    copy_flux=np.delete(copy_flux,a)\n",
    "    a=np.argmax(copy_flux)\n",
    "    ymax2=copy_flux[a]\n",
    "    tmax2=copy_time[a]\n",
    "    \n",
    "    one_to_second_flux=abs(ymax1-ymax2)\n",
    "    one_to_second_time=abs(tmax1-tmax2)\n",
    "\n",
    "    \n",
    "    min_max_peak= abs(max(y)-min(y))\n",
    "    arr=[1,-1]\n",
    "    for i in range(len(y)):\n",
    "        if np.isnan(y[i])==False:\n",
    "            error=y_err[i]\n",
    "            choice=np.random.choice(arr,1)\n",
    "            if choice ==1:\n",
    "                y[i]=y[i]+error\n",
    "            if choice==-1:\n",
    "                y[i]=y[i]-error\n",
    "                \n",
    "    peak_magnitudes=y\n",
    "    nb_peaks=len(y)\n",
    "    delta_energy_arr=[]\n",
    "    delta_time_arr=[]\n",
    "    \n",
    "    for i in range(nb_peaks-1):\n",
    "        \n",
    "        delta = x[i+1]-x[i]\n",
    "        delta_energy=abs(y[i+1]-y[i])\n",
    "        delta_time_arr.append(delta)\n",
    "        delta_energy_arr.append(delta_energy)\n",
    "    delay_std=np.std(delta_time_arr)\n",
    "    max_time_delay=max(delta_time_arr)\n",
    "    min_time_delay=min(delta_time_arr)\n",
    "    NG_diff_mean_peaks=np.mean(delta_energy_arr)\n",
    "    max_diff_mean_peaks=max(y)-np.mean(y)\n",
    "    min_diff_mean_peaks=min(y)-np.mean(y)\n",
    "    peaks_time_delay=np.mean(delta_time_arr)\n",
    "    std_peaks=np.std(y)\n",
    "    mean_peaks=np.mean(y)\n",
    "    var_peaks=np.var(y)\n",
    "    mean_arr=np.mean(array)\n",
    "    maxi=max(array)\n",
    "    mini=min(array)\n",
    "    amplitude=max(array)-min(array)\n",
    "    std=np.std(array)\n",
    "    return nb_peaks,peak_magnitudes,peaks_time_delay,std_peaks,mean_peaks,NG_diff_mean_peaks,max_diff_mean_peaks,min_diff_mean_peaks,mean_arr,maxi,mini,amplitude,std,one_to_second_flux,one_to_second_time,min_max_peak,max_time_delay,min_time_delay,delay_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7ed82f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Machine learning section and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a625d7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating features and putting them in arrays for each class\n",
    "\n",
    "lgON=len(dataON)\n",
    "lgOFF=len(dataOFF)\n",
    "lg=lgON+lgOFF\n",
    "\n",
    "Labels=[]\n",
    "\n",
    "\n",
    "\n",
    "nb_peaks_arr_flux =[]\n",
    "peak_magnitudes_arr_flux =[]\n",
    "peaks_time_delay_arr_flux =[]\n",
    "std_peaks_arr_flux =[]\n",
    "mean_peaks_arr_flux =[]\n",
    "\n",
    "NG_diff_mean_peaks_arr_flux =[]\n",
    "max_diff_mean_peaks_arr_flux =[]\n",
    "min_diff_mean_peaks_arr_flux =[]\n",
    "mean_arr_flux=[]\n",
    "max_arr_flux=[]\n",
    "min_arr_flux=[]\n",
    "amplitude_flux=[]\n",
    "std_flux=[]\n",
    "one_two_flux=[]\n",
    "one_two_time=[]\n",
    "min_max=[]\n",
    "min_deltaT=[]\n",
    "max_deltaT=[]\n",
    "deltaT_std=[]\n",
    "for i in range(len(dataOFF)):\n",
    "    \n",
    "    dataframe=pd.read_csv(dataOFF[i],index_col=[0])\n",
    "    dataframe.columns=['MJD','Flux','Delta Flux','Photon Index','Delta Index','TS','fratio']\n",
    "    \n",
    "    mjd=np.array(dataframe['MJD'])\n",
    "    flux=np.array(dataframe['Flux'])\n",
    "    photon_idx=np.array(dataframe['Photon Index'])\n",
    "    delta_index=np.array(dataframe['Delta Index'])\n",
    "    delta_flux=np.array(dataframe['Delta Flux'])\n",
    "    \n",
    "\n",
    "    nb_peaks,peak_magnitudes,peaks_time_delay,std_peaks,mean_peaks,NG_diff_mean_peaks,max_diff_mean_peaks,min_diff_mean_peaks,mean_arr,maxi,mini,amplitude,std,one_to_second_flux,one_to_second_time,min_max_peak,max_time_delay,min_time_delay,delay_std=peak_study(flux,mjd,delta_flux)\n",
    "                                                                                                                                           \n",
    "    nb_peaks_arr_flux.append(nb_peaks)\n",
    "    peak_magnitudes_arr_flux.append(peak_magnitudes)\n",
    "    peaks_time_delay_arr_flux.append(peaks_time_delay)\n",
    "    std_peaks_arr_flux.append(std_peaks)\n",
    "    mean_peaks_arr_flux.append(mean_peaks)\n",
    "\n",
    "    NG_diff_mean_peaks_arr_flux.append(NG_diff_mean_peaks)\n",
    "    max_diff_mean_peaks_arr_flux.append(max_diff_mean_peaks)\n",
    "    min_diff_mean_peaks_arr_flux.append(min_diff_mean_peaks)\n",
    "    \n",
    "    mean_arr_flux.append(mean_arr)\n",
    "    max_arr_flux.append(maxi)\n",
    "    min_arr_flux.append(mini)\n",
    "    amplitude_flux.append(amplitude)\n",
    "    std_flux.append(std)\n",
    "    one_two_flux.append(one_to_second_flux)\n",
    "    one_two_time.append(one_to_second_time)\n",
    "    min_max.append(min_max_peak)\n",
    "    min_deltaT.append(min_time_delay)\n",
    "    max_deltaT.append(max_time_delay)\n",
    "    deltaT_std.append(delay_std)\n",
    "for i in range(len(dataON)):\n",
    "    \n",
    "    dataframe=pd.read_csv(dataON[i],index_col=[0])\n",
    "    dataframe.columns=['MJD','Flux','Delta Flux','Photon Index','Delta Index','TS','fratio']\n",
    "    \n",
    "    mjd=np.array(dataframe['MJD'])\n",
    "    flux=np.array(dataframe['Flux'])\n",
    "    photon_idx=np.array(dataframe['Photon Index'])\n",
    "    delta_index=np.array(dataframe['Delta Index'])\n",
    "    delta_flux=np.array(dataframe['Delta Flux'])\n",
    "\n",
    "    nb_peaks,peak_magnitudes,peaks_time_delay,std_peaks,mean_peaks,NG_diff_mean_peaks,max_diff_mean_peaks,min_diff_mean_peaks,mean_arr,maxi,mini,amplitude,std,one_to_second_flux,one_to_second_time,min_max_peak,max_time_delay,min_time_delay,delay_std=peak_study(flux,mjd,delta_flux)\n",
    "    nb_peaks_arr_flux.append(nb_peaks)\n",
    "    peak_magnitudes_arr_flux.append(peak_magnitudes)\n",
    "    peaks_time_delay_arr_flux.append(peaks_time_delay)\n",
    "    std_peaks_arr_flux.append(std_peaks)\n",
    "    mean_peaks_arr_flux.append(mean_peaks)\n",
    "\n",
    "    NG_diff_mean_peaks_arr_flux.append(NG_diff_mean_peaks)\n",
    "    max_diff_mean_peaks_arr_flux.append(max_diff_mean_peaks)\n",
    "    min_diff_mean_peaks_arr_flux.append(min_diff_mean_peaks)\n",
    "    \n",
    "    mean_arr_flux.append(mean_arr)\n",
    "    max_arr_flux.append(maxi)\n",
    "    min_arr_flux.append(mini)\n",
    "    amplitude_flux.append(amplitude)\n",
    "    std_flux.append(std)\n",
    "    one_two_flux.append(one_to_second_flux)\n",
    "    one_two_time.append(one_to_second_time)\n",
    "    min_max.append(min_max_peak)\n",
    "    min_deltaT.append(min_time_delay)\n",
    "    max_deltaT.append(max_time_delay)\n",
    "    deltaT_std.append(delay_std)\n",
    "#Creating labels\n",
    "for i in range(lgOFF):\n",
    "    \n",
    "    Labels.append(int(0))\n",
    "    \n",
    "for i in range(lgON):\n",
    "    Labels.append(int(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6750a0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbfeatures=5\n",
    "\n",
    "data_matrix=np.zeros((nbfeatures,lg))\n",
    "\n",
    "\n",
    "#Best combination of  features for best performance of classification\n",
    "data_matrix[0]=nb_peaks_arr_flux\n",
    "data_matrix[1]=peaks_time_delay_arr_flux\n",
    "data_matrix[2]=std_peaks_arr_flux\n",
    "\n",
    "# data_matrix[3]=deltaT_std\n",
    "# data_matrix[3]=min_deltaT\n",
    "# data_matrix[5]=max_deltaT\n",
    "\n",
    "#Other features created by peak study function \n",
    "\n",
    "# data_matrix[4]=mean_peaks_arr_flux\n",
    "# data_matrix[5]=min_max\n",
    "# data_matrix[8]=std_flux\n",
    "# data_matrix[9]=amplitude_flux\n",
    "# data_matrix[10]=mean_arr_flux\n",
    "# data_matrix[11]=min_arr_flux\n",
    "data_matrix[3]=one_two_flux\n",
    "data_matrix[4]=one_two_time\n",
    "\n",
    "# data_matrix[7]=NG_diff_mean_peaks_arr_flux\n",
    "# data_matrix[4]=max_diff_mean_peaks_arr_flux\n",
    "# data_matrix[5]=min_diff_mean_peaks_arr_flux\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_matrix=data_matrix.reshape((lg,nbfeatures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd559537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for ON class:  52.733773536405124 %\n",
      "Accuracy for OFF class:  95.77735262988861 %\n",
      "False Positive rate:  47.26622646359489 %\n",
      "False Negative rate:  4.222647370111373 %\n",
      "F1 score:  0.9132626567441802\n"
     ]
    }
   ],
   "source": [
    "import cesium\n",
    "from cesium import featurize\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "ON_accuracy=[]\n",
    "OFF_accuracy=[]\n",
    "fscore=[]\n",
    "FPOS=[]\n",
    "FNEG= []\n",
    "bad_ON=[]\n",
    "iterations=20\n",
    "lgON=len(dataON)\n",
    "lgOFF=len(dataOFF)\n",
    "lg=lgON+lgOFF\n",
    "weight_for_0 = (1 / lgOFF) * (lg / 2.0)\n",
    "weight_for_1 = (1 / lgON) * (lg / 2.0)\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "for i in  range(iterations):\n",
    "                 \n",
    "    x_train, x_test, y_train, y_test = train_test_split(data_matrix, Labels, test_size=0.1, random_state=i)\n",
    "\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators=1000, max_features=\"auto\",\n",
    "                                          random_state=i,class_weight =class_weight)\n",
    "    Labels=np.hstack(Labels)\n",
    "    model.fit(x_train, y_train)\n",
    "    prediction= model.predict(x_test)\n",
    "    on_score=0\n",
    "    on_nbs=0\n",
    "    off_nbs=0\n",
    "    off_score=0\n",
    "    foff=0\n",
    "    fon=0\n",
    "\n",
    "    for i in range(len(y_test)):\n",
    "        if y_test[i]==1 :\n",
    "            on_nbs+=1\n",
    "            if prediction[i]==1:\n",
    "                on_score+=1\n",
    "                \n",
    "            else : \n",
    "                fon+=1\n",
    "                bad_ON.append(i)\n",
    "        if y_test[i]==0 :\n",
    "            off_nbs+=1\n",
    "            if prediction[i]==0:\n",
    "                off_score+=1 \n",
    "            else:\n",
    "                foff+=1\n",
    "    if on_nbs>0:    \n",
    "        ON_accuracy.append(100*(on_score/on_nbs))\n",
    "        FPOS.append(100*(fon/on_nbs))\n",
    "    OFF_accuracy.append(100*(off_score/off_nbs))\n",
    "    FNEG.append(100*(foff/off_nbs))\n",
    "    f1= f1_score(prediction,y_test,average='weighted')\n",
    "    fscore.append(f1)\n",
    "print(\"Accuracy for ON class: \",np.mean(ON_accuracy) ,\"%\")\n",
    "print(\"Accuracy for OFF class: \",np.mean(OFF_accuracy) ,\"%\")\n",
    "print(\"False Positive rate: \",np.mean(FPOS) ,\"%\")\n",
    "print(\"False Negative rate: \",np.mean(FNEG) ,\"%\")\n",
    "print(\"F1 score: \",np.mean(fscore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ab2b03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214b2c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71788018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for ON class:  35.976263395187544 %\n",
      "Accuracy for OFF class:  92.18103243345372 %\n",
      "False Positive rate:  64.02373660481246 %\n",
      "False Negative rate:  7.818967566546277 %\n",
      "F1 score:  0.8599958112776847\n"
     ]
    }
   ],
   "source": [
    "import cesium\n",
    "from cesium import featurize\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "ON_accuracy=[]\n",
    "OFF_accuracy=[]\n",
    "fscore=[]\n",
    "FPOS=[]\n",
    "FNEG= []\n",
    "bad_ON=[]\n",
    "iterations=300\n",
    "lgON=len(dataON)\n",
    "lgOFF=len(dataOFF)\n",
    "lg=lgON+lgOFF\n",
    "weight_for_0 = (1 / lgOFF) * (lg / 2.0)\n",
    "weight_for_1 = (1 / lgON) * (lg / 2.0)\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "for i in  range(iterations):\n",
    "                 \n",
    "    x_train, x_test, y_train, y_test = train_test_split(data_matrix, Labels, test_size=0.1, random_state=i)\n",
    "\n",
    "\n",
    "    model = KNeighborsClassifier(2,weights='distance')\n",
    "    Labels=np.hstack(Labels)\n",
    "    model.fit(x_train, y_train)\n",
    "    prediction= model.predict(x_test)\n",
    "    on_score=0\n",
    "    on_nbs=0\n",
    "    off_nbs=0\n",
    "    off_score=0\n",
    "    foff=0\n",
    "    fon=0\n",
    "\n",
    "    for i in range(len(y_test)):\n",
    "        if y_test[i]==1 :\n",
    "            on_nbs+=1\n",
    "            if prediction[i]==1:\n",
    "                on_score+=1\n",
    "                \n",
    "            else : \n",
    "                fon+=1\n",
    "                bad_ON.append(i)\n",
    "        if y_test[i]==0 :\n",
    "            off_nbs+=1\n",
    "            if prediction[i]==0:\n",
    "                off_score+=1 \n",
    "            else:\n",
    "                foff+=1\n",
    "    if on_nbs>0:    \n",
    "        ON_accuracy.append(100*(on_score/on_nbs))\n",
    "        FPOS.append(100*(fon/on_nbs))\n",
    "    OFF_accuracy.append(100*(off_score/off_nbs))\n",
    "    FNEG.append(100*(foff/off_nbs))\n",
    "    f1= f1_score(prediction,y_test,average='weighted')\n",
    "    fscore.append(f1)\n",
    "print(\"Accuracy for ON class: \",np.mean(ON_accuracy) ,\"%\")\n",
    "print(\"Accuracy for OFF class: \",np.mean(OFF_accuracy) ,\"%\")\n",
    "print(\"False Positive rate: \",np.mean(FPOS) ,\"%\")\n",
    "print(\"False Negative rate: \",np.mean(FNEG) ,\"%\")\n",
    "print(\"F1 score: \",np.mean(fscore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad2b454d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for ON class:  0.0 %\n",
      "Accuracy for OFF class:  99.4116527037319 %\n",
      "False Positive rate:  100.0 %\n",
      "False Negative rate:  0.5883472962680883 %\n",
      "F1 score:  0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "\n",
    "\n",
    "ON_accuracy=[]\n",
    "OFF_accuracy=[]\n",
    "fscore=[]\n",
    "FPOS=[]\n",
    "FNEG= []\n",
    "bad_ON=[]\n",
    "iterations=10\n",
    "lgON=len(dataON)\n",
    "lgOFF=len(dataOFF)\n",
    "lg=lgON+lgOFF\n",
    "weight_for_0 = (1 / lgOFF) * (lg / 2.0)\n",
    "weight_for_1 = (1 / lgON) * (lg / 2.0)\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "for i in range(iterations):\n",
    "                 \n",
    "    x_train, x_test, y_train, y_test = train_test_split(data_matrix, Labels, test_size=0.1, random_state=i)\n",
    "\n",
    "\n",
    "    \n",
    "    model = AdaBoostClassifier(n_estimators=500, random_state=0)\n",
    "    model.fit(x_train, y_train)\n",
    "    Labels=np.hstack(Labels)\n",
    "    prediction= model.predict(x_test)\n",
    "    on_score=0\n",
    "    on_nbs=0\n",
    "    off_nbs=0\n",
    "    off_score=0\n",
    "    foff=0\n",
    "    fon=0\n",
    "\n",
    "    for i in range(len(y_test)):\n",
    "        if y_test[i]==1 :\n",
    "            on_nbs+=1\n",
    "            if prediction[i]==1:\n",
    "                on_score+=1\n",
    "                \n",
    "            else : \n",
    "                fon+=1\n",
    "                bad_ON.append(i)\n",
    "        if y_test[i]==0 :\n",
    "            off_nbs+=1\n",
    "            if prediction[i]==0:\n",
    "                off_score+=1 \n",
    "            else:\n",
    "                foff+=1\n",
    "    if on_nbs>0:    \n",
    "        ON_accuracy.append(100*(on_score/on_nbs))\n",
    "        FPOS.append(100*(fon/on_nbs))\n",
    "    OFF_accuracy.append(100*(off_score/off_nbs))\n",
    "    FNEG.append(100*(foff/off_nbs))\n",
    "    f1= f1_score(prediction,y_test)\n",
    "    fscore.append(f1)\n",
    "print(\"Accuracy for ON class: \",np.mean(ON_accuracy) ,\"%\")\n",
    "print(\"Accuracy for OFF class: \",np.mean(OFF_accuracy) ,\"%\")\n",
    "print(\"False Positive rate: \",np.mean(FPOS) ,\"%\")\n",
    "print(\"False Negative rate: \",np.mean(FNEG) ,\"%\")\n",
    "print(\"F1 score: \",np.mean(fscore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a63ae1c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Obtaining  mean values of features for each class\n",
    "%matplotlib widget\n",
    "from mpl_toolkits import mplot3d\n",
    " \n",
    "lgON=len(dataON)\n",
    "lgOFF=len(dataOFF)\n",
    "lg=lgON+lgOFF\n",
    "\n",
    "Labels=[]\n",
    "\n",
    "\n",
    "\n",
    "nb_peaks_arr_flux =[]\n",
    "peak_magnitudes_arr_flux =[]\n",
    "peaks_time_delay_arr_flux =[]\n",
    "std_peaks_arr_flux =[]\n",
    "mean_peaks_arr_flux =[]\n",
    "\n",
    "NG_diff_mean_peaks_arr_flux =[]\n",
    "max_diff_mean_peaks_arr_flux =[]\n",
    "min_diff_mean_peaks_arr_flux =[]\n",
    "mean_arr_flux=[]\n",
    "max_arr_flux=[]\n",
    "min_arr_flux=[]\n",
    "amplitude_flux=[]\n",
    "std_flux=[]\n",
    "one_two_flux=[]\n",
    "one_two_time=[]\n",
    "min_max=[]\n",
    "for i in range(len(dataOFF)):\n",
    "    \n",
    "    dataframe=pd.read_csv(dataOFF[i],index_col=[0])\n",
    "    dataframe.columns=['MJD','Flux','Delta Flux','Photon Index','Delta Index','TS','fratio']\n",
    "    \n",
    "    mjd=np.array(dataframe['MJD'])\n",
    "    flux=np.array(dataframe['Flux'])\n",
    "    photon_idx=np.array(dataframe['Photon Index'])\n",
    "    delta_index=np.array(dataframe['Delta Index'])\n",
    "    delta_flux=np.array(dataframe['Delta Flux'])\n",
    "    \n",
    "\n",
    "    nb_peaks,peak_magnitudes,peaks_time_delay,std_peaks,mean_peaks,NG_diff_mean_peaks,max_diff_mean_peaks,min_diff_mean_peaks,mean_arr,maxi,mini,amplitude,std,one_to_second_flux,one_to_second_time,min_max_peak=peak_study(flux,mjd,delta_flux)\n",
    "                                                                                                                                           \n",
    "    nb_peaks_arr_flux.append(nb_peaks)\n",
    "    peak_magnitudes_arr_flux.append(peak_magnitudes)\n",
    "    peaks_time_delay_arr_flux.append(peaks_time_delay)\n",
    "    std_peaks_arr_flux.append(std_peaks)\n",
    "    mean_peaks_arr_flux.append(mean_peaks)\n",
    "\n",
    "    NG_diff_mean_peaks_arr_flux.append(NG_diff_mean_peaks)\n",
    "    max_diff_mean_peaks_arr_flux.append(max_diff_mean_peaks)\n",
    "    min_diff_mean_peaks_arr_flux.append(min_diff_mean_peaks)\n",
    "    \n",
    "    mean_arr_flux.append(mean_arr)\n",
    "    max_arr_flux.append(maxi)\n",
    "    min_arr_flux.append(mini)\n",
    "    amplitude_flux.append(amplitude)\n",
    "    std_flux.append(std)\n",
    "    one_two_flux.append(one_to_second_flux)\n",
    "    one_two_time.append(one_to_second_time)\n",
    "    min_max.append(min_max_peak)\n",
    "    \n",
    "#Creating labels\n",
    "for i in range(lgOFF):\n",
    "    \n",
    "    Labels.append(int(0))\n",
    "    \n",
    "for i in range(lgON):\n",
    "    Labels.append(int(1))\n",
    "\n",
    "\n",
    "z=std_peaks_arr_flux\n",
    "x=nb_peaks_arr_flux\n",
    "y=peaks_time_delay_arr_flux\n",
    "fig = plt.figure(figsize = (7, 4))\n",
    "ax = plt.axes(projection =\"3d\")\n",
    " \n",
    "# Create Plot\n",
    "\n",
    "ax.scatter3D(x, y, z, marker='<', s=20, label='Triangle')\n",
    "\n",
    "# plt.show()    \n",
    "\n",
    "print(\"Mean standard deviation of peak magnitudes for OFF class: \",np.mean(std_peaks_arr_flux))\n",
    "print(\"Mean number of peaks in lightsource curve for OFF class: \",np.mean(nb_peaks_arr_flux))\n",
    "print(\"Mean time difference between peaks for OFF class: \",np.mean(peaks_time_delay_arr_flux))\n",
    "print(\"  \")\n",
    "\n",
    "\n",
    "Labels=[]\n",
    "nb_peaks_arr_flux =[]\n",
    "peak_magnitudes_arr_flux =[]\n",
    "peaks_time_delay_arr_flux =[]\n",
    "std_peaks_arr_flux =[]\n",
    "mean_peaks_arr_flux =[]\n",
    "\n",
    "NG_diff_mean_peaks_arr_flux =[]\n",
    "max_diff_mean_peaks_arr_flux =[]\n",
    "min_diff_mean_peaks_arr_flux =[]\n",
    "mean_arr_flux=[]\n",
    "max_arr_flux=[]\n",
    "min_arr_flux=[]\n",
    "amplitude_flux=[]\n",
    "std_flux=[]\n",
    "one_two_flux=[]\n",
    "one_two_time=[]\n",
    "min_max=[]\n",
    "for i in range(len(dataON)):\n",
    "    \n",
    "    dataframe=pd.read_csv(dataON[i],index_col=[0])\n",
    "    dataframe.columns=['MJD','Flux','Delta Flux','Photon Index','Delta Index','TS','fratio']\n",
    "    \n",
    "    mjd=np.array(dataframe['MJD'])\n",
    "    flux=np.array(dataframe['Flux'])\n",
    "    photon_idx=np.array(dataframe['Photon Index'])\n",
    "    delta_index=np.array(dataframe['Delta Index'])\n",
    "    delta_flux=np.array(dataframe['Delta Flux'])\n",
    "\n",
    "    nb_peaks,peak_magnitudes,peaks_time_delay,std_peaks,mean_peaks,NG_diff_mean_peaks,max_diff_mean_peaks,min_diff_mean_peaks,mean_arr,maxi,mini,amplitude,std,one_to_second_flux,one_to_second_time,min_max_peak=peak_study(flux,mjd,delta_flux)\n",
    "    nb_peaks_arr_flux.append(nb_peaks)\n",
    "    peak_magnitudes_arr_flux.append(peak_magnitudes)\n",
    "    peaks_time_delay_arr_flux.append(peaks_time_delay)\n",
    "    std_peaks_arr_flux.append(std_peaks)\n",
    "    mean_peaks_arr_flux.append(mean_peaks)\n",
    "\n",
    "    NG_diff_mean_peaks_arr_flux.append(NG_diff_mean_peaks)\n",
    "    max_diff_mean_peaks_arr_flux.append(max_diff_mean_peaks)\n",
    "    min_diff_mean_peaks_arr_flux.append(min_diff_mean_peaks)\n",
    "    \n",
    "    mean_arr_flux.append(mean_arr)\n",
    "    max_arr_flux.append(maxi)\n",
    "    min_arr_flux.append(mini)\n",
    "    amplitude_flux.append(amplitude)\n",
    "    std_flux.append(std)\n",
    "    one_two_flux.append(one_to_second_flux)\n",
    "    one_two_time.append(one_to_second_time)\n",
    "    min_max.append(min_max_peak)\n",
    "    \n",
    "#Creating labels\n",
    "for i in range(lgOFF):\n",
    "    \n",
    "    Labels.append(int(0))\n",
    "    \n",
    "for i in range(lgON):\n",
    "    Labels.append(int(1))\n",
    "    \n",
    "# fig = plt.figure()\n",
    "# ax = plt.axes(projection=\"3d\")\n",
    "\n",
    "z=std_peaks_arr_flux\n",
    "x=nb_peaks_arr_flux\n",
    "y=peaks_time_delay_arr_flux\n",
    "\n",
    "ax.scatter3D(x, y, z, marker='o', s=20, label='Circle' );\n",
    "\n",
    "# Add legend\n",
    "\n",
    "ax.legend(loc=1)\n",
    " \n",
    "# Show plot\n",
    "\n",
    "plt.show()\n",
    "    \n",
    "print(\"Mean standard deviation of peak magnitudes for ON class: \",np.mean(std_peaks_arr_flux))\n",
    "print(\"Mean number of peaks in lightsource curve for ON class: \",np.mean(nb_peaks_arr_flux))\n",
    "print(\"Mean time difference between peaks for ON class: \",np.mean(peaks_time_delay_arr_flux))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13e5ded",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
