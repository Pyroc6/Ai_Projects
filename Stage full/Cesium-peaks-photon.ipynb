{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "2669d8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import math\n",
    "import sys \n",
    "import shutil\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from datetime import datetime\n",
    "from pandas import read_csv\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ShuffleSplit # or StratifiedShuffleSplit\n",
    "from pandas import DataFrame\n",
    "from pandas import DataFrame\n",
    "import sktime\n",
    "from sktime.classification.distance_based import KNeighborsTimeSeriesClassifier\n",
    "from scipy.signal import find_peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "ef988fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data location\n",
    "pathON=\"C:/Users/pierr/Documents_kanop/Shps/Pierre_points_for_louis/Bureau/interpo_ONOFF/ON_data/\"\n",
    "pathOFF=\"C:/Users/pierr/Documents_kanop/Shps/Pierre_points_for_louis/Bureau/interpo_ONOFF/OFF_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "d02351a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing in a list the adresses of all the data files in the notebook\n",
    "\n",
    "data_files_ON=[]\n",
    "file_names_ON=[]\n",
    "\n",
    "data_files_OFF=[]\n",
    "file_names_OFF=[]\n",
    "data_files_ALL=[]\n",
    "file_names_ALL=[]\n",
    "\n",
    "#Storing in a list the adresses of all the data files in the notebook\n",
    "for filename in os.listdir(pathOFF):\n",
    "    f = os.path.join(pathOFF,filename)\n",
    "    if os.path.isfile(f):\n",
    "        data_files_OFF.append(f)\n",
    "        file_names_OFF.append(filename)\n",
    "        data_files_ALL.append(f)\n",
    "        file_names_ALL.append(filename)\n",
    "for filename in os.listdir(pathON):\n",
    "    f = os.path.join(pathON,filename)\n",
    "    if os.path.isfile(f):\n",
    "        data_files_ON.append(f)\n",
    "        file_names_ON.append(filename)\n",
    "        data_files_ALL.append(f)\n",
    "        file_names_ALL.append(filename)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "6274a0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using data_files with only the 36 binning\n",
    "Filter=True\n",
    "binning=37\n",
    "\n",
    "\n",
    "if Filter==True:\n",
    "    \n",
    "    idx_OFF=[]\n",
    "    idx_ON=[]\n",
    "    dataON=[]\n",
    "    dataOFF=[]\n",
    "    for i in range(len(data_files_OFF)):\n",
    "        dataframe = pd.read_csv(data_files_OFF[i])\n",
    "        lg = len(dataframe)\n",
    "        \n",
    "        if lg==binning:\n",
    "            idx_OFF.append(i)\n",
    "    \n",
    "    for i in range(len(data_files_ON)):\n",
    "        dataframe = pd.read_csv(data_files_ON[i])\n",
    "        lg = len(dataframe)\n",
    "        if lg==binning:\n",
    "            idx_ON.append(i)\n",
    "\n",
    "\n",
    "    for i in range(len(idx_OFF)):\n",
    "\n",
    "        a=idx_OFF[i]\n",
    "        dataOFF.append(data_files_OFF[a])\n",
    "    for i in range(len(idx_ON)):\n",
    "\n",
    "        a=idx_ON[i]\n",
    "        dataON.append(data_files_ON[a])\n",
    "\n",
    "idx = idx_OFF+idx_ON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "c5aef3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to obtain my custom features\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "def peak_study(array,time,delta_array):\n",
    "    \n",
    "    indices = find_peaks(array)\n",
    "    indices=np.delete(indices,-1)\n",
    "    y=[array[j] for j in indices]\n",
    "    x=[time[j] for j in indices]\n",
    "    y_err=[delta_array[j] for j in indices]\n",
    "    \n",
    "    x=np.hstack(x)\n",
    "    y=np.hstack(y)\n",
    "    y_err=np.hstack(y_err)\n",
    "    \n",
    "    arr=[1,-1]\n",
    "    for i in range(len(y)):\n",
    "        if np.isnan(y[i])==False:\n",
    "            error=y_err[i]\n",
    "            choice=np.random.choice(arr,1)\n",
    "            if choice ==1:\n",
    "                y[i]=y[i]+error\n",
    "            if choice==-1:\n",
    "                y[i]=y[i]-error\n",
    "                \n",
    "    peak_magnitudes=y\n",
    "    nb_peaks=len(y)\n",
    "    delta_energy_arr=[]\n",
    "    delta_time_arr=[]\n",
    "    \n",
    "    for i in range(nb_peaks-1):\n",
    "        \n",
    "        delta = x[i+1]-x[i]\n",
    "        delta_energy=abs(y[i+1]-y[i])\n",
    "        delta_time_arr.append(delta)\n",
    "        delta_energy_arr.append(delta_energy)\n",
    "\n",
    "        \n",
    "    NG_diff_mean_peaks=np.mean(delta_energy_arr)\n",
    "    max_diff_mean_peaks=max(y)-np.mean(y)\n",
    "    min_diff_mean_peaks=min(y)-np.mean(y)\n",
    "    peaks_time_delay=np.mean(delta_time_arr)\n",
    "    std_peaks=np.std(y)\n",
    "    mean_peaks=np.mean(y)\n",
    "    var_peaks=np.var(y)\n",
    "    mean_arr=np.mean(array)\n",
    "    maxi=max(array)\n",
    "    mini=min(array)\n",
    "    amplitude=max(array)-min(array)\n",
    "    return nb_peaks,peak_magnitudes,peaks_time_delay,std_peaks,mean_peaks,NG_diff_mean_peaks,max_diff_mean_peaks,min_diff_mean_peaks,mean_arr,maxi,mini,amplitude\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "aff8d847",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "peak_study() missing 1 required positional argument: 'delta_array'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [367]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m photon_idx\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray(dataframe[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPhoton Index\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     18\u001b[0m delta_index\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray(dataframe[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDelta Index\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 20\u001b[0m nb_peaks,peak_magnitudes,peaks_time_delay,std_peaks,mean_peaks,NG_diff_mean_peaks,max_diff_mean_peaks,min_diff_mean_peaks \u001b[38;5;241m=\u001b[39m \u001b[43mpeak_study\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflux\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmjd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m nb_peaksON\u001b[38;5;241m.\u001b[39mappend(nb_peaks)\n\u001b[0;32m     24\u001b[0m peak_magnitudesON\u001b[38;5;241m.\u001b[39mappend(peak_magnitudes)\n",
      "\u001b[1;31mTypeError\u001b[0m: peak_study() missing 1 required positional argument: 'delta_array'"
     ]
    }
   ],
   "source": [
    "\n",
    "nb_peaksON=[]\n",
    "peak_magnitudesON=[]\n",
    "peaks_time_delayON=[]\n",
    "std_peaksON=[]\n",
    "mean_peaksON=[]\n",
    "\n",
    "NG_diff_mean_peaksON=[]\n",
    "max_diff_mean_peaksON=[]\n",
    "min_diff_mean_peaksON=[]\n",
    "for i in range(len(dataON)):\n",
    "    \n",
    "    dataframe=pd.read_csv(dataON[i],index_col=[0])\n",
    "    dataframe.columns=['MJD','Flux','Delta Flux','Photon Index','Delta Index','TS','fratio']\n",
    "    \n",
    "    mjd=np.array(dataframe['MJD'])\n",
    "    flux=np.array(dataframe['Flux'])\n",
    "    photon_idx=np.array(dataframe['Photon Index'])\n",
    "    delta_index=np.array(dataframe['Delta Index'])\n",
    "    \n",
    "    nb_peaks,peak_magnitudes,peaks_time_delay,std_peaks,mean_peaks,NG_diff_mean_peaks,max_diff_mean_peaks,min_diff_mean_peaks = peak_study(flux,mjd)\n",
    "    \n",
    "    nb_peaksON.append(nb_peaks)\n",
    "    \n",
    "    peak_magnitudesON.append(peak_magnitudes)\n",
    "    peaks_time_delayON.append(peaks_time_delay)\n",
    "    std_peaksON.append(std_peaks)\n",
    "    mean_peaksON.append(mean_peaks)\n",
    "\n",
    "    NG_diff_mean_peaksON.append(NG_diff_mean_peaks)\n",
    "    max_diff_mean_peaksON.append(max_diff_mean_peaks)\n",
    "    min_diff_mean_peaksON.append(min_diff_mean_peaks)\n",
    "    \n",
    "print(\"Mean Number of  peaks: \",np.mean(nb_peaksON))\n",
    "print(\"Average magnitude of peaks :  \",np.mean(mean_peaksON))\n",
    "print(\"Average delay between peaks: \",np.mean(peaks_time_delayON))\n",
    "print(\"Average difference between next neighbour peak magnitudes :  \",np.mean(NG_diff_mean_peaksON))\n",
    "print(\"Average difference between biggest peak and mean peak magnitude :  \",np.mean(max_diff_mean_peaksON))\n",
    "print(\"Average difference between minimum peak and mean peak magnitude :  \",np.mean(min_diff_mean_peaksON))\n",
    "print(\"Average standard deviation of peaks :  \",np.mean(std_peaksON))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659299bd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nb_peaksOFF=[]\n",
    "peak_magnitudesOFF=[]\n",
    "peaks_time_delayOFF=[]\n",
    "std_peaksOFF=[]\n",
    "mean_peaksOFF=[]\n",
    "\n",
    "NG_diff_mean_peaksOFF=[]\n",
    "max_diff_mean_peaksOFF=[]\n",
    "min_diff_mean_peaksOFF=[]\n",
    "for i in range(len(dataOFF)):\n",
    "    \n",
    "    dataframe=pd.read_csv(dataOFF[i],index_col=[0])\n",
    "    dataframe.columns=['MJD','Flux','Delta Flux','PhotOFF Index','Delta Index','TS','fratio']\n",
    "    \n",
    "    mjd=np.array(dataframe['MJD'])\n",
    "    flux=np.array(dataframe['Flux'])\n",
    "    photOFF_idx=np.array(dataframe['PhotOFF Index'])\n",
    "    delta_index=np.array(dataframe['Delta Index'])\n",
    "    \n",
    "    nb_peaks,peak_magnitudes,peaks_time_delay,std_peaks,mean_peaks,NG_diff_mean_peaks,max_diff_mean_peaks,min_diff_mean_peaks = peak_study(flux,mjd)\n",
    "    \n",
    "    nb_peaksOFF.append(nb_peaks)\n",
    "    \n",
    "    peak_magnitudesOFF.append(peak_magnitudes)\n",
    "    peaks_time_delayOFF.append(peaks_time_delay)\n",
    "    std_peaksOFF.append(std_peaks)\n",
    "    mean_peaksOFF.append(mean_peaks)\n",
    "\n",
    "    NG_diff_mean_peaksOFF.append(NG_diff_mean_peaks)\n",
    "    max_diff_mean_peaksOFF.append(max_diff_mean_peaks)\n",
    "    min_diff_mean_peaksOFF.append(min_diff_mean_peaks)\n",
    "    \n",
    "print(\"Mean Number of  peaks: \",np.mean(nb_peaksOFF))\n",
    "print(\"Average magnitude of peaks :  \",np.mean(mean_peaksOFF))\n",
    "print(\"Average delay between peaks: \",np.mean(peaks_time_delayOFF))\n",
    "print(\"Average difference between next neighbour peak magnitudes :  \",np.mean(NG_diff_mean_peaksOFF))\n",
    "print(\"Average difference between biggest peak and mean peak magnitude :  \",np.mean(max_diff_mean_peaksOFF))\n",
    "print(\"Average difference between minimum peak and mean peak magnitude :  \",np.mean(min_diff_mean_peaksOFF))\n",
    "lgON=len(dataON)\n",
    "lgOFF=len(dataOFF)\n",
    "lg=lgON+lgOFF\n",
    "print(\"Average standard deviation of peaks :  \",np.mean(std_peaksOFF))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "a7ed82f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Machine learning section and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "a625d7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgON=len(dataON)\n",
    "lgOFF=len(dataOFF)\n",
    "lg=lgON+lgOFF\n",
    "\n",
    "Labels=[]\n",
    "\n",
    "nb_peaks_arr_photon =[]\n",
    "peak_magnitudes_arr_photon =[]\n",
    "peaks_time_delay_arr_photon =[]\n",
    "std_peaks_arr_photon =[]\n",
    "mean_peaks_arr_photon =[]\n",
    "\n",
    "NG_diff_mean_peaks_arr_photon =[]\n",
    "max_diff_mean_peaks_arr_photon =[]\n",
    "min_diff_mean_peaks_arr_photon =[]\n",
    "\n",
    "nb_peaks_arr_flux =[]\n",
    "peak_magnitudes_arr_flux =[]\n",
    "peaks_time_delay_arr_flux =[]\n",
    "std_peaks_arr_flux =[]\n",
    "mean_peaks_arr_flux =[]\n",
    "\n",
    "NG_diff_mean_peaks_arr_flux =[]\n",
    "max_diff_mean_peaks_arr_flux =[]\n",
    "min_diff_mean_peaks_arr_flux =[]\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(dataOFF)):\n",
    "    \n",
    "    dataframe=pd.read_csv(dataOFF[i],index_col=[0])\n",
    "    dataframe.columns=['MJD','Flux','Delta Flux','Photon Index','Delta Index','TS','fratio']\n",
    "    \n",
    "    mjd=np.array(dataframe['MJD'])\n",
    "    flux=np.array(dataframe['Flux'])\n",
    "    photon_idx=np.array(dataframe['Photon Index'])\n",
    "    delta_index=np.array(dataframe['Delta Index'])\n",
    "    delta_flux=np.array(dataframe['Delta Flux'])\n",
    "    \n",
    "    nb_peaks,peak_magnitudes,peaks_time_delay,std_peaks,\n",
    "    mean_peaks,NG_diff_mean_peaks,max_diff_mean_peaks,min_diff_mean_peaks,mean_arr,maxi,mini,amplitude= peak_study(photon_idx,mjd,delta_index)\n",
    "    \n",
    "    nb_peaks_arr_photon.append(nb_peaks)\n",
    "    peak_magnitudes_arr_photon.append(peak_magnitudes)\n",
    "    peaks_time_delay_arr_photon.append(peaks_time_delay)\n",
    "    std_peaks_arr_photon.append(std_peaks)\n",
    "    mean_peaks_arr_photon.append(mean_peaks)\n",
    "    NG_diff_mean_peaks_arr_photon.append(NG_diff_mean_peaks)\n",
    "    max_diff_mean_peaks_arr_photon.append(max_diff_mean_peaks)\n",
    "    min_diff_mean_peaks_arr_photon.append(min_diff_mean_peaks)\n",
    "    nb_peaks,peak_magnitudes,peaks_time_delay,std_peaks,mean_peaks,NG_diff_mean_peaks,max_diff_mean_peaks,min_diff_mean_peaks = peak_study(flux,mjd,delta_flux)\n",
    "\n",
    "                                                                                                                                           \n",
    "    nb_peaks_arr_flux.append(nb_peaks)\n",
    "    peak_magnitudes_arr_flux.append(peak_magnitudes)\n",
    "    peaks_time_delay_arr_flux.append(peaks_time_delay)\n",
    "    std_peaks_arr_flux.append(std_peaks)\n",
    "    mean_peaks_arr_flux.append(mean_peaks)\n",
    "\n",
    "    NG_diff_mean_peaks_arr_flux.append(NG_diff_mean_peaks)\n",
    "    max_diff_mean_peaks_arr_flux.append(max_diff_mean_peaks)\n",
    "    min_diff_mean_peaks_arr_flux.append(min_diff_mean_peaks)\n",
    "for i in range(len(dataON)):\n",
    "    \n",
    "    dataframe=pd.read_csv(dataON[i],index_col=[0])\n",
    "    dataframe.columns=['MJD','Flux','Delta Flux','Photon Index','Delta Index','TS','fratio']\n",
    "    \n",
    "    mjd=np.array(dataframe['MJD'])\n",
    "    flux=np.array(dataframe['Flux'])\n",
    "    photon_idx=np.array(dataframe['Photon Index'])\n",
    "    delta_index=np.array(dataframe['Delta Index'])\n",
    "    delta_flux=np.array(dataframe['Delta Flux'])\n",
    "    \n",
    "    nb_peaks,peak_magnitudes,peaks_time_delay,std_peaks,mean_peaks,NG_diff_mean_peaks,max_diff_mean_peaks,min_diff_mean_peaks = peak_study(photon_idx,mjd,delta_index)\n",
    "    \n",
    "    nb_peaks_arr_photon.append(nb_peaks)\n",
    "    peak_magnitudes_arr_photon.append(peak_magnitudes)\n",
    "    peaks_time_delay_arr_photon.append(peaks_time_delay)\n",
    "    std_peaks_arr_photon.append(std_peaks)\n",
    "    mean_peaks_arr_photon.append(mean_peaks)\n",
    "\n",
    "    NG_diff_mean_peaks_arr_photon.append(NG_diff_mean_peaks)\n",
    "    max_diff_mean_peaks_arr_photon.append(max_diff_mean_peaks)\n",
    "    min_diff_mean_peaks_arr_photon.append(min_diff_mean_peaks)\n",
    "\n",
    "    nb_peaks,peak_magnitudes,peaks_time_delay,std_peaks,mean_peaks,NG_diff_mean_peaks,max_diff_mean_peaks,min_diff_mean_peaks = peak_study(flux,mjd,delta_flux)\n",
    "\n",
    "    nb_peaks_arr_flux.append(nb_peaks)\n",
    "    peak_magnitudes_arr_flux.append(peak_magnitudes)\n",
    "    peaks_time_delay_arr_flux.append(peaks_time_delay)\n",
    "    std_peaks_arr_flux.append(std_peaks)\n",
    "    mean_peaks_arr_flux.append(mean_peaks)\n",
    "\n",
    "    NG_diff_mean_peaks_arr_flux.append(NG_diff_mean_peaks)\n",
    "    max_diff_mean_peaks_arr_flux.append(max_diff_mean_peaks)\n",
    "    min_diff_mean_peaks_arr_flux.append(min_diff_mean_peaks)\n",
    "\n",
    "#Creating labels\n",
    "for i in range(lgOFF):\n",
    "    \n",
    "    Labels.append(int(0))\n",
    "    \n",
    "for i in range(lgON):\n",
    "    Labels.append(int(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49659976",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best performance\n",
    "\n",
    "nbfeatures=14\n",
    "\n",
    "data_matrix=np.zeros((nbfeatures,lg))\n",
    "\n",
    "data_matrix[0]=nb_peaks_arr_photon\n",
    "data_matrix[1]=peaks_time_delay_arr_photon\n",
    "data_matrix[2]=std_peaks_arr_photon\n",
    "data_matrix[3]=mean_peaks_arr_photon\n",
    "data_matrix[4]=NG_diff_mean_peaks_arr_photon\n",
    "data_matrix[5]=max_diff_mean_peaks_arr_photon\n",
    "data_matrix[6]=min_diff_mean_peaks_arr_photon\n",
    "\n",
    "data_matrix[7]=nb_peaks_arr_flux\n",
    "data_matrix[8]=peaks_time_delay_arr_flux\n",
    "data_matrix[9]=std_peaks_arr_flux\n",
    "data_matrix[10]=mean_peaks_arr_flux\n",
    "data_matrix[11]=NG_diff_mean_peaks_arr_flux\n",
    "data_matrix[12]=max_diff_mean_peaks_arr_flux\n",
    "data_matrix[13]=min_diff_mean_peaks_arr_flux\n",
    "\n",
    "\n",
    "data_matrix=data_matrix.reshape((lg,nbfeatures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "cd559537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for ON class:  94.53414195453668 %\n",
      "Accuracy for OFF class:  73.59036749269708 %\n",
      "False Positive rate:  5.465858045463309 %\n",
      "False Negative rate:  26.40963250730291 %\n",
      "F1 score:  0.6982718400447031\n"
     ]
    }
   ],
   "source": [
    "import cesium\n",
    "from cesium import featurize\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "ON_accuracy=[]\n",
    "OFF_accuracy=[]\n",
    "fscore=[]\n",
    "FPOS=[]\n",
    "FNEG= []\n",
    "bad_ON=[]\n",
    "iterations=300\n",
    "lgON=len(dataON)\n",
    "lgOFF=len(dataOFF)\n",
    "lg=lgON+lgOFF\n",
    "weight_for_0 = (1 / lgOFF) * (lg / 2.0)\n",
    "weight_for_1 = (1 / lgON) * (lg / 2.0)\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "for i in  range(iterations):\n",
    "                 \n",
    "    x_train, x_test, y_train, y_test = train_test_split(data_matrix, Labels, test_size=0.1, random_state=i)\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators=1000, max_features=\"auto\",\n",
    "                                          random_state=0,class_weight =class_weight)\n",
    "    Labels=np.hstack(Labels)\n",
    "    model.fit(x_train, y_train)\n",
    "    prediction= model.predict(x_test)\n",
    "    on_score=0\n",
    "    on_nbs=0\n",
    "    off_nbs=0\n",
    "    off_score=0\n",
    "    foff=0\n",
    "    fon=0\n",
    "\n",
    "    for i in range(len(y_test)):\n",
    "        if y_test[i]==1 :\n",
    "            on_nbs+=1\n",
    "            if prediction[i]==1:\n",
    "                on_score+=1\n",
    "                \n",
    "            else : \n",
    "                fon+=1\n",
    "                bad_ON.append(i)\n",
    "        if y_test[i]==0 :\n",
    "            off_nbs+=1\n",
    "            if prediction[i]==0:\n",
    "                off_score+=1 \n",
    "            else:\n",
    "                foff+=1\n",
    "    if on_nbs>0:    \n",
    "        ON_accuracy.append(100*(on_score/on_nbs))\n",
    "        FPOS.append(100*(fon/on_nbs))\n",
    "    OFF_accuracy.append(100*(off_score/off_nbs))\n",
    "    FNEG.append(100*(foff/off_nbs))\n",
    "    f1= f1_score(prediction,y_test,average='weighted')\n",
    "    fscore.append(f1)\n",
    "print(\"Accuracy for ON class: \",np.mean(ON_accuracy) ,\"%\")\n",
    "print(\"Accuracy for OFF class: \",np.mean(OFF_accuracy) ,\"%\")\n",
    "print(\"False Positive rate: \",np.mean(FPOS) ,\"%\")\n",
    "print(\"False Negative rate: \",np.mean(FNEG) ,\"%\")\n",
    "print(\"F1 score: \",np.mean(fscore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "052aabc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best performance\n",
    "\n",
    "nbfeatures=14\n",
    "\n",
    "data_matrix=np.zeros((nbfeatures,lg))\n",
    "\n",
    "data_matrix[0]=nb_peaks_arr_photon\n",
    "data_matrix[1]=peaks_time_delay_arr_photon\n",
    "data_matrix[2]=std_peaks_arr_photon\n",
    "data_matrix[3]=mean_peaks_arr_photon\n",
    "data_matrix[4]=NG_diff_mean_peaks_arr_photon\n",
    "data_matrix[5]=max_diff_mean_peaks_arr_photon\n",
    "data_matrix[6]=min_diff_mean_peaks_arr_photon\n",
    "\n",
    "data_matrix[7]=nb_peaks_arr_flux\n",
    "data_matrix[8]=peaks_time_delay_arr_flux\n",
    "data_matrix[9]=std_peaks_arr_flux\n",
    "data_matrix[10]=mean_peaks_arr_flux\n",
    "data_matrix[11]=NG_diff_mean_peaks_arr_flux\n",
    "data_matrix[12]=max_diff_mean_peaks_arr_fux\n",
    "data_matrix[13]=min_diff_mean_peaks_arr_flux\n",
    "\n",
    "\n",
    "data_matrix=data_matrix.reshape((lg,nbfeatures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7eaba374",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best performance\n",
    "\n",
    "nbfeatures=10\n",
    "\n",
    "data_matrix=np.zeros((nbfeatures,lg))\n",
    "\n",
    "data_matrix[0]=nb_peaks_arr_photon\n",
    "data_matrix[1]=peaks_time_delay_arr_photon\n",
    "data_matrix[2]=std_peaks_arr_photon\n",
    "data_matrix[3]=mean_peaks_arr_photon\n",
    "\n",
    "data_matrix[4]=nb_peaks_arr_flux\n",
    "data_matrix[5]=peaks_time_delay_arr_flux\n",
    "data_matrix[6]=std_peaks_arr_flux\n",
    "data_matrix[7]=mean_peaks_arr_flux\n",
    "\n",
    "data_matrix[8]=max_diff_mean_peaks_arr_photon\n",
    "data_matrix[9]=min_diff_mean_peaks_arr_photon\n",
    "\n",
    "\n",
    "data_matrix=data_matrix.reshape((lg,nbfeatures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca4062e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e9d64b6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "bcd30530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for ON class:  100.0 %\n",
      "Accuracy for OFF class:  51.87486650353194 %\n",
      "False Positive rate:  0.0 %\n",
      "False Negative rate:  48.12513349646805 %\n",
      "F1 score:  0.41372865185372165\n"
     ]
    }
   ],
   "source": [
    "import cesium\n",
    "from cesium import featurize\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "ON_accuracy=[]\n",
    "OFF_accuracy=[]\n",
    "fscore=[]\n",
    "FPOS=[]\n",
    "FNEG= []\n",
    "bad_ON=[]\n",
    "iterations=10\n",
    "lgON=len(dataON)\n",
    "lgOFF=len(dataOFF)\n",
    "lg=lgON+lgOFF\n",
    "weight_for_0 = (1 / lgOFF) * (lg / 2.0)\n",
    "weight_for_1 = (1 / lgON) * (lg / 2.0)\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "for i in  range(iterations):\n",
    "                 \n",
    "    x_train, x_test, y_train, y_test = train_test_split(data_matrix, Labels, test_size=0.1, random_state=i)\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators=2000, max_features=\"auto\",\n",
    "                                          random_state=0,class_weight =class_weight)\n",
    "    Labels=np.hstack(Labels)\n",
    "    model.fit(x_train, y_train)\n",
    "    prediction= model.predict(x_test)\n",
    "    on_score=0\n",
    "    on_nbs=0\n",
    "    off_nbs=0\n",
    "    off_score=0\n",
    "    foff=0\n",
    "    fon=0\n",
    "\n",
    "    for i in range(len(y_test)):\n",
    "        if y_test[i]==1 :\n",
    "            on_nbs+=1\n",
    "            if prediction[i]==1:\n",
    "                on_score+=1\n",
    "                \n",
    "            else : \n",
    "                fon+=1\n",
    "                bad_ON.append(i)\n",
    "        if y_test[i]==0 :\n",
    "            off_nbs+=1\n",
    "            if prediction[i]==0:\n",
    "                off_score+=1 \n",
    "            else:\n",
    "                foff+=1\n",
    "    if on_nbs>0:    \n",
    "        ON_accuracy.append(100*(on_score/on_nbs))\n",
    "        FPOS.append(100*(fon/on_nbs))\n",
    "    OFF_accuracy.append(100*(off_score/off_nbs))\n",
    "    FNEG.append(100*(foff/off_nbs))\n",
    "    f1= f1_score(prediction,y_test,average='weighted')\n",
    "    fscore.append(f1)\n",
    "print(\"Accuracy for ON class: \",np.mean(ON_accuracy) ,\"%\")\n",
    "print(\"Accuracy for OFF class: \",np.mean(OFF_accuracy) ,\"%\")\n",
    "print(\"False Positive rate: \",np.mean(FPOS) ,\"%\")\n",
    "print(\"False Negative rate: \",np.mean(FNEG) ,\"%\")\n",
    "print(\"F1 score: \",np.mean(fscore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5a6e09a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbfeatures=1\n",
    "\n",
    "data_matrix=np.zeros((nbfeatures,lg))\n",
    "\n",
    "\n",
    "# data_matrix[0]=NG_diff_mean_peaks_arr_flux\n",
    "# Accuracy for ON class:  26.75 %\n",
    "# Accuracy for OFF class:  70.09345794392524 %\n",
    "\n",
    "# data_matrix[0]=nb_peaks_arr_flux\n",
    "# Accuracy for ON class:  31.963203463203456 %\n",
    "# Accuracy for OFF class:  67.50933126425241 %\n",
    "\n",
    "# data_matrix[0]=peaks_time_delay_arr_flux\n",
    "# Accuracy for ON class:  27.4025974025974 %\n",
    "# Accuracy for OFF class:  80.40975514402771 %\n",
    "\n",
    "# data_matrix[0]=std_peaks_arr_flux\n",
    "# Accuracy for ON class:  26.75 %\n",
    "# Accuracy for OFF class:  70.27782586117362 %\n",
    "\n",
    "# data_matrix[0]=mean_peaks_arr_flux\n",
    "# Accuracy for ON class:  26.75 %\n",
    "# Accuracy for OFF class:  69.71333490347962 %\n",
    "\n",
    "# data_matrix[0]=max_diff_mean_peaks_arr_flux\n",
    "# Accuracy for ON class:  8.0 %\n",
    "# Accuracy for OFF class:  90.08198813300353 %\n",
    "\n",
    "# data_matrix[0]=min_diff_mean_peaks_arr_flux\n",
    "# Accuracy for ON class:  30.909090909090914 %\n",
    "# Accuracy for OFF class:  70.18348623853211 %\n",
    "\n",
    "# data_matrix[0]=nb_peaks_arr_photon\n",
    "# Accuracy for ON class:  48.1017316017316 %\n",
    "# Accuracy for OFF class:  48.44518257846103 %\n",
    "\n",
    "# data_matrix[0]=peaks_time_delay_arr_photon\n",
    "# Accuracy for ON class:  49.740620490620486 %\n",
    "# Accuracy for OFF class:  59.03121287237995 %\n",
    "\n",
    "# data_matrix[0]=std_peaks_arr_photon\n",
    "# Accuracy for ON class:  9.31060606060606 %\n",
    "# Accuracy for OFF class:  90.68022215603669 %\n",
    "\n",
    "# data_matrix[0]=mean_peaks_arr_photon\n",
    "# Accuracy for ON class:  14.19047619047619 %\n",
    "# Accuracy for OFF class:  92.45828502016062 %\n",
    "\n",
    "\n",
    "# data_matrix[0]=max_diff_mean_peaks_arr_photon\n",
    "# Accuracy for ON class:  3.7272727272727266 %\n",
    "# Accuracy for OFF class:  91.04562696945551 %\n",
    "\n",
    "# data_matrix[0]=min_diff_mean_peaks_arr_photon\n",
    "# Accuracy for ON class:  8.163419913419911 %\n",
    "# Accuracy for OFF class:  90.27475417186 %\n",
    "\n",
    "# data_matrix[0]=NG_diff_mean_peaks_arr_photon\n",
    "# Accuracy for ON class:  1.7424242424242422 %\n",
    "# Accuracy for OFF class:  92.46567222305951 %\n",
    "    \n",
    "data_matrix=data_matrix.reshape((lg,nbfeatures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "17ce73c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbfeatures=5\n",
    "\n",
    "data_matrix=np.zeros((nbfeatures,lg))\n",
    "\n",
    "data_matrix[0]=min_diff_mean_peaks_arr_flux\n",
    "data_matrix[1]=peaks_time_delay_arr_flux\n",
    "data_matrix[2]=nb_peaks_arr_flux\n",
    "data_matrix[3]=std_peaks_arr_flux\n",
    "data_matrix=data_matrix.reshape((lg,nbfeatures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad414f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "nbfeatures=8\n",
    "\n",
    "data_matrix=np.zeros((nbfeatures,lg))\n",
    "\n",
    "data_matrix[0]=std_peaks_arr_photon\n",
    "data_matrix[1]=NG_diff_mean_peaks_arr_photon\n",
    "data_matrix[2]=max_diff_mean_peaks_arr_photon\n",
    "data_matrix[3]=min_diff_mean_peaks_arr_photon\n",
    "\n",
    "data_matrix[4]=std_peaks_arr_flux\n",
    "data_matrix[5]=NG_diff_mean_peaks_arr_flux\n",
    "data_matrix[6]=max_diff_mean_peaks_arr_flux\n",
    "data_matrix[7]=min_diff_mean_peaks_arr_flux\n",
    "\n",
    "\n",
    "data_matrix=data_matrix.reshape((lg,nbfeatures))\n",
    "\n",
    "# 10 MC\n",
    "\n",
    "# Accuracy for ON class:  100.0 %\n",
    "# Accuracy for OFF class:  56.868194814724596 %\n",
    "# False Positive rate:  0.0 %\n",
    "# False Negative rate:  43.131805185275404 %\n",
    "# F1 score:  0.5168033220908891\n",
    "\n",
    "# 300 MC steps\n",
    "\n",
    "# Accuracy for ON class:  99.97222222222221 %\n",
    "# Accuracy for OFF class:  58.16069649236325 %\n",
    "# False Positive rate:  0.027777777777777773 %\n",
    "# False Negative rate:  41.839303507636735 %\n",
    "# F1 score:  0.5325707055521958"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3b7efac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "nbfeatures=8\n",
    "\n",
    "data_matrix=np.zeros((nbfeatures,lg))\n",
    "\n",
    "data_matrix[0]=nb_peaks_arr_photon\n",
    "data_matrix[1]=peaks_time_delay_arr_photon\n",
    "data_matrix[2]=std_peaks_arr_photon\n",
    "data_matrix[3]=mean_peaks_arr_photon\n",
    "\n",
    "data_matrix[4]=nb_peaks_arr_flux\n",
    "data_matrix[5]=peaks_time_delay_arr_flux\n",
    "data_matrix[6]=std_peaks_arr_flux\n",
    "data_matrix[7]=mean_peaks_arr_flux\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_matrix=data_matrix.reshape((lg,nbfeatures))\n",
    "\n",
    "# Accuracy for ON class:  94.93181818181817 %\n",
    "# Accuracy for OFF class:  82.54595549480776 %\n",
    "# False Positive rate:  5.068181818181818 %\n",
    "# False Negative rate:  17.454044505192243 %\n",
    "# F1 score:  0.8023587748899297"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cc2c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbfeatures=10\n",
    "\n",
    "data_matrix=np.zeros((nbfeatures,lg))\n",
    "\n",
    "data_matrix[0]=nb_peaks_arr_photon\n",
    "data_matrix[1]=peaks_time_delay_arr_photon\n",
    "data_matrix[2]=std_peaks_arr_photon\n",
    "data_matrix[3]=mean_peaks_arr_photon\n",
    "\n",
    "data_matrix[4]=nb_peaks_arr_flux\n",
    "data_matrix[5]=peaks_time_delay_arr_flux\n",
    "data_matrix[6]=std_peaks_arr_flux\n",
    "data_matrix[7]=mean_peaks_arr_flux\n",
    "\n",
    "data_matrix[8]=NG_diff_mean_peaks_arr_photon\n",
    "data_matrix[9]=NG_diff_mean_peaks_arr_flux\n",
    "\n",
    "\n",
    "data_matrix=data_matrix.reshape((lg,nbfeatures))\n",
    "\n",
    "# Accuracy for ON class:  89.80122655122655 %\n",
    "# Accuracy for OFF class:  77.222579721542 %\n",
    "# False Positive rate:  10.198773448773448 %\n",
    "# False Negative rate:  22.777420278458003 %\n",
    "# F1 score:  0.7353412123485507"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f30a08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nbfeatures=12\n",
    "\n",
    "data_matrix=np.zeros((nbfeatures,lg))\n",
    "\n",
    "data_matrix[0]=nb_peaks_arr_photon\n",
    "data_matrix[1]=peaks_time_delay_arr_photon\n",
    "data_matrix[2]=std_peaks_arr_photon\n",
    "data_matrix[3]=mean_peaks_arr_photon\n",
    "\n",
    "data_matrix[4]=nb_peaks_arr_flux\n",
    "data_matrix[5]=peaks_time_delay_arr_flux\n",
    "data_matrix[6]=std_peaks_arr_flux\n",
    "data_matrix[7]=mean_peaks_arr_flux\n",
    "\n",
    "data_matrix[8]=max_diff_mean_peaks_arr_photon\n",
    "data_matrix[9]=min_diff_mean_peaks_arr_photon\n",
    "\n",
    "data_matrix[10]=max_diff_mean_peaks_arr_flux\n",
    "data_matrix[11]=min_diff_mean_peaks_arr_flux\n",
    "\n",
    "data_matrix=data_matrix.reshape((lg,nbfeatures))\n",
    "\n",
    "# Accuracy for ON class:  100.0 %\n",
    "# Accuracy for OFF class:  74.90497932413209 %\n",
    "# False Positive rate:  0.0 %\n",
    "# False Negative rate:  25.09502067586792 %\n",
    "# F1 score:  0.7175846530763869"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "f38f830e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best performance ?\n",
    "\n",
    "nbfeatures=10\n",
    "\n",
    "data_matrix=np.zeros((nbfeatures,lg))\n",
    "\n",
    "data_matrix[0]=nb_peaks_arr_photon\n",
    "data_matrix[1]=peaks_time_delay_arr_photon\n",
    "data_matrix[2]=std_peaks_arr_photon\n",
    "data_matrix[3]=mean_peaks_arr_photon\n",
    "\n",
    "data_matrix[4]=nb_peaks_arr_flux\n",
    "data_matrix[5]=peaks_time_delay_arr_flux\n",
    "data_matrix[6]=std_peaks_arr_flux\n",
    "data_matrix[7]=mean_peaks_arr_flux\n",
    "\n",
    "data_matrix[8]=max_diff_mean_peaks_arr_photon\n",
    "data_matrix[9]=min_diff_mean_peaks_arr_photon\n",
    "\n",
    "\n",
    "data_matrix=data_matrix.reshape((lg,nbfeatures))\n",
    "\n",
    "# 10 MC steps\n",
    "\n",
    "# Accuracy for ON class:  95.95959595959596 %\n",
    "# Accuracy for OFF class:  98.38054141266277 %\n",
    "# False Positive rate:  4.04040404040404 %\n",
    "# False Negative rate:  1.6194585873372422 %\n",
    "# F1 score:  0.9810545428449524\n",
    "\n",
    "\n",
    "# 300 MC steps\n",
    "\n",
    "# Accuracy for ON class:  94.97461664152841 %\n",
    "# Accuracy for OFF class:  97.9965918395496 %\n",
    "# False Positive rate:  5.0253833584715935 %\n",
    "# False Negative rate:  2.0034081604504066 %\n",
    "# F1 score:  0.9761534399258989"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e12560d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nbfeatures=6\n",
    "\n",
    "data_matrix=np.zeros((nbfeatures,lg))\n",
    "\n",
    "data_matrix[0]=nb_peaks_arr_flux\n",
    "data_matrix[1]=peaks_time_delay_arr_flux\n",
    "data_matrix[2]=std_peaks_arr_flux\n",
    "data_matrix[3]=mean_peaks_arr_flux\n",
    "\n",
    "data_matrix[4]=max_diff_mean_peaks_arr_flux\n",
    "data_matrix[5]=min_diff_mean_peaks_arr_flux\n",
    "\n",
    "\n",
    "data_matrix=data_matrix.reshape((lg,nbfeatures))\n",
    "\n",
    "# 30 MC steps : \n",
    "\n",
    "# Accuracy for ON class:  98.9469696969697 %\n",
    "# Accuracy for OFF class:  40.384589109757464 %\n",
    "# False Positive rate:  1.0530303030303032 %\n",
    "# False Negative rate:  59.61541089024253 %\n",
    "# F1 score:  0.3578440452228621"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "5f2d3fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nbfeatures=4\n",
    "\n",
    "data_matrix=np.zeros((nbfeatures,lg))\n",
    "\n",
    "data_matrix[0]=nb_peaks_arr_flux\n",
    "data_matrix[1]=peaks_time_delay_arr_flux\n",
    "data_matrix[2]=std_peaks_arr_flux\n",
    "data_matrix[3]=mean_peaks_arr_flux\n",
    "\n",
    "# data_matrix[4]=max_diff_mean_peaks_arr_flux\n",
    "# data_matrix[5]=min_diff_mean_peaks_arr_flux\n",
    "\n",
    "\n",
    "data_matrix=data_matrix.reshape((lg,nbfeatures))\n",
    "\n",
    "# Accuracy for ON class:  93.3388370888371 %\n",
    "# Accuracy for OFF class:  55.171118790298614 %\n",
    "# False Positive rate:  6.661162911162912 %\n",
    "# False Negative rate:  44.828881209701386 %\n",
    "# F1 score:  0.4926499253078917"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "84b349e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n"
     ]
    }
   ],
   "source": [
    "data=pd.read_csv(dataON[5],index_col=[0])\n",
    "dataframe.columns=['MJD','Flux','Delta Flux','Photon Index','Delta Index','TS','fratio']\n",
    "\n",
    "mjd=np.array(dataframe['MJD'])\n",
    "flux=np.array(dataframe['Flux'])\n",
    "photon_idx=np.array(dataframe['Photon Index'])\n",
    "delta_index=np.array(dataframe['Delta Index'])\n",
    "delta_flux=np.array(dataframe['Delta Flux'])\n",
    "\n",
    "print(len(delta_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce49a38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
