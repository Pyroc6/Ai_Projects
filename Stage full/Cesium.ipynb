{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2669d8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import math\n",
    "from scipy import interpolate\n",
    "import sys \n",
    "from re import search\n",
    "from astropy.io import fits\n",
    "from astropy import units as u\n",
    "from astropy.time import Time\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.coordinates import ICRS, Galactic, FK4, FK5  # Low-level frames\n",
    "from astropy.coordinates import Angle, Latitude, Longitude\n",
    "import shutil\n",
    "from keras.callbacks import CSVLogger\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import glob\n",
    "from datetime import datetime\n",
    "from keras.layers import Conv1D, Dense, Dropout, Input, Concatenate, GlobalMaxPooling1D, BatchNormalization\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import dstack\n",
    "from pandas import read_csv\n",
    "from matplotlib import pyplot\n",
    "import tensorflow.keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten,LSTM\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Activation, Concatenate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ShuffleSplit # or StratifiedShuffleSplit\n",
    "from pandas import DataFrame\n",
    "from pyts.classification import BOSSVS\n",
    "from pyts.multivariate.classification import MultivariateClassifier\n",
    "import shutil\n",
    "from keras.callbacks import CSVLogger\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import dstack\n",
    "from pandas import read_csv\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ShuffleSplit # or StratifiedShuffleSplit\n",
    "from pandas import DataFrame\n",
    "import sktime\n",
    "from sktime.classification.distance_based import KNeighborsTimeSeriesClassifier\n",
    "import cesium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ef988fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathON=\"C:/Users/pierr/Documents_kanop/Shps/Pierre_points_for_louis/Bureau/interpo_ONOFF/ON_data/\"\n",
    "pathOFF=\"C:/Users/pierr/Documents_kanop/Shps/Pierre_points_for_louis/Bureau/interpo_ONOFF/OFF_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d02351a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing in a list the adresses of all the data files in the notebook\n",
    "\n",
    "data_files_ON=[]\n",
    "file_names_ON=[]\n",
    "\n",
    "data_files_OFF=[]\n",
    "file_names_OFF=[]\n",
    "data_files_ALL=[]\n",
    "file_names_ALL=[]\n",
    "\n",
    "#Storing in a list the adresses of all the data files in the notebook\n",
    "for filename in os.listdir(pathOFF):\n",
    "    f = os.path.join(pathOFF,filename)\n",
    "    if os.path.isfile(f):\n",
    "        data_files_OFF.append(f)\n",
    "        file_names_OFF.append(filename)\n",
    "        data_files_ALL.append(f)\n",
    "        file_names_ALL.append(filename)\n",
    "for filename in os.listdir(pathON):\n",
    "    f = os.path.join(pathON,filename)\n",
    "    if os.path.isfile(f):\n",
    "        data_files_ON.append(f)\n",
    "        file_names_ON.append(filename)\n",
    "        data_files_ALL.append(f)\n",
    "        file_names_ALL.append(filename)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6274a0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using data_files with only the 36 binning\n",
    "Filter=True\n",
    "binning=37\n",
    "\n",
    "\n",
    "if Filter==True:\n",
    "    \n",
    "    idx_OFF=[]\n",
    "    idx_ON=[]\n",
    "    dataON=[]\n",
    "    dataOFF=[]\n",
    "    for i in range(len(data_files_OFF)):\n",
    "        dataframe = pd.read_csv(data_files_OFF[i])\n",
    "        lg = len(dataframe)\n",
    "        \n",
    "        if lg==binning:\n",
    "            idx_OFF.append(i)\n",
    "    \n",
    "    for i in range(len(data_files_ON)):\n",
    "        dataframe = pd.read_csv(data_files_ON[i])\n",
    "        lg = len(dataframe)\n",
    "        if lg==binning:\n",
    "            idx_ON.append(i)\n",
    "\n",
    "\n",
    "    for i in range(len(idx_OFF)):\n",
    "\n",
    "        a=idx_OFF[i]\n",
    "        dataOFF.append(data_files_OFF[a])\n",
    "    for i in range(len(idx_ON)):\n",
    "\n",
    "        a=idx_ON[i]\n",
    "        dataON.append(data_files_ON[a])\n",
    "\n",
    "idx = idx_OFF+idx_ON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8050af6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "lgON=len(dataON)\n",
    "lgOFF=len(dataOFF)\n",
    "lg=lgON+lgOFF\n",
    "Labels = []\n",
    "\n",
    "nbfeatures=5\n",
    "\n",
    "a=binning\n",
    "b=nbfeatures\n",
    "c=lg\n",
    "\n",
    "# multivariate\n",
    "\n",
    "data_matrix= np.zeros((b,a))\n",
    "on=[]\n",
    "off=[]\n",
    "photon_idx=[]\n",
    "delta_index=[]\n",
    "flux=[]\n",
    "delta_flux=[]\n",
    "mjd=[]\n",
    "fratio=[]\n",
    "flux_idx=[]\n",
    "flux_error=[]\n",
    "#Construct data matrix\n",
    "for j in range(len(dataOFF)):\n",
    "\n",
    "    dataframe=pd.read_csv(dataOFF[j],index_col=[0])\n",
    "    dataframe.columns=['MJD','Flux','Delta Flux','Photon Index','Delta Index','TS','fratio']\n",
    "\n",
    "    photon_idx.append(np.array(dataframe['Photon Index']))\n",
    "    delta_index.append(np.array(dataframe['Delta Index']))\n",
    "    mjd.append(np.array(dataframe['MJD']))\n",
    "    flux.append(np.array(dataframe['Flux']))\n",
    "    delta_flux.append(np.array(dataframe['Delta Flux']))\n",
    "    flux_idx.append(np.array(dataframe['Flux'])*np.array(dataframe['Photon Index']))\n",
    "    fratio.append(np.array(dataframe['fratio']))\n",
    "    flux_error.append(np.array(dataframe['Delta Flux']))\n",
    "for j in range(len(dataON)):\n",
    "\n",
    "    v=j+len(dataOFF)\n",
    "    dataframe=pd.read_csv(dataON[j],index_col=[0])\n",
    "    dataframe.columns=['MJD','Flux','Delta Flux','Photon Index','Delta Index','TS','fratio']\n",
    "\n",
    "    photon_idx.append(np.array(dataframe['Photon Index']))\n",
    "    delta_index.append(np.array(dataframe['Delta Index']))\n",
    "    mjd.append(np.array(dataframe['MJD']))\n",
    "    flux.append(np.array(dataframe['Flux']))\n",
    "    flux_idx.append(np.array(dataframe['Flux'])*np.array(dataframe['Photon Index']))\n",
    "    fratio.append(np.array(dataframe['fratio']))\n",
    "    flux_error.append(np.array(dataframe['Delta Flux']))\n",
    "#Creating labels\n",
    "for i in range(lgOFF):\n",
    "    \n",
    "    Labels.append(int(0))\n",
    "    \n",
    "for i in range(lgON):\n",
    "    Labels.append(int(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3413d06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature       std amplitude max_slope\n",
      "channel         0         0         0\n",
      "0        0.235597  0.544947  0.008073\n",
      "1        0.612850  1.515464  0.020950\n",
      "2        0.530785  1.571777  0.019666\n",
      "3        0.304529  0.809423  0.009043\n",
      "4        0.760772  1.860631  0.021070\n",
      "...           ...       ...       ...\n",
      "1135     1.096254  2.499288  0.030368\n",
      "1136     0.252557  0.710988  0.008264\n",
      "1137     0.506804  1.582686  0.022016\n",
      "1138     1.118603  2.282258  0.021814\n",
      "1139     0.639641  2.078685  0.023031\n",
      "\n",
      "[1140 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cesium\n",
    "from cesium import featurize\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "features_to_use = [\n",
    "                  'std','amplitude','max_slope']\n",
    "# features_to_use = [\n",
    "#                   \"max_slope\",\n",
    "#                   \"med_err\",\"stetson_j\",\"std_err\"]\n",
    "fset_cesium = featurize.featurize_time_series(times=mjd,\n",
    "                                              values=photon_idx,\n",
    "                                              errors=delta_index,\n",
    "                                              features_to_use=features_to_use)\n",
    "print(fset_cesium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "bdf917c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature percent_difference_flux_percentile\n",
      "channel                                  0\n",
      "0                             2.331179e-08\n",
      "1                             2.632709e-08\n",
      "2                             1.639528e-08\n",
      "3                             1.006381e-08\n",
      "4                             2.119383e-08\n",
      "...                                    ...\n",
      "1135                          1.948568e-08\n",
      "1136                          1.483723e-08\n",
      "1137                          5.946276e-08\n",
      "1138                          2.156158e-07\n",
      "1139                          1.911282e-08\n",
      "\n",
      "[1140 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cesium\n",
    "from cesium import featurize\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "# features_to_use = [\n",
    "#                   'std','percent_amplitude','amplitude','max_slope']\n",
    "\n",
    "features_to_use = [\n",
    "                  'percent_difference_flux_percentile']\n",
    "fset_cesium = featurize.featurize_time_series(times=mjd,\n",
    "                                              values=flux, \n",
    "                                              errors=flux_error,\n",
    "                                              features_to_use=features_to_use)\n",
    "print(fset_cesium )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2c38698f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature       std percent_amplitude\n",
      "channel         0                 0\n",
      "0        0.813682          6.115137\n",
      "1        0.324348          0.738986\n",
      "2        0.320741          1.442672\n",
      "3        0.427394          3.434630\n",
      "4        0.659141          4.826915\n",
      "..            ...               ...\n",
      "119      0.283727          1.399212\n",
      "120      0.639092          7.111547\n",
      "121      0.972994          5.631823\n",
      "122      0.527917          2.911923\n",
      "123      0.310025          1.432305\n",
      "\n",
      "[124 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "features_to_use = [\n",
    "                  'std','percent_amplitude']\n",
    "fset_cesium = featurize.featurize_time_series(times=mjd,\n",
    "                                              values=photon_idx,\n",
    "                                              errors=delta_index,\n",
    "                                              features_to_use=features_to_use)\n",
    "print(fset_cesium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2aae1ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for ON class:  80.0 %\n",
      "Accuracy for OFF class:  20.651650708941556 %\n",
      "False Positive rate:  20.0 %\n",
      "False Negative rate:  79.34834929105844 %\n",
      "F1 score:  0.24236496509802313\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "ON_accuracy=[]\n",
    "OFF_accuracy=[]\n",
    "fscore=[]\n",
    "FPOS=[]\n",
    "FNEG= []\n",
    "bad_ON=[]\n",
    "iterations=5\n",
    "lgON=len(dataON)\n",
    "lgOFF=len(dataOFF)\n",
    "lg=lgON+lgOFF\n",
    "weight_for_0 = (1 / lgOFF) * (lg / 2.0)\n",
    "weight_for_1 = (1 / lgON) * (lg / 2.0)\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "for i in  range(iterations):\n",
    "                 \n",
    "    train, test = train_test_split(np.arange(lg), random_state=i)\n",
    "\n",
    "    model_cesium = RandomForestClassifier(n_estimators=100, max_features=\"auto\",\n",
    "                                          random_state=0,class_weight =class_weight)\n",
    "    Labels=np.hstack(Labels)\n",
    "    model_cesium.fit(fset_cesium.iloc[train], Labels[train])\n",
    "    prediction= model_cesium.predict(fset_cesium.iloc[test])\n",
    "    on_score=0\n",
    "    on_nbs=0\n",
    "    off_nbs=0\n",
    "    off_score=0\n",
    "    foff=0\n",
    "    fon=0\n",
    "    y_test=Labels[test]\n",
    "    for i in range(len(y_test)):\n",
    "        if y_test[i]==1 :\n",
    "            on_nbs+=1\n",
    "            if prediction[i]==1:\n",
    "                on_score+=1\n",
    "                \n",
    "            else : \n",
    "                fon+=1\n",
    "                bad_ON.append(i)\n",
    "        if y_test[i]==0 :\n",
    "            off_nbs+=1\n",
    "            if prediction[i]==0:\n",
    "                off_score+=1 \n",
    "            else:\n",
    "                foff+=1\n",
    "    if on_nbs>0:    \n",
    "        ON_accuracy.append(100*(on_score/on_nbs))\n",
    "        FPOS.append(100*(fon/on_nbs))\n",
    "    OFF_accuracy.append(100*(off_score/off_nbs))\n",
    "    FNEG.append(100*(foff/off_nbs))\n",
    "    f1= f1_score(prediction,y_test,average='weighted')\n",
    "    fscore.append(f1)\n",
    "print(\"Accuracy for ON class: \",np.mean(ON_accuracy) ,\"%\")\n",
    "print(\"Accuracy for OFF class: \",np.mean(OFF_accuracy) ,\"%\")\n",
    "print(\"False Positive rate: \",np.mean(FPOS) ,\"%\")\n",
    "print(\"False Negative rate: \",np.mean(FNEG) ,\"%\")\n",
    "print(\"F1 score: \",np.mean(fscore))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6b3bd4bc",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "gg = np.unique(bad_ON)\n",
    "bad_ON2=[]\n",
    "bad_ON3=[]\n",
    "if len(gg)>0:\n",
    "    for i in range(len(bad_ON)):\n",
    "        aa=bad_ON[i]\n",
    "        bad_ON2.append(file_names_ALL[aa])\n",
    "    gg = np.unique(bad_ON2)    \n",
    "    for i in range(len(gg)):\n",
    "        bb=gg[i]\n",
    "        count = bad_ON2.count(bb)\n",
    "        if count>1:\n",
    "            bad_ON3.append([bb,count])\n",
    "print(len(dataON))\n",
    "print(len(gg))\n",
    "print(bad_ON3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4b2c9a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(n_neighbors=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(n_neighbors=2)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=2)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(np.arange(lg), random_state=0)\n",
    "\n",
    "model_cesium = KNeighborsClassifier(2)\n",
    "Labels=np.hstack(Labels)\n",
    "model_cesium.fit(fset_cesium.iloc[train], Labels[train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8d5acb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for ON class:  0.0 %\n",
      "Accuracy for OFF class:  100.0 %\n",
      "False Positive rate:  100.0 %\n",
      "False Negative rate:  0.0 %\n",
      "F1 score:  0.983957219251337\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "train, test = train_test_split(np.arange(lg), random_state=0)\n",
    "ON_accuracy=[]\n",
    "OFF_accuracy=[]\n",
    "fscore=[]\n",
    "FPOS=[]\n",
    "FNEG= []\n",
    "bad_ON=[]\n",
    "iterations=5\n",
    "lgON=len(dataON)\n",
    "lgOFF=len(dataOFF)\n",
    "lg=lgON+lgOFF\n",
    "prediction = model_cesium.predict(fset_cesium.iloc[test])\n",
    "on_score=0\n",
    "on_nbs=0\n",
    "off_nbs=0\n",
    "off_score=0\n",
    "foff=0\n",
    "fon=0\n",
    "y_test=Labels[test]\n",
    "for i in range(len(y_test)):\n",
    "    if y_test[i]==1 :\n",
    "        on_nbs+=1\n",
    "        if prediction[i]==1:\n",
    "            on_score+=1\n",
    "\n",
    "        else : \n",
    "            fon+=1\n",
    "            bad_ON.append(i)\n",
    "    if y_test[i]==0 :\n",
    "        off_nbs+=1\n",
    "        if prediction[i]==0:\n",
    "            off_score+=1 \n",
    "        else:\n",
    "            foff+=1\n",
    "if on_nbs>0:    \n",
    "    ON_accuracy.append(100*(on_score/on_nbs))\n",
    "    FPOS.append(100*(fon/on_nbs))\n",
    "OFF_accuracy.append(100*(off_score/off_nbs))\n",
    "FNEG.append(100*(foff/off_nbs))\n",
    "f1= f1_score(prediction,y_test,average='weighted')\n",
    "fscore.append(f1)\n",
    "print(\"Accuracy for ON class: \",np.mean(ON_accuracy) ,\"%\")\n",
    "print(\"Accuracy for OFF class: \",np.mean(OFF_accuracy) ,\"%\")\n",
    "print(\"False Positive rate: \",np.mean(FPOS) ,\"%\")\n",
    "print(\"False Negative rate: \",np.mean(FNEG) ,\"%\")\n",
    "print(\"F1 score: \",np.mean(fscore))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
