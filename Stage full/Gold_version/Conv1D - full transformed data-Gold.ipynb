{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "931f53cf",
   "metadata": {},
   "source": [
    "# Neural networks code and tests notebook\n",
    "\n",
    "# This notebook takes the modified raw data and try to predict classes on  :\n",
    "\n",
    "# - Conv1D network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4954cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import math\n",
    "from scipy import interpolate\n",
    "import sys \n",
    "from re import search\n",
    "from astropy.io import fits\n",
    "from astropy import units as u\n",
    "from astropy.time import Time\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.coordinates import ICRS, Galactic, FK4, FK5  # Low-level frames\n",
    "from astropy.coordinates import Angle, Latitude, Longitude\n",
    "import shutil\n",
    "from keras.callbacks import CSVLogger\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import glob\n",
    "from datetime import datetime\n",
    "from keras.layers import Conv1D, Dense, Dropout, Input, Concatenate, GlobalMaxPooling1D, BatchNormalization\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import dstack\n",
    "from pandas import read_csv\n",
    "from matplotlib import pyplot\n",
    "import tensorflow.keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten,LSTM\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Activation, Concatenate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ShuffleSplit # or StratifiedShuffleSplit\n",
    "from pandas import DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da642277",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathON=\"C:/Users/pierr/Documents_kanop/Shps/Pierre_points_for_louis/Bureau/interpo_ONOFF_GOLD/ON_data/\"\n",
    "pathOFF=\"C:/Users/pierr/Documents_kanop/Shps/Pierre_points_for_louis/Bureau/interpo_ONOFF_GOLD/OFF_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7596790",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing in a list the adresses of all the data files in the notebook\n",
    "\n",
    "data_files_ON=[]\n",
    "file_names_ON=[]\n",
    "\n",
    "data_files_OFF=[]\n",
    "file_names_OFF=[]\n",
    "data_files_ALL=[]\n",
    "file_names_ALL=[]\n",
    "\n",
    "#Storing in a list the adresses of all the data files in the notebook\n",
    "for filename in os.listdir(pathOFF):\n",
    "    f = os.path.join(pathOFF,filename)\n",
    "    if os.path.isfile(f):\n",
    "        data_files_OFF.append(f)\n",
    "        file_names_OFF.append(filename)\n",
    "        data_files_ALL.append(f)\n",
    "        file_names_ALL.append(filename)\n",
    "for filename in os.listdir(pathON):\n",
    "    f = os.path.join(pathON,filename)\n",
    "    if os.path.isfile(f):\n",
    "        data_files_ON.append(f)\n",
    "        file_names_ON.append(filename)\n",
    "        data_files_ALL.append(f)\n",
    "        file_names_ALL.append(filename)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c01a285",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using data_files with only the 36 binning\n",
    "Filter=True\n",
    "binning=37\n",
    "\n",
    "\n",
    "if Filter==True:\n",
    "    \n",
    "    idx_OFF=[]\n",
    "    idx_ON=[]\n",
    "    dataON=[]\n",
    "    dataOFF=[]\n",
    "    for i in range(len(data_files_OFF)):\n",
    "        dataframe = pd.read_csv(data_files_OFF[i])\n",
    "        lg = len(dataframe)\n",
    "        \n",
    "        if lg==binning:\n",
    "            idx_OFF.append(i)\n",
    "    \n",
    "    for i in range(len(data_files_ON)):\n",
    "        dataframe = pd.read_csv(data_files_ON[i])\n",
    "        lg = len(dataframe)\n",
    "\n",
    "        if lg==binning:\n",
    "            idx_ON.append(i)\n",
    "\n",
    "    for i in range(len(idx_OFF)):\n",
    "\n",
    "        a=idx_OFF[i]\n",
    "        dataOFF.append(data_files_OFF[a])\n",
    "    for i in range(len(idx_ON)):\n",
    "\n",
    "        a=idx_ON[i]\n",
    "        dataON.append(data_files_ON[a])\n",
    "\n",
    "idx = idx_OFF+idx_ON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72b3edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature_time_series=['Iteration','MJD','Flux','Delta Flux','Photon Index','Delta Index','TS','fratio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba738f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "lgON=len(dataON)\n",
    "lgOFF=len(dataOFF)\n",
    "lg=lgON+lgOFF\n",
    "Class_ON = []\n",
    "\n",
    "nbfeatures=2\n",
    "\n",
    "a=binning\n",
    "b=nbfeatures\n",
    "c=lg\n",
    "\n",
    "# multivariate\n",
    "y=np.zeros((2,lg))\n",
    "bigdata= np.zeros((b,c,a))\n",
    "\n",
    "#Construct data matrix\n",
    "for j in range(len(dataOFF)):\n",
    "\n",
    "    dataframe=pd.read_csv(dataOFF[j])\n",
    "    dataframe.columns=['Iteration','MJD','Flux','Delta Flux','Photon Index','Delta Index','TS','fratio']\n",
    "\n",
    "    bigdata[0][j]=dataframe['Flux']\n",
    "    bigdata[1][j]=dataframe['fratio']\n",
    "    bigdata[2][j]=dataframe['Delta Flux']*dataframe['Flux']\n",
    "    bigdata[3][j]=dataframe['Photon Index']*dataframe['Flux']\n",
    "    bigdata[4][j]=dataframe['fratio']*dataframe['Photon Index']\n",
    "    bigdata[5][j]=dataframe['Photon Index']\n",
    "    bigdata[6][j]=abs(dataframe['Photon Index'])-dataframe['fratio']\n",
    "    bigdata[7][j]=np.var(dataframe['Photon Index'])*dataframe['Photon Index'] \n",
    "    bigdata[8][j]=np.var(dataframe['fratio'])*dataframe['fratio'] \n",
    "\n",
    "for j in range(len(dataON)):\n",
    "\n",
    "    v=j+len(dataOFF)\n",
    "    dataframe=pd.read_csv(dataON[j])\n",
    "    dataframe.columns=['Iteration','MJD','Flux','Delta Flux','Photon Index','Delta Index','TS','fratio']\n",
    "\n",
    "    bigdata[0][v]=dataframe['Flux']\n",
    "    bigdata[1][v]=dataframe['fratio']  \n",
    "    bigdata[2][v]=dataframe['Delta Flux']*dataframe['Flux']\n",
    "    bigdata[3][v]=dataframe['Photon Index']*dataframe['Flux']\n",
    "    bigdata[4][v]=dataframe['fratio']*dataframe['Photon Index']\n",
    "    bigdata[5][v]=dataframe['Photon Index']\n",
    "    bigdata[6][v]=abs(dataframe['Photon Index'])-dataframe['fratio']\n",
    "    bigdata[7][v]=np.var(dataframe['Photon Index'])*dataframe['Photon Index']\n",
    "    bigdata[8][v]=np.var(dataframe['fratio'])*dataframe['fratio'] \n",
    "    \n",
    "\n",
    "\n",
    "#Creating labels\n",
    "for i in range(lgOFF):\n",
    "    \n",
    "    Class_ON.append(int(0))\n",
    "    \n",
    "for i in range(lgON):\n",
    "    Class_ON.append(int(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649468fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshaping data matrix\n",
    "bigdata=bigdata.reshape(c,nbfeatures,binning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e16ea3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Conv1D architecture\n",
    "from keras.layers import GlobalAveragePooling1D\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.layers import Conv1D\n",
    "\n",
    "csv_logger = CSVLogger(model_history4, append=True)\n",
    "weight_for_0 = (1 / lgOFF) * (lg / 2.0)\n",
    "weight_for_1 = (1 / lgON) * (lg / 2.0)\n",
    "\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "bigdata=bigdata.reshape(c,nbfeatures,binning)\n",
    "nb_filters = 20\n",
    "fsize=1\n",
    "x_train, x_test, y_train, y_test = train_test_split(bigdata, Class_ON, test_size=0.3, random_state=38)\n",
    "y_test2=y_test.copy()\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(nb_filters, 1, padding=\"same\", activation=\"relu\",input_shape=(nbfeatures,binning)))\n",
    "model.add(Conv1D(15, 1, padding=\"same\", activation=\"relu\",input_shape=(nb_filters,binning)))\n",
    "model.add(Conv1D(9, 1, padding=\"same\", activation=\"relu\",input_shape=(15,binning)))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(binning, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train,epochs=160,batch_size=4,class_weight=class_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9a4a46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ON_accuracy=[]\n",
    "OFF_accuracy=[]\n",
    "\n",
    "#Measuring the meaned accuracy of  correct prediction of labellisation for each class  over a defined number of steps\n",
    "for i in range(1,35):\n",
    "    \n",
    "    #Conv1D Architecture\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(nb_filters, 1, padding=\"same\", activation=\"relu\",input_shape=(nbfeatures,binning)))\n",
    "    model.add(Conv1D(15, 1, padding=\"same\", activation=\"relu\",input_shape=(nb_filters,binning)))\n",
    "    model.add(Conv1D(9, 1, padding=\"same\", activation=\"relu\",input_shape=(15,binning)))\n",
    "\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "\n",
    "    model.add(Dense(binning, activation='relu'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['binary_accuracy'])\n",
    "\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(bigdata, Class_ON, test_size=0.33, random_state=i)\n",
    "    y_test2=y_test.copy()\n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n",
    "    \n",
    "    #fitting data\n",
    "    model.fit(x_train, y_train,epochs=70,batch_size=4,class_weight=class_weight)\n",
    "\n",
    "    #Obtain the accuradcy of prediction for each class\n",
    "    prediction= model.predict(x_test)\n",
    "    predicted_labels=[]\n",
    "    for i in range(len(prediction)):\n",
    "        if prediction[i][0]>0.5:\n",
    "            predicted_labels.append(0)\n",
    "        else:\n",
    "            predicted_labels.append(1)\n",
    "    on_score=0\n",
    "    on_nbs=0\n",
    "    off_nbs=0\n",
    "    off_score=0\n",
    "\n",
    "    for i in range(len(y_test)):\n",
    "        if y_test2[i]==1 :\n",
    "            on_nbs+=1\n",
    "            if predicted_labels[i]==1:\n",
    "                on_score+=1\n",
    "        if y_test2[i]==0 :\n",
    "            off_nbs+=1\n",
    "            if predicted_labels[i]==0:\n",
    "                off_score+=1 \n",
    "    if on_nbs>0:    \n",
    "        ON_accuracy.append(100*(on_score/on_nbs))\n",
    "    OFF_accuracy.append(100*(off_score/off_nbs))\n",
    "print(\"Accuracy for ON class: \",np.mean(ON_accuracy) ,\"%\")\n",
    "print(\"Accuracy for OFF class: \",np.mean(OFF_accuracy) ,\"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
