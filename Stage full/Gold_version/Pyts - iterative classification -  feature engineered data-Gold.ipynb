{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff1139a5",
   "metadata": {},
   "source": [
    "# PYTS/BOSSVS classification algorithm test and measured accuracies\n",
    "# Tested on transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06b1435d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import sys \n",
    "import shutil\n",
    "from keras.callbacks import CSVLogger\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import dstack\n",
    "from pandas import read_csv\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ShuffleSplit # or StratifiedShuffleSplit\n",
    "from pandas import DataFrame\n",
    "import sktime\n",
    "from sktime.classification.distance_based import KNeighborsTimeSeriesClassifier\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "527823ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathON=\"C:/Users/pierr/Documents_kanop/Shps/Pierre_points_for_louis/Bureau/interpo_ONOFF_GOLD/ON_data/\"\n",
    "pathOFF=\"C:/Users/pierr/Documents_kanop/Shps/Pierre_points_for_louis/Bureau/interpo_ONOFF_GOLD/OFF_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d6ba841",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing in a list the adresses of all the data files in the notebook\n",
    "\n",
    "data_files_ON=[]\n",
    "file_names_ON=[]\n",
    "\n",
    "data_files_OFF=[]\n",
    "file_names_OFF=[]\n",
    "data_files_ALL=[]\n",
    "file_names_ALL=[]\n",
    "\n",
    "#Storing in a list the adresses of all the data files in the notebook\n",
    "for filename in os.listdir(pathOFF):\n",
    "    f = os.path.join(pathOFF,filename)\n",
    "    if os.path.isfile(f):\n",
    "        data_files_OFF.append(f)\n",
    "        file_names_OFF.append(filename)\n",
    "        data_files_ALL.append(f)\n",
    "        file_names_ALL.append(filename)\n",
    "for filename in os.listdir(pathON):\n",
    "    f = os.path.join(pathON,filename)\n",
    "    if os.path.isfile(f):\n",
    "        data_files_ON.append(f)\n",
    "        file_names_ON.append(filename)\n",
    "        data_files_ALL.append(f)\n",
    "        file_names_ALL.append(filename)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e8e998f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using data_files with only the 36 binning\n",
    "Filter=True\n",
    "binning=37\n",
    "\n",
    "\n",
    "if Filter==True:\n",
    "    \n",
    "    idx_OFF=[]\n",
    "    idx_ON=[]\n",
    "    dataON=[]\n",
    "    dataOFF=[]\n",
    "    for i in range(len(data_files_OFF)):\n",
    "        dataframe = pd.read_csv(data_files_OFF[i])\n",
    "        lg = len(dataframe)\n",
    "        \n",
    "        if lg==binning:\n",
    "            idx_OFF.append(i)\n",
    "    \n",
    "    for i in range(len(data_files_ON)):\n",
    "        dataframe = pd.read_csv(data_files_ON[i])\n",
    "        lg = len(dataframe)\n",
    "\n",
    "        if lg==binning:\n",
    "            idx_ON.append(i)\n",
    "\n",
    "\n",
    "    for i in range(len(idx_OFF)):\n",
    "\n",
    "        a=idx_OFF[i]\n",
    "        dataOFF.append(data_files_OFF[a])\n",
    "    for i in range(len(idx_ON)):\n",
    "\n",
    "        a=idx_ON[i]\n",
    "        dataON.append(data_files_ON[a])\n",
    "\n",
    "idx = idx_OFF+idx_ON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9392c0f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1089, 3, 37)\n",
      "Accuracy for ON class:  100.0 %\n",
      "Accuracy for OFF class:  67.71792677308095 %\n",
      "False Positive rate:  32.28207322691904 %\n",
      "False Negative rate:  0.0 %\n",
      "F1 score:  0.7894862371209471\n",
      "[['4FGL J0932+5306.csv', 36], ['4FGL J0940-1335.csv', 37], ['4FGL J1018+3540.csv', 55], ['4FGL J1208+5441.csv', 37], ['4FGL J1253+6242.csv', 42], ['4FGL J1351+1115.csv', 30], ['4FGL J1418-0233.csv', 39], ['4FGL J1419+5423.csv', 50], ['4FGL J1445-0326.csv', 39], ['4FGL J1445-1626.csv', 32], ['4FGL J1448+3608.csv', 35], ['4FGL J1450+5201.csv', 39], ['4FGL J1451+6355.csv', 34], ['4FGL J1514-0949.csv', 31], ['4FGL J1517+6525.csv', 36], ['4FGL J1518+4044.csv', 42], ['4FGL J1520+5546.csv', 39], ['4FGL J1520-0348.csv', 50], ['4FGL J1522+3144.csv', 39], ['4FGL J1526-0831.csv', 33], ['4FGL J1530+5736.csv', 43], ['4FGL J1532-1319.csv', 38], ['4FGL J1533+1855.csv', 43], ['4FGL J1533+3416.csv', 46], ['4FGL J1534+3716.csv', 47], ['4FGL J1535+5320.csv', 41], ['4FGL J1539+2743.csv', 40], ['4FGL J1539-1127.csv', 38], ['4FGL J1540+1449.csv', 34], ['4FGL J1540+8155.csv', 39], ['4FGL J1543+0452.csv', 40], ['4FGL J1543+6130.csv', 42], ['4FGL J1544-0649.csv', 45], ['4FGL J1544-1126.csv', 43], ['4FGL J1546+0819.csv', 37], ['4FGL J1546-1003.csv', 35], ['4FGL J1548+1456.csv', 42], ['4FGL J1549+0236.csv', 42], ['4FGL J1549-0659.csv', 38], ['4FGL J1550+0528.csv', 43], ['4FGL J1552+0850.csv', 51], ['4FGL J1553+1257.csv', 38], ['4FGL J1555+1111.csv', 38], ['4FGL J1558+5625.csv', 43], ['4FGL J1559+2319.csv', 38], ['4FGL J1600+8510.csv', 38], ['4FGL J1602+3324.csv', 50], ['4FGL J1603+5009.csv', 43], ['4FGL J1604+5714.csv', 42], ['4FGL J1607+1550.csv', 39], ['4FGL J1608+1029.csv', 40], ['4FGL J1608-2038.csv', 41], ['4FGL J1613+3411.csv', 41], ['4FGL J1615+2130.csv', 40], ['4FGL J1615+4712.csv', 43], ['4FGL J1619+5536.csv', 42], ['4FGL J1619+7536.csv', 33], ['4FGL J1621-1103.csv', 40], ['4FGL J1625+4134.csv', 30], ['4FGL J1628+7706.csv', 41], ['4FGL J1630+5221.csv', 42], ['4FGL J1630+8234.csv', 46], ['4FGL J1631+1046.csv', 44], ['4FGL J1632+0854.csv', 34], ['4FGL J1635+3628.csv', 35], ['4FGL J1635+3808.csv', 41], ['4FGL J1636+2627.csv', 47], ['4FGL J1637+4717.csv', 44], ['4FGL J1638+5721.csv', 44], ['4FGL J1639+4129.csv', 43], ['4FGL J1640+1143.csv', 56], ['4FGL J1640+3945.csv', 42], ['4FGL J1640+6850.csv', 42], ['4FGL J1641-0621.csv', 41], ['4FGL J1642+3948.csv', 31], ['4FGL J1645+6329.csv', 35], ['4FGL J1647+4950.csv', 49], ['4FGL J1648+4232.csv', 47], ['4FGL J1649+5235.csv', 39], ['4FGL J1650+0831.csv', 49], ['4FGL J1651+7219.csv', 46], ['4FGL J1652+4024.csv', 36], ['4FGL J1653+3945.csv', 46], ['4FGL J1656+2047.csv', 43], ['4FGL J1656-2010.csv', 34], ['4FGL J1657+4808.csv', 32], ['4FGL J1657+6010.csv', 37], ['4FGL J1701+3956.csv', 36], ['4FGL J1701+6613.csv', 42], ['4FGL J1702+3114.csv', 36], ['4FGL J1704+7647.csv', 35], ['4FGL J1704-0527.csv', 54], ['4FGL J1705+5436.csv', 40], ['4FGL J1707+1649.csv', 33], ['4FGL J1712+2932.csv', 46], ['4FGL J1716+6836.csv', 48], ['4FGL J1719+1205.csv', 46], ['4FGL J1719+1745.csv', 43], ['4FGL J1722+1014.csv', 42], ['4FGL J1725+1152.csv', 31], ['4FGL J1725+5851.csv', 35], ['4FGL J1727+4530.csv', 38], ['4FGL J1728+0427.csv', 44], ['4FGL J1728+1216.csv', 39], ['4FGL J1728+5013.csv', 39], ['4FGL J1730+0024.csv', 32], ['4FGL J1730+3715.csv', 35], ['4FGL J1736+2033.csv', 38], ['4FGL J1739+4955.csv', 43], ['4FGL J1740+4737.csv', 36], ['4FGL J1740+5211.csv', 36], ['4FGL J1740+5346.csv', 35], ['4FGL J1742+5944.csv', 48], ['4FGL J1744+1935.csv', 38], ['4FGL J1748+3403.csv', 30], ['4FGL J1748+7005.csv', 41], ['4FGL J1749+4321.csv', 43], ['4FGL J1751+2921.csv', 32], ['4FGL J1754+3212.csv', 33], ['4FGL J1756+5522.csv', 38], ['4FGL J1800+7828.csv', 43], ['4FGL J1801+4404.csv', 36], ['4FGL J1806+6949.csv', 33], ['4FGL J1808+3500.csv', 45], ['4FGL J1809+2042.csv', 48], ['4FGL J1809+2910.csv', 43], ['4FGL J1810+5335.csv', 46], ['4FGL J1813+3144.csv', 38], ['4FGL J1818+0903.csv', 45], ['4FGL J1820+3624.csv', 37], ['4FGL J1821+6819.csv', 34], ['4FGL J1823+6858.csv', 41], ['4FGL J1824+5651.csv', 34], ['4FGL J1828+3230.csv', 36], ['4FGL J1829+4845.csv', 39], ['4FGL J1829+5402.csv', 44], ['4FGL J1830+0617.csv', 42], ['4FGL J1830+1324.csv', 38], ['4FGL J1833-2103.csv', 40], ['4FGL J1836+1948.csv', 32], ['4FGL J1836+3137.csv', 43], ['4FGL J1837+5347.csv', 34], ['4FGL J1838+4802.csv', 46], ['4FGL J1841+2909.csv', 44], ['4FGL J1841+3218.csv', 48], ['4FGL J1841+6115.csv', 42], ['4FGL J1842+6810.csv', 49], ['4FGL J1844+1547.csv', 51], ['4FGL J1844+5709.csv', 39], ['4FGL J1848+3217.csv', 40], ['4FGL J1848+3243.csv', 42], ['4FGL J1848+4247.csv', 34], ['4FGL J1849+6705.csv', 40], ['4FGL J1903+5540.csv', 37], ['4FGL J1904+3627.csv', 33], ['4FGL J1911-1908.csv', 48], ['4FGL J1911-2006.csv', 33], ['4FGL J1913+4439.csv', 39], ['4FGL J1917-1921.csv', 52], ['4FGL J1921-1231.csv', 30], ['4FGL J1921-1607.csv', 35], ['4FGL J1923-2104.csv', 29], ['4FGL J1925+1227.csv', 30], ['4FGL J1925+2815.csv', 46], ['4FGL J1926+6154.csv', 40], ['4FGL J1927+6117.csv', 39], ['4FGL J1931+0937.csv', 47], ['4FGL J1933+0726.csv', 38], ['4FGL J1934+6002.csv', 34], ['4FGL J1934+6541.csv', 38], ['4FGL J1942+1033.csv', 29], ['4FGL J1944+2117.csv', 36], ['4FGL J1944+3921.csv', 37], ['4FGL J1944-2143.csv', 33], ['4FGL J1949+0906.csv', 48], ['4FGL J1949+1314.csv', 35], ['4FGL J1950+1211.csv', 37], ['4FGL J1954-1122.csv', 52], ['4FGL J1955+0214.csv', 45], ['4FGL J1955+1358.csv', 42], ['4FGL J1955-1604.csv', 39], ['4FGL J1959+3844.csv', 46], ['4FGL J2000+6508.csv', 35], ['4FGL J2000-1328.csv', 45], ['4FGL J2001+4353.csv', 42], ['4FGL J2001+7040.csv', 37], ['4FGL J2005+6424.csv', 42], ['4FGL J2005+7003.csv', 41], ['4FGL J2005+7752.csv', 42], ['4FGL J2007+6607.csv', 34], ['4FGL J2010+7229.csv', 35], ['4FGL J2012+4629.csv', 33], ['4FGL J2012-1646.csv', 37], ['4FGL J2014+0648.csv', 32], ['4FGL J2014-0047.csv', 43], ['4FGL J2015+3710.csv', 32], ['4FGL J2015-0137.csv', 33], ['4FGL J2016-0903.csv', 42], ['4FGL J2018+3852.csv', 44], ['4FGL J2021+0629.csv', 44], ['4FGL J2022+7612.csv', 49], ['4FGL J2023+3153.csv', 42], ['4FGL J2023-0123.csv', 40], ['4FGL J2023-1139.csv', 32], ['4FGL J2024-0847.csv', 50], ['4FGL J2025+0317.csv', 46], ['4FGL J2025+3341.csv', 40], ['4FGL J2025-0735.csv', 33], ['4FGL J2029+4925.csv', 45], ['4FGL J2032+1219.csv', 40], ['4FGL J2034+1154.csv', 36], ['4FGL J2035+1056.csv', 41], ['4FGL J2035+4901.csv', 37], ['4FGL J2036+6553.csv', 35], ['4FGL J2039+5218.csv', 42], ['4FGL J2039-1046.csv', 35], ['4FGL J2040-1705.csv', 46], ['4FGL J2050+0408.csv', 36], ['4FGL J2053+2922.csv', 44], ['4FGL J2055-0020.csv', 34], ['4FGL J2056+4939.csv', 38], ['4FGL J2056-0202.csv', 43], ['4FGL J2102+4702.csv', 39], ['4FGL J2104-0212.csv', 34], ['4FGL J2108+3655.csv', 41], ['4FGL J2108-0250.csv', 49], ['4FGL J2110+0808.csv', 35], ['4FGL J2112+0819.csv', 47], ['4FGL J2115+1218.csv', 42], ['4FGL J2115-0113.csv', 38], ['4FGL J2119-1105.csv', 42], ['4FGL J2121+1901.csv', 50], ['4FGL J2123+0535.csv', 41], ['4FGL J2131-0916.csv', 44], ['4FGL J2133+6646.csv', 36], ['4FGL J2134-0154.csv', 47], ['4FGL J2134-2130.csv', 44], ['4FGL J2143+1743.csv', 41], ['4FGL J2146-1344.csv', 32], ['4FGL J2147+0931.csv', 33], ['4FGL J2148-0733.csv', 45], ['4FGL J2149+0323.csv', 40], ['4FGL J2149+1917.csv', 39], ['4FGL J2150-1410.csv', 47], ['4FGL J2151+4156.csv', 34], ['4FGL J2153-1137.csv', 49], ['4FGL J2156+1818.csv', 44], ['4FGL J2156-0036.csv', 43], ['4FGL J2157+3127.csv', 36], ['4FGL J2158-1501.csv', 43], ['4FGL J2200+2138.csv', 42], ['4FGL J2202+4216.csv', 37], ['4FGL J2203+1725.csv', 48], ['4FGL J2204+0438.csv', 42], ['4FGL J2206-0032.csv', 48], ['4FGL J2207+4316.csv', 47], ['4FGL J2209-0451.csv', 31], ['4FGL J2211-1325.csv', 39], ['4FGL J2212+2356.csv', 53], ['4FGL J2212+2800.csv', 42], ['4FGL J2216+2421.csv', 47], ['4FGL J2219+1806.csv', 33], ['4FGL J2219-0342.csv', 42], ['4FGL J2225-0457.csv', 40], ['4FGL J2227+0036.csv', 35], ['4FGL J2228-1636.csv', 42], ['4FGL J2229-0832.csv', 32], ['4FGL J2232+1143.csv', 33], ['4FGL J2236+2828.csv', 43], ['4FGL J2236-1433.csv', 37], ['4FGL J2236-1706.csv', 44], ['4FGL J2241+4120.csv', 44], ['4FGL J2243-1231.csv', 39], ['4FGL J2244+4057.csv', 35], ['4FGL J2245+1544.csv', 36], ['4FGL J2247+4413.csv', 44], ['4FGL J2247-0001.csv', 35], ['4FGL J2248+2106.csv', 42], ['4FGL J2250+3825.csv', 50], ['4FGL J2250-1250.csv', 42], ['4FGL J2252+4031.csv', 33], ['4FGL J2253+1609.csv', 41], ['4FGL J2255+2411.csv', 44], ['4FGL J2256-2011.csv', 49], ['4FGL J2259-1552.csv', 37], ['4FGL J2300+3136.csv', 40], ['4FGL J2301-0158.csv', 50], ['4FGL J2304+3704.csv', 45], ['4FGL J2311+0205.csv', 53], ['4FGL J2311+2604.csv', 32], ['4FGL J2311+3425.csv', 38], ['4FGL J2313+3945.csv', 35], ['4FGL J2314+1445.csv', 46], ['4FGL J2318+1915.csv', 45], ['4FGL J2321+5111.csv', 40], ['4FGL J2329+6101.csv', 36], ['4FGL J2347+5141.csv', 43], ['4FGL J2347+5436.csv', 33], ['4FGL J2352+1750.csv', 34], ['4FGL J2356+4036.csv', 44], ['4FGL J2357-1718.csv', 45], ['4FGL J2358+3830.csv', 34], ['4FGL J2358-1021.csv', 45], ['4FGL J2358-1808.csv', 40], ['4FGL J2359-2049.csv', 43], ['4FGLJ0001+2113.csv', 42], ['4FGLJ0001-0747.csv', 44], ['4FGLJ0003-1149.csv', 45], ['4FGLJ0003-1928.csv', 42], ['4FGLJ0005+3824.csv', 40], ['4FGLJ0007+4008.csv', 38], ['4FGLJ0008+1455.csv', 50], ['4FGLJ0008+4711.csv', 50], ['4FGLJ0009+0628.csv', 38], ['4FGLJ0009+5030.csv', 36], ['4FGLJ0011+0057.csv', 29], ['4FGLJ0013-1854.csv', 44], ['4FGLJ0014+3212.csv', 40], ['4FGLJ0014+6118.csv', 46], ['4FGLJ0014-0500.csv', 36], ['4FGLJ0015+5551.csv', 36], ['4FGLJ0016-0016.csv', 42], ['4FGLJ0017-0514.csv', 30], ['4FGLJ0017-0649.csv', 38], ['4FGLJ0018+2946.csv', 45], ['4FGLJ0019+2022.csv', 34], ['4FGLJ0019+7327.csv', 49], ['4FGLJ0022+0608.csv', 42], ['4FGLJ0022-1854.csv', 39], ['4FGLJ0023+4457.csv', 45], ['4FGLJ0028+2001.csv', 36], ['4FGLJ0028+7505.csv', 37], ['4FGLJ0030-0212.csv', 32], ['4FGLJ0030-1647.csv', 36], ['4FGLJ0033-1921.csv', 29], ['4FGLJ0152+0147.csv', 46], ['4FGLJ0206-1151.csv', 44], ['4FGLJ0256+0334.csv', 43], ['4FGLJ0259+0746.csv', 43], ['4FGLJ0308+0407.csv', 44], ['4FGLJ0309+1029.csv', 39], ['4FGLJ0312+0134.csv', 37], ['4FGLJ0314+0620.csv', 46], ['4FGLJ0316+0905.csv', 38], ['4FGLJ0330+0438.csv', 49], ['4FGLJ0409-0359.csv', 35], ['4FGLJ0505+0415.csv', 35], ['4FGLJ0505+0459.csv', 49], ['4FGLJ0509+0542.csv', 40], ['4FGLJ0805-0110.csv', 32], ['4FGLJ0922+0434.csv', 35], ['4FGLJ1103+1157.csv', 36], ['4FGLJ1107+1501.csv', 31], ['4FGLJ1110-1836.csv', 35], ['4FGLJ1125-2101.csv', 47], ['4FGLJ1534+0131.csv', 36], ['4FGLJ1644+2620.csv', 37], ['4FGLJ1702+2642.csv', 33], ['4FGLJ1709+4318.csv', 47], ['4FGLJ1724+4005.csv', 50], ['4FGLJ1734+3858.csv', 41], ['4FGLJ1736+0628.csv', 29], ['4FGLJ1751+0938.csv', 45], ['4FGLJ2049+1002.csv', 31], ['4FGLJ2108+1434.csv', 42]]\n"
     ]
    }
   ],
   "source": [
    "from pyts.classification import BOSSVS\n",
    "from pyts.multivariate.classification import MultivariateClassifier\n",
    "\n",
    "iteration=200\n",
    "lgON=len(dataON)\n",
    "lgOFF=len(dataOFF)\n",
    "lg=lgON+lgOFF\n",
    "\n",
    "binning=37\n",
    "\n",
    "Labels = []\n",
    "Q_ON= []\n",
    "\n",
    "nbfeatures=3\n",
    "ON_accuracy=[]\n",
    "OFF_accuracy=[]\n",
    "fscore=[]\n",
    "FPOS=[]\n",
    "FNEG= []\n",
    "a=binning\n",
    "b=nbfeatures\n",
    "c=len(data_files_ALL)\n",
    "\n",
    "# multivariate\n",
    "y=np.zeros((2,lg))\n",
    "bigdata= np.zeros((b,c,a))\n",
    "\n",
    "for j in range(len(data_files_ALL)):\n",
    "\n",
    "\n",
    "    dataframe=pd.read_csv(data_files_ALL[j],index_col=[0])\n",
    "    dataframe.columns=['MJD','Flux','Delta Flux','Photon Index','Delta Index','TS','fratio']\n",
    "\n",
    "    bigdata[0][j]=dataframe['Flux']\n",
    "    bigdata[1][j]=dataframe['Photon Index']*dataframe['Flux']\n",
    "    bigdata[2][j]=dataframe['Photon Index']\n",
    "\n",
    "for i in range(lgOFF):\n",
    "    \n",
    "    Labels.append(int(0))\n",
    "    \n",
    "for i in range(lgON): \n",
    "    \n",
    "    \n",
    "    Labels.append(int(1))\n",
    "    \n",
    "    \n",
    "bigdata=bigdata.reshape(c,nbfeatures,binning)    \n",
    "print(bigdata.shape)\n",
    "for i in range(iteration):\n",
    "    \n",
    "    #3D shape of data, multivariate time series\n",
    "\n",
    "    sss = ShuffleSplit(n_splits=1, test_size=0.2,random_state=i)\n",
    "    X=bigdata\n",
    "    y=Labels\n",
    "\n",
    "    sss.get_n_splits(X, y)\n",
    "    train_index, test_index = next(sss.split(X, y)) \n",
    "\n",
    "    x_train, x_test = X[train_index], X[test_index] \n",
    "\n",
    "    y_train=[]\n",
    "    y_test=[]\n",
    "\n",
    "    for i in range(len(train_index)):\n",
    "        a=train_index[i]\n",
    "        y_train.append(y[a])\n",
    "    for i in range(len(test_index)):\n",
    "\n",
    "        a=test_index[i]\n",
    "        y_test.append(y[a])\n",
    "\n",
    "    y_test2=y_test.copy()\n",
    "\n",
    "    clf = MultivariateClassifier(BOSSVS(n_bins=5,anova=True,word_size=9,drop_sum=True))\n",
    "    clf.fit(x_train, y_train)\n",
    "    MultivariateClassifier(...)\n",
    "    clf.predict(x_test)\n",
    "\n",
    "    prediction=np.array(clf.predict(x_test))\n",
    "    on_score=0\n",
    "    on_nbs=0\n",
    "    off_nbs=0\n",
    "    off_score=0\n",
    "    foff=0\n",
    "    fon=0\n",
    "    for i in range(len(y_test)):\n",
    "        if y_test2[i]==1 :\n",
    "            on_nbs+=1\n",
    "            if prediction[i]==1:\n",
    "                on_score+=1\n",
    "                a=test_index[i]\n",
    "                Q_ON.append(a)\n",
    "            else : \n",
    "                fon+=1\n",
    "        if y_test2[i]==0 :\n",
    "            off_nbs+=1\n",
    "            if prediction[i]==0:\n",
    "                off_score+=1 \n",
    "            else:\n",
    "                foff+=1\n",
    "                a=test_index[i]\n",
    "                Q_ON.append(a)\n",
    "    if on_nbs>0:    \n",
    "        ON_accuracy.append(100*(on_score/on_nbs))\n",
    "        FPOS.append(100*(fon/on_nbs))\n",
    "    OFF_accuracy.append(100*(off_score/off_nbs))\n",
    "    FNEG.append(100*(foff/off_nbs))\n",
    "    f1= f1_score(y_test2,prediction,average='weighted')\n",
    "    fscore.append(f1)\n",
    "    \n",
    "print(\"Accuracy for ON class: \",np.mean(ON_accuracy) ,\"%\")\n",
    "print(\"Accuracy for OFF class: \",np.mean(OFF_accuracy) ,\"%\")\n",
    "print(\"False Positive rate: \",np.mean(FNEG) ,\"%\")\n",
    "print(\"False Negative rate: \",np.mean(FPOS) ,\"%\")\n",
    "print(\"F1 score: \",np.mean(fscore))\n",
    "\n",
    "\n",
    "\n",
    "gg = np.unique(Q_ON)\n",
    "Q_ON2=[]\n",
    "Q_ON3=[]\n",
    "errOFF=[]\n",
    "\n",
    "\n",
    "\n",
    "if len(gg)>0:\n",
    "    for i in range(len(Q_ON)):\n",
    "        aa=Q_ON[i]\n",
    "        Q_ON2.append(file_names_ALL[aa])\n",
    "    gg = np.unique(Q_ON2)    \n",
    "    for i in range(len(gg)):\n",
    "        bb=gg[i]\n",
    "        count = Q_ON2.count(bb)\n",
    "        if count>int(iteration/7):\n",
    "            Q_ON3.append([bb,count])\n",
    "            errOFF.append(bb)\n",
    "print(Q_ON3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b8a8771",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing in a list the adresses of all the data files in the notebook\n",
    "\n",
    "data_files_ON=[]\n",
    "file_names_ON=[]\n",
    "\n",
    "data_files_OFF=[]\n",
    "file_names_OFF=[]\n",
    "data_files_ALL=[]\n",
    "file_names_ALL=[]\n",
    "\n",
    "#Storing in a list the adresses of all the data files in the notebook\n",
    "for filename in os.listdir(pathOFF):\n",
    "    f = os.path.join(pathOFF,filename)\n",
    "    if os.path.isfile(f) and filename in errOFF :\n",
    "        data_files_OFF.append(f)\n",
    "        file_names_OFF.append(filename)\n",
    "        data_files_ALL.append(f)\n",
    "        file_names_ALL.append(filename)\n",
    "        \n",
    "for filename in os.listdir(pathON):\n",
    "    f = os.path.join(pathON,filename)\n",
    "    if os.path.isfile(f):\n",
    "        data_files_ON.append(f)\n",
    "        file_names_ON.append(filename)\n",
    "        data_files_ALL.append(f)\n",
    "        file_names_ALL.append(filename)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9033fed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using data_files with only the 36 binning\n",
    "Filter=True\n",
    "binning=37\n",
    "\n",
    "\n",
    "if Filter==True:\n",
    "    \n",
    "    idx_OFF=[]\n",
    "    idx_ON=[]\n",
    "    dataON=[]\n",
    "    dataOFF=[]\n",
    "    for i in range(len(data_files_OFF)):\n",
    "        dataframe = pd.read_csv(data_files_OFF[i])\n",
    "        lg = len(dataframe)\n",
    "        \n",
    "        if lg==binning:\n",
    "            idx_OFF.append(i)\n",
    "    \n",
    "    for i in range(len(data_files_ON)):\n",
    "        dataframe = pd.read_csv(data_files_ON[i])\n",
    "        lg = len(dataframe)\n",
    "\n",
    "        if lg==binning:\n",
    "            idx_ON.append(i)\n",
    "\n",
    "\n",
    "    for i in range(len(idx_OFF)):\n",
    "\n",
    "        a=idx_OFF[i]\n",
    "        dataOFF.append(data_files_OFF[a])\n",
    "    for i in range(len(idx_ON)):\n",
    "\n",
    "        a=idx_ON[i]\n",
    "        dataON.append(data_files_ON[a])\n",
    "\n",
    "idx = idx_OFF+idx_ON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2421f965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(365, 3, 37)\n",
      "Accuracy for ON class:  100.0 %\n",
      "Accuracy for OFF class:  71.70899707230845 %\n",
      "False Positive rate:  28.291002927691558 %\n",
      "False Negative rate:  0.0 %\n",
      "F1 score:  0.7995841887509093\n",
      "73\n",
      "100\n",
      "[['4FGL J1707+1649.csv', 30], ['4FGL J2034+1154.csv', 34], ['4FGL J2150-1410.csv', 45], ['4FGL J2151+4156.csv', 28], ['4FGL J2153-1137.csv', 47], ['4FGL J2156+1818.csv', 42], ['4FGL J2156-0036.csv', 44], ['4FGL J2157+3127.csv', 39], ['4FGL J2158-1501.csv', 47], ['4FGL J2200+2138.csv', 29], ['4FGL J2202+4216.csv', 43], ['4FGL J2203+1725.csv', 32], ['4FGL J2204+0438.csv', 38], ['4FGL J2206-0032.csv', 39], ['4FGL J2207+4316.csv', 44], ['4FGL J2209-0451.csv', 38], ['4FGL J2211-1325.csv', 39], ['4FGL J2212+2356.csv', 35], ['4FGL J2212+2800.csv', 39], ['4FGL J2216+2421.csv', 37], ['4FGL J2219+1806.csv', 40], ['4FGL J2219-0342.csv', 41], ['4FGL J2225-0457.csv', 45], ['4FGL J2227+0036.csv', 37], ['4FGL J2228-1636.csv', 49], ['4FGL J2229-0832.csv', 37], ['4FGL J2232+1143.csv', 51], ['4FGL J2236+2828.csv', 36], ['4FGL J2236-1433.csv', 43], ['4FGL J2236-1706.csv', 45], ['4FGL J2241+4120.csv', 42], ['4FGL J2243-1231.csv', 39], ['4FGL J2244+4057.csv', 38], ['4FGL J2245+1544.csv', 42], ['4FGL J2247+4413.csv', 28], ['4FGL J2247-0001.csv', 57], ['4FGL J2248+2106.csv', 40], ['4FGL J2250+3825.csv', 46], ['4FGL J2250-1250.csv', 39], ['4FGL J2252+4031.csv', 47], ['4FGL J2253+1609.csv', 48], ['4FGL J2255+2411.csv', 41], ['4FGL J2256-2011.csv', 40], ['4FGL J2259-1552.csv', 35], ['4FGL J2300+3136.csv', 30], ['4FGL J2301-0158.csv', 37], ['4FGL J2304+3704.csv', 46], ['4FGL J2311+0205.csv', 33], ['4FGL J2311+2604.csv', 39], ['4FGL J2311+3425.csv', 26], ['4FGL J2313+3945.csv', 39], ['4FGL J2314+1445.csv', 43], ['4FGL J2318+1915.csv', 49], ['4FGL J2321+5111.csv', 42], ['4FGL J2329+6101.csv', 32], ['4FGL J2347+5141.csv', 32], ['4FGL J2347+5436.csv', 37], ['4FGL J2352+1750.csv', 36], ['4FGL J2356+4036.csv', 42], ['4FGL J2357-1718.csv', 39], ['4FGL J2358+3830.csv', 51], ['4FGL J2358-1021.csv', 41], ['4FGL J2358-1808.csv', 44], ['4FGL J2359-2049.csv', 36], ['4FGLJ0001+2113.csv', 44], ['4FGLJ0001-0747.csv', 35], ['4FGLJ0003-1149.csv', 52], ['4FGLJ0003-1928.csv', 39], ['4FGLJ0005+3824.csv', 39], ['4FGLJ0007+4008.csv', 48], ['4FGLJ0008+1455.csv', 35], ['4FGLJ0008+4711.csv', 40], ['4FGLJ0009+0628.csv', 33], ['4FGLJ0009+5030.csv', 42], ['4FGLJ0011+0057.csv', 43], ['4FGLJ0013-1854.csv', 35], ['4FGLJ0014+3212.csv', 42], ['4FGLJ0014+6118.csv', 38], ['4FGLJ0014-0500.csv', 37], ['4FGLJ0015+5551.csv', 47], ['4FGLJ0016-0016.csv', 29], ['4FGLJ0017-0514.csv', 40], ['4FGLJ0017-0649.csv', 43], ['4FGLJ0018+2946.csv', 40], ['4FGLJ0019+2022.csv', 36], ['4FGLJ0019+7327.csv', 49], ['4FGLJ0022+0608.csv', 38], ['4FGLJ0022-1854.csv', 32], ['4FGLJ0023+4457.csv', 37], ['4FGLJ0028+2001.csv', 41], ['4FGLJ0028+7505.csv', 42], ['4FGLJ0030-0212.csv', 36], ['4FGLJ0030-1647.csv', 41], ['4FGLJ0033-1921.csv', 34]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyts.classification import BOSSVS\n",
    "from pyts.multivariate.classification import MultivariateClassifier\n",
    "\n",
    "iteration=200\n",
    "lgON=len(dataON)\n",
    "lgOFF=len(dataOFF)\n",
    "lg=lgON+lgOFF\n",
    "\n",
    "binning=37\n",
    "\n",
    "Labels = []\n",
    "Q_ON= []\n",
    "\n",
    "nbfeatures=3\n",
    "ON_accuracy=[]\n",
    "OFF_accuracy=[]\n",
    "fscore=[]\n",
    "FPOS=[]\n",
    "FNEG=[]\n",
    "a=binning\n",
    "b=nbfeatures\n",
    "c=len(data_files_ALL)\n",
    "\n",
    "# multivariate\n",
    "y=np.zeros((2,lg))\n",
    "bigdata= np.zeros((b,c,a))\n",
    "\n",
    "for j in range(len(data_files_ALL)):\n",
    "\n",
    "    dataframe=pd.read_csv(data_files_ALL[j],index_col=[0])\n",
    "    dataframe.columns=['MJD','Flux','Delta Flux','Photon Index','Delta Index','TS','fratio']\n",
    "\n",
    "    bigdata[0][j]=dataframe['Flux']\n",
    "    bigdata[1][j]=dataframe['Photon Index']*dataframe['Flux']\n",
    "    bigdata[2][j]=dataframe['Photon Index']\n",
    "\n",
    "for i in range(lgOFF):\n",
    "    \n",
    "    Labels.append(int(0))\n",
    "    \n",
    "for i in range(lgON): \n",
    "    \n",
    "    \n",
    "    Labels.append(int(1))\n",
    "    \n",
    "    \n",
    "bigdata=bigdata.reshape(c,nbfeatures,binning)    \n",
    "print(bigdata.shape)\n",
    "for i in range(iteration):\n",
    "    \n",
    "    #3D shape of data, multivariate time series\n",
    "\n",
    "    sss = ShuffleSplit(n_splits=1, test_size=0.2,random_state=i)\n",
    "    X=bigdata\n",
    "    y=Labels\n",
    "\n",
    "    sss.get_n_splits(X, y)\n",
    "    train_index, test_index = next(sss.split(X, y)) \n",
    "\n",
    "    x_train, x_test = X[train_index], X[test_index] \n",
    "\n",
    "    y_train=[]\n",
    "    y_test=[]\n",
    "\n",
    "    for i in range(len(train_index)):\n",
    "        a=train_index[i]\n",
    "        y_train.append(y[a])\n",
    "    for i in range(len(test_index)):\n",
    "\n",
    "        a=test_index[i]\n",
    "        y_test.append(y[a])\n",
    "\n",
    "    y_test2=y_test.copy()\n",
    "\n",
    "    clf = MultivariateClassifier(BOSSVS(n_bins=5,anova=True,word_size=9,drop_sum=True))\n",
    "    clf.fit(x_train, y_train)\n",
    "    MultivariateClassifier(...)\n",
    "    clf.predict(x_test)\n",
    "\n",
    "    prediction=np.array(clf.predict(x_test))\n",
    "    on_score=0\n",
    "    on_nbs=0\n",
    "    off_nbs=0\n",
    "    off_score=0\n",
    "    foff=0\n",
    "    fon=0\n",
    "    for i in range(len(y_test)):\n",
    "        if y_test2[i]==1 :\n",
    "            on_nbs+=1\n",
    "            if prediction[i]==1:\n",
    "                on_score+=1\n",
    "            else : \n",
    "                fon+=1\n",
    "        if y_test2[i]==0 :\n",
    "            off_nbs+=1\n",
    "            if prediction[i]==0:\n",
    "                off_score+=1 \n",
    "            else:\n",
    "                foff+=1\n",
    "                a=test_index[i]\n",
    "                Q_ON.append(a)\n",
    "    if on_nbs>0:    \n",
    "        ON_accuracy.append(100*(on_score/on_nbs))\n",
    "        FPOS.append(100*(fon/on_nbs))\n",
    "    OFF_accuracy.append(100*(off_score/off_nbs))\n",
    "    FNEG.append(100*(foff/off_nbs))\n",
    "    f1= f1_score(y_test2,prediction,average='weighted')\n",
    "    fscore.append(f1)\n",
    "    \n",
    "print(\"Accuracy for ON class: \",np.mean(ON_accuracy) ,\"%\")\n",
    "print(\"Accuracy for OFF class: \",np.mean(OFF_accuracy) ,\"%\")\n",
    "print(\"False Positive rate: \",np.mean(FNEG) ,\"%\")\n",
    "print(\"False Negative rate: \",np.mean(FPOS) ,\"%\")\n",
    "print(\"F1 score: \",np.mean(fscore))\n",
    "\n",
    "\n",
    "\n",
    "gg = np.unique(Q_ON)\n",
    "Q_ON2=[]\n",
    "Q_ON3=[]\n",
    "errOFF=[]\n",
    "print(len(test_index))\n",
    "print(len(np.unique(Q_ON)))\n",
    "\n",
    "if len(gg)>0:\n",
    "    for i in range(len(Q_ON)):\n",
    "        aa=Q_ON[i]\n",
    "        Q_ON2.append(file_names_ALL[aa])\n",
    "    gg = np.unique(Q_ON2)    \n",
    "    for i in range(len(gg)):\n",
    "        bb=gg[i]\n",
    "        count = Q_ON2.count(bb)\n",
    "        if count>int(iteration/8):\n",
    "            Q_ON3.append([bb,count])\n",
    "            errOFF.append(bb)\n",
    "print(Q_ON3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b177a67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing in a list the adresses of all the data files in the notebook\n",
    "\n",
    "data_files_ON=[]\n",
    "file_names_ON=[]\n",
    "\n",
    "data_files_OFF=[]\n",
    "file_names_OFF=[]\n",
    "data_files_ALL=[]\n",
    "file_names_ALL=[]\n",
    "\n",
    "#Storing in a list the adresses of all the data files in the notebook\n",
    "for filename in os.listdir(pathOFF):\n",
    "    f = os.path.join(pathOFF,filename)\n",
    "    if os.path.isfile(f) and filename in errOFF :\n",
    "        data_files_OFF.append(f)\n",
    "        file_names_OFF.append(filename)\n",
    "        data_files_ALL.append(f)\n",
    "        file_names_ALL.append(filename)\n",
    "        \n",
    "for filename in os.listdir(pathON):\n",
    "    f = os.path.join(pathON,filename)\n",
    "    if os.path.isfile(f):\n",
    "        data_files_ON.append(f)\n",
    "        file_names_ON.append(filename)\n",
    "        data_files_ALL.append(f)\n",
    "        file_names_ALL.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33637ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using data_files with only the 36 binning\n",
    "Filter=True\n",
    "binning=37\n",
    "\n",
    "\n",
    "if Filter==True:\n",
    "    \n",
    "    idx_OFF=[]\n",
    "    idx_ON=[]\n",
    "    dataON=[]\n",
    "    dataOFF=[]\n",
    "    for i in range(len(data_files_OFF)):\n",
    "        dataframe = pd.read_csv(data_files_OFF[i])\n",
    "        lg = len(dataframe)\n",
    "        \n",
    "        if lg==binning:\n",
    "            idx_OFF.append(i)\n",
    "    \n",
    "    for i in range(len(data_files_ON)):\n",
    "        dataframe = pd.read_csv(data_files_ON[i])\n",
    "        lg = len(dataframe)\n",
    "\n",
    "        if lg==binning:\n",
    "            idx_ON.append(i)\n",
    "\n",
    "\n",
    "    for i in range(len(idx_OFF)):\n",
    "\n",
    "        a=idx_OFF[i]\n",
    "        dataOFF.append(data_files_OFF[a])\n",
    "    for i in range(len(idx_ON)):\n",
    "\n",
    "        a=idx_ON[i]\n",
    "        dataON.append(data_files_ON[a])\n",
    "\n",
    "idx = idx_OFF+idx_ON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c751043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124, 3, 37)\n",
      "Accuracy for ON class:  100.0 %\n",
      "Accuracy for OFF class:  85.79759892010817 %\n",
      "False Positive rate:  14.202401079891825 %\n",
      "False Negative rate:  0.0 %\n",
      "F1 score:  0.900423406593497\n",
      "[['4FGL J2329+6101.csv', 47], ['4FGLJ0018+2946.csv', 53], ['4FGLJ0019+7327.csv', 44], ['4FGLJ0028+2001.csv', 42], ['4FGLJ0033-1921.csv', 41]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyts.classification import BOSSVS\n",
    "from pyts.multivariate.classification import MultivariateClassifier\n",
    "\n",
    "iteration=200\n",
    "lgON=len(dataON)\n",
    "lgOFF=len(dataOFF)\n",
    "lg=lgON+lgOFF\n",
    "\n",
    "binning=37\n",
    "\n",
    "Labels = []\n",
    "Q_ON= []\n",
    "\n",
    "nbfeatures=3\n",
    "ON_accuracy=[]\n",
    "OFF_accuracy=[]\n",
    "fscore= []\n",
    "FPOS=[]\n",
    "FNEG=[]\n",
    "a=binning\n",
    "b=nbfeatures\n",
    "c=len(data_files_ALL)\n",
    "\n",
    "# multivariate\n",
    "y=np.zeros((2,lg))\n",
    "bigdata= np.zeros((b,c,a))\n",
    "\n",
    "for j in range(len(data_files_ALL)):\n",
    "\n",
    "#         a=Q_ON2[j]\n",
    "\n",
    "    dataframe=pd.read_csv(data_files_ALL[j],index_col=[0])\n",
    "    dataframe.columns=['MJD','Flux','Delta Flux','Photon Index','Delta Index','TS','fratio']\n",
    "\n",
    "    bigdata[0][j]=dataframe['Flux']\n",
    "    bigdata[1][j]=dataframe['Photon Index']*dataframe['Flux']\n",
    "    bigdata[2][j]=dataframe['Photon Index']\n",
    "\n",
    "for i in range(lgOFF):\n",
    "    \n",
    "    Labels.append(int(0))\n",
    "    \n",
    "for i in range(lgON): \n",
    "    \n",
    "    \n",
    "    Labels.append(int(1))\n",
    "    \n",
    "    \n",
    "bigdata=bigdata.reshape(c,nbfeatures,binning)    \n",
    "print(bigdata.shape)\n",
    "\n",
    "for i in range(iteration):\n",
    "    #3D shape of data, multivariate time series\n",
    "\n",
    "    sss = ShuffleSplit(n_splits=1, test_size=0.2,random_state=i)\n",
    "    X=bigdata\n",
    "    y=Labels\n",
    "\n",
    "    sss.get_n_splits(X, y)\n",
    "    train_index, test_index = next(sss.split(X, y)) \n",
    "\n",
    "    x_train, x_test = X[train_index], X[test_index] \n",
    "\n",
    "    y_train=[]\n",
    "    y_test=[]\n",
    "\n",
    "    for i in range(len(train_index)):\n",
    "        a=train_index[i]\n",
    "        y_train.append(y[a])\n",
    "    for i in range(len(test_index)):\n",
    "\n",
    "        a=test_index[i]\n",
    "        y_test.append(y[a])\n",
    "\n",
    "    y_test2=y_test.copy()\n",
    "\n",
    "    clf = MultivariateClassifier(BOSSVS(n_bins=5,anova=True,word_size=9,drop_sum=True))\n",
    "    clf.fit(x_train, y_train)\n",
    "    MultivariateClassifier(...)\n",
    "    clf.predict(x_test)\n",
    "\n",
    "    prediction=np.array(clf.predict(x_test))\n",
    "    on_score=0\n",
    "    on_nbs=0\n",
    "    off_nbs=0\n",
    "    off_score=0\n",
    "    foff=0\n",
    "    fon=0\n",
    "    for i in range(len(y_test)):\n",
    "        if y_test2[i]==1 :\n",
    "            on_nbs+=1\n",
    "            if prediction[i]==1:\n",
    "                on_score+=1\n",
    "            else : \n",
    "                fon+=1\n",
    "        if y_test2[i]==0 :\n",
    "            off_nbs+=1\n",
    "            if prediction[i]==0:\n",
    "                off_score+=1 \n",
    "            else:\n",
    "                foff+=1\n",
    "                a=test_index[i]\n",
    "                Q_ON.append(a)\n",
    "    if on_nbs>0:    \n",
    "        ON_accuracy.append(100*(on_score/on_nbs))\n",
    "        FPOS.append(100*(fon/on_nbs))\n",
    "    OFF_accuracy.append(100*(off_score/off_nbs))\n",
    "    FNEG.append(100*(foff/off_nbs))\n",
    "    f1= f1_score(y_test2,prediction,average='weighted')\n",
    "    fscore.append(f1)\n",
    "    \n",
    "print(\"Accuracy for ON class: \",np.mean(ON_accuracy) ,\"%\")\n",
    "print(\"Accuracy for OFF class: \",np.mean(OFF_accuracy) ,\"%\")\n",
    "print(\"False Positive rate: \",np.mean(FNEG) ,\"%\")\n",
    "print(\"False Negative rate: \",np.mean(FPOS) ,\"%\")\n",
    "print(\"F1 score: \",np.mean(fscore))\n",
    "\n",
    "\n",
    "gg = np.unique(Q_ON)\n",
    "Q_ON2=[]\n",
    "Q_ON3=[]\n",
    "errOFF=[]\n",
    "\n",
    "if len(gg)>0:\n",
    "    for i in range(len(Q_ON)):\n",
    "        aa=Q_ON[i]\n",
    "        Q_ON2.append(file_names_ALL[aa])\n",
    "    gg = np.unique(Q_ON2)    \n",
    "    for i in range(len(gg)):\n",
    "        bb=gg[i]\n",
    "        count = Q_ON2.count(bb)\n",
    "        if count>int(iteration/5):\n",
    "            Q_ON3.append([bb,count])\n",
    "            errOFF.append(bb)\n",
    "print(Q_ON3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a11d55ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing in a list the adresses of all the data files in the notebook\n",
    "\n",
    "data_files_ON=[]\n",
    "file_names_ON=[]\n",
    "\n",
    "data_files_OFF=[]\n",
    "file_names_OFF=[]\n",
    "data_files_ALL=[]\n",
    "file_names_ALL=[]\n",
    "\n",
    "#Storing in a list the adresses of all the data files in the notebook\n",
    "for filename in os.listdir(pathOFF):\n",
    "    f = os.path.join(pathOFF,filename)\n",
    "    if os.path.isfile(f) and filename in errOFF :\n",
    "        data_files_OFF.append(f)\n",
    "        file_names_OFF.append(filename)\n",
    "        data_files_ALL.append(f)\n",
    "        file_names_ALL.append(filename)\n",
    "        \n",
    "for filename in os.listdir(pathON):\n",
    "    f = os.path.join(pathON,filename)\n",
    "    if os.path.isfile(f):\n",
    "        data_files_ON.append(f)\n",
    "        file_names_ON.append(filename)\n",
    "        data_files_ALL.append(f)\n",
    "        file_names_ALL.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e182b57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using data_files with only the 36 binning\n",
    "Filter=True\n",
    "binning=37\n",
    "\n",
    "\n",
    "if Filter==True:\n",
    "    \n",
    "    idx_OFF=[]\n",
    "    idx_ON=[]\n",
    "    dataON=[]\n",
    "    dataOFF=[]\n",
    "    \n",
    "    for i in range(len(data_files_OFF)):\n",
    "        dataframe = pd.read_csv(data_files_OFF[i])\n",
    "        lg = len(dataframe)\n",
    "        \n",
    "        if lg==binning:\n",
    "            idx_OFF.append(i)\n",
    "    \n",
    "    for i in range(len(data_files_ON)):\n",
    "        dataframe = pd.read_csv(data_files_ON[i])\n",
    "        lg = len(dataframe)\n",
    "\n",
    "        if lg==binning:\n",
    "            idx_ON.append(i)\n",
    "\n",
    "\n",
    "    for i in range(len(idx_OFF)):\n",
    "\n",
    "        a=idx_OFF[i]\n",
    "        dataOFF.append(data_files_OFF[a])\n",
    "    for i in range(len(idx_ON)):\n",
    "\n",
    "        a=idx_ON[i]\n",
    "        dataON.append(data_files_ON[a])\n",
    "\n",
    "idx = idx_OFF+idx_ON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a5b39bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35, 3, 37)\n",
      "Accuracy for ON class:  64.44761904761904 %\n",
      "Accuracy for OFF class:  82.5925925925926 %\n",
      "False Positive rate:  17.40740740740741 %\n",
      "False Negative rate:  35.55238095238095 %\n",
      "F1 score:  0.7270905483405484\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyts.classification import BOSSVS\n",
    "from pyts.multivariate.classification import MultivariateClassifier\n",
    "\n",
    "iteration=200\n",
    "lgON=len(dataON)\n",
    "lgOFF=len(dataOFF)\n",
    "lg=lgON+lgOFF\n",
    "\n",
    "binning=37\n",
    "\n",
    "Labels = []\n",
    "Q_ON= []\n",
    "\n",
    "nbfeatures=3\n",
    "ON_accuracy=[]\n",
    "OFF_accuracy=[]\n",
    "fscore=[]\n",
    "FPOS=[]\n",
    "FNEG=[]\n",
    "a=binning\n",
    "b=nbfeatures\n",
    "c=len(data_files_ALL)\n",
    "\n",
    "# multivariate\n",
    "y=np.zeros((2,lg))\n",
    "bigdata= np.zeros((b,c,a))\n",
    "\n",
    "for j in range(len(data_files_ALL)):\n",
    "\n",
    "\n",
    "    dataframe=pd.read_csv(data_files_ALL[j],index_col=[0])\n",
    "    dataframe.columns=['MJD','Flux','Delta Flux','Photon Index','Delta Index','TS','fratio']\n",
    "\n",
    "    bigdata[0][j]=dataframe['Flux']\n",
    "    bigdata[1][j]=dataframe['Photon Index']*dataframe['Flux']\n",
    "    bigdata[2][j]=dataframe['Photon Index']\n",
    "\n",
    "for i in range(lgOFF):\n",
    "    \n",
    "    Labels.append(int(0))\n",
    "    \n",
    "for i in range(lgON): \n",
    "    \n",
    "    \n",
    "    Labels.append(int(1))\n",
    "    \n",
    "    \n",
    "bigdata=bigdata.reshape(c,nbfeatures,binning)    \n",
    "print(bigdata.shape)\n",
    "for i in range(iteration):\n",
    "    #3D shape of data, multivariate time series\n",
    "\n",
    "    sss = ShuffleSplit(n_splits=1, test_size=0.2,random_state=i)\n",
    "    X=bigdata\n",
    "    y=Labels\n",
    "\n",
    "    sss.get_n_splits(X, y)\n",
    "    train_index, test_index = next(sss.split(X, y)) \n",
    "\n",
    "    x_train, x_test = X[train_index], X[test_index] \n",
    "\n",
    "    y_train=[]\n",
    "    y_test=[]\n",
    "\n",
    "    for i in range(len(train_index)):\n",
    "        a=train_index[i]\n",
    "        y_train.append(y[a])\n",
    "    for i in range(len(test_index)):\n",
    "\n",
    "        a=test_index[i]\n",
    "        y_test.append(y[a])\n",
    "\n",
    "    y_test2=y_test.copy()\n",
    "\n",
    "    clf = MultivariateClassifier(BOSSVS(n_bins=5,anova=True,word_size=9,drop_sum=True))\n",
    "    clf.fit(x_train, y_train)\n",
    "    MultivariateClassifier(...)\n",
    "    clf.predict(x_test)\n",
    "\n",
    "    prediction=np.array(clf.predict(x_test))\n",
    "    on_score=0\n",
    "    on_nbs=0\n",
    "    off_nbs=0\n",
    "    off_score=0\n",
    "    foff=0\n",
    "    fon=0\n",
    "    for i in range(len(y_test)):\n",
    "        if y_test2[i]==1 :\n",
    "            on_nbs+=1\n",
    "            if prediction[i]==1:\n",
    "                on_score+=1\n",
    "            else : \n",
    "                fon+=1\n",
    "        if y_test2[i]==0 :\n",
    "            off_nbs+=1\n",
    "            if prediction[i]==0:\n",
    "                off_score+=1 \n",
    "            else:\n",
    "                foff+=1\n",
    "    if on_nbs>0:    \n",
    "        ON_accuracy.append(100*(on_score/on_nbs))\n",
    "        FPOS.append(100*(fon/on_nbs))\n",
    "    if off_nbs>0: \n",
    "        OFF_accuracy.append(100*(off_score/off_nbs))\n",
    "        FNEG.append(100*(foff/off_nbs))\n",
    "    f1= f1_score(y_test2,prediction,average='weighted')\n",
    "    fscore.append(f1)\n",
    "    \n",
    "print(\"Accuracy for ON class: \",np.mean(ON_accuracy) ,\"%\")\n",
    "print(\"Accuracy for OFF class: \",np.mean(OFF_accuracy) ,\"%\")\n",
    "print(\"False Positive rate: \",np.mean(FNEG) ,\"%\")\n",
    "print(\"False Negative rate: \",np.mean(FPOS) ,\"%\")\n",
    "print(\"F1 score: \",np.mean(fscore))\n",
    "\n",
    "\n",
    "\n",
    "gg = np.unique(Q_ON)\n",
    "Q_ON2=[]\n",
    "Q_ON3=[]\n",
    "errOFF=[]\n",
    "\n",
    "if len(gg)>0:\n",
    "    for i in range(len(Q_ON)):\n",
    "        aa=Q_ON[i]\n",
    "        Q_ON2.append(file_names_ALL[aa])\n",
    "    gg = np.unique(Q_ON2)    \n",
    "    for i in range(len(gg)):\n",
    "        bb=gg[i]\n",
    "        count = Q_ON2.count(bb)\n",
    "        if count>1:\n",
    "            Q_ON3.append([bb,count])\n",
    "            errOFF.append(bb)\n",
    "print(Q_ON3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781b617f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
