{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecca3a13",
   "metadata": {},
   "source": [
    "# Neural networks code and tests notebook\n",
    "\n",
    "# This notebook takes the flux, photon index and fratio data and try to predict classes from this data on a :\n",
    "\n",
    "# - LSTM network\n",
    "# - ConvLSTM network\n",
    "# - Conv1D network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95e6f94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import math\n",
    "from scipy import interpolate\n",
    "import sys \n",
    "from re import search\n",
    "from astropy.io import fits\n",
    "from astropy import units as u\n",
    "from astropy.time import Time\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.coordinates import ICRS, Galactic, FK4, FK5  # Low-level frames\n",
    "from astropy.coordinates import Angle, Latitude, Longitude\n",
    "import shutil\n",
    "from keras.callbacks import CSVLogger\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import glob\n",
    "from datetime import datetime\n",
    "from keras.layers import Conv1D, Dense, Dropout, Input, Concatenate, GlobalMaxPooling1D, BatchNormalization\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import dstack\n",
    "from pandas import read_csv\n",
    "from matplotlib import pyplot\n",
    "import tensorflow.keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten,LSTM\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Activation, Concatenate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ShuffleSplit # or StratifiedShuffleSplit\n",
    "from pandas import DataFrame\n",
    "from pyts.classification import BOSSVS\n",
    "from pyts.multivariate.classification import MultivariateClassifier\n",
    "import shutil\n",
    "from keras.callbacks import CSVLogger\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import dstack\n",
    "from pandas import read_csv\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ShuffleSplit # or StratifiedShuffleSplit\n",
    "from pandas import DataFrame\n",
    "import sktime\n",
    "from sktime.classification.distance_based import KNeighborsTimeSeriesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da642277",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathON=\"C:/Users/pierr/Documents_kanop/Shps/Pierre_points_for_louis/Bureau/interpo_ONOFF/ON_data/\"\n",
    "pathOFF=\"C:/Users/pierr/Documents_kanop/Shps/Pierre_points_for_louis/Bureau/interpo_ONOFF/OFF_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7596790",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing in a list the adresses of all the data files in the notebook\n",
    "\n",
    "data_files_ON=[]\n",
    "file_names_ON=[]\n",
    "\n",
    "data_files_OFF=[]\n",
    "file_names_OFF=[]\n",
    "data_files_ALL=[]\n",
    "file_names_ALL=[]\n",
    "\n",
    "#Storing in a list the adresses of all the data files in the notebook\n",
    "for filename in os.listdir(pathOFF):\n",
    "    f = os.path.join(pathOFF,filename)\n",
    "    if os.path.isfile(f):\n",
    "        data_files_OFF.append(f)\n",
    "        file_names_OFF.append(filename)\n",
    "        data_files_ALL.append(f)\n",
    "        file_names_ALL.append(filename)\n",
    "for filename in os.listdir(pathON):\n",
    "    f = os.path.join(pathON,filename)\n",
    "    if os.path.isfile(f):\n",
    "        data_files_ON.append(f)\n",
    "        file_names_ON.append(filename)\n",
    "        data_files_ALL.append(f)\n",
    "        file_names_ALL.append(filename)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c01a285",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using data_files with only the 36 binning\n",
    "Filter=True\n",
    "binning=37\n",
    "\n",
    "\n",
    "if Filter==True:\n",
    "    \n",
    "    idx_OFF=[]\n",
    "    idx_ON=[]\n",
    "    dataON=[]\n",
    "    dataOFF=[]\n",
    "    for i in range(len(data_files_OFF)):\n",
    "        dataframe = pd.read_csv(data_files_OFF[i])\n",
    "        lg = len(dataframe)\n",
    "        \n",
    "        if lg==binning:\n",
    "            idx_OFF.append(i)\n",
    "    \n",
    "    for i in range(len(data_files_ON)):\n",
    "        dataframe = pd.read_csv(data_files_ON[i])\n",
    "        lg = len(dataframe)\n",
    "        if lg==binning:\n",
    "            idx_ON.append(i)\n",
    "\n",
    "\n",
    "    for i in range(len(idx_OFF)):\n",
    "\n",
    "        a=idx_OFF[i]\n",
    "        dataOFF.append(data_files_OFF[a])\n",
    "    for i in range(len(idx_ON)):\n",
    "\n",
    "        a=idx_ON[i]\n",
    "        dataON.append(data_files_ON[a])\n",
    "\n",
    "idx = idx_OFF+idx_ON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fba738f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "lgON=len(dataON)\n",
    "lgOFF=len(dataOFF)\n",
    "lg=lgON+lgOFF\n",
    "Labels = []\n",
    "\n",
    "nbfeatures=3\n",
    "\n",
    "a=binning\n",
    "b=nbfeatures\n",
    "c=lg\n",
    "\n",
    "# multivariate\n",
    "\n",
    "data_matrix= np.zeros((b,c,a))\n",
    "\n",
    "#Construct data matrix\n",
    "for j in range(len(dataOFF)):\n",
    "\n",
    "    dataframe=pd.read_csv(dataOFF[j],index_col=[0])\n",
    "    dataframe.columns=['MJD','Flux','Delta Flux','Photon Index','Delta Index','TS','fratio']\n",
    "\n",
    "    data_matrix[0][j]=dataframe['Flux']\n",
    "    data_matrix[1][j]=dataframe['Photon Index']*dataframe['Flux']\n",
    "    data_matrix[2][j]=dataframe['Photon Index']\n",
    "    \n",
    "for j in range(len(dataON)):\n",
    "\n",
    "    v=j+len(dataOFF)\n",
    "    dataframe=pd.read_csv(dataON[j],index_col=[0])\n",
    "    dataframe.columns=['MJD','Flux','Delta Flux','Photon Index','Delta Index','TS','fratio']\n",
    "\n",
    "    data_matrix[0][v]=dataframe['Flux']\n",
    "    data_matrix[1][v]=dataframe['Photon Index']*dataframe['Flux']\n",
    "    data_matrix[2][v]=dataframe['Photon Index']     \n",
    "#Creating labels\n",
    "for i in range(lgOFF):\n",
    "    \n",
    "    Labels.append(int(0))\n",
    "    \n",
    "for i in range(lgON):\n",
    "    Labels.append(int(1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "323077f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "lgON=len(dataON)\n",
    "lgOFF=len(dataOFF)\n",
    "lg=lgON+lgOFF\n",
    "Labels = []\n",
    "\n",
    "nbfeatures=3\n",
    "nb_dels=10\n",
    "a=binning-nb_dels\n",
    "b=nbfeatures\n",
    "\n",
    "c=lg\n",
    "\n",
    "# multivariate\n",
    "\n",
    "data_matrix= np.zeros((b,c,a))\n",
    "delete=np.arange(nb_dels)\n",
    "\n",
    "#Construct data matrix\n",
    "for j in range(len(dataOFF)):\n",
    "\n",
    "    dataframe=pd.read_csv(dataOFF[j],index_col=[0])\n",
    "    dataframe.columns=['MJD','Flux','Delta Flux','Photon Index','Delta Index','TS','fratio']\n",
    "    flux=np.array(dataframe['Flux'])\n",
    "    flux=np.delete(flux,delete)\n",
    "    photon_idx=np.array(dataframe['Photon Index'])\n",
    "    photon_idx=np.delete(photon_idx,delete)\n",
    "    flux_idx=np.array(photon_idx*flux)\n",
    "    \n",
    "    data_matrix[0][j]=flux\n",
    "    data_matrix[1][j]=photon_idx\n",
    "    data_matrix[2][j]=flux_idx\n",
    "    \n",
    "for j in range(len(dataON)):\n",
    "\n",
    "    v=j+len(dataOFF)\n",
    "    dataframe=pd.read_csv(dataON[j],index_col=[0])\n",
    "    dataframe.columns=['MJD','Flux','Delta Flux','Photon Index','Delta Index','TS','fratio']\n",
    "\n",
    "    flux=np.array(dataframe['Flux'])\n",
    "    flux=np.delete(flux,delete)\n",
    "    photon_idx=np.array(dataframe['Photon Index'])\n",
    "    photon_idx=np.delete(photon_idx,delete)\n",
    "    flux_idx=np.array(photon_idx*flux)\n",
    "    \n",
    "    data_matrix[0][v]=flux\n",
    "    data_matrix[1][v]=photon_idx\n",
    "    data_matrix[2][v]=flux_idx\n",
    "    \n",
    "#Creating labels\n",
    "for i in range(lgOFF):\n",
    "    \n",
    "    Labels.append(int(0))\n",
    "    \n",
    "for i in range(lgON):\n",
    "    Labels.append(int(1))\n",
    "\n",
    "#Reshaping data matrix\n",
    "data_matrix=data_matrix.reshape(c,b,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "649468fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshaping data matrix\n",
    "data_matrix=data_matrix.reshape(c,b,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62b24ef6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "871/871 [==============================] - 3s 2ms/step - loss: 0.4867 - binary_accuracy: 0.7819\n",
      "Epoch 2/20\n",
      "871/871 [==============================] - 2s 2ms/step - loss: 0.3855 - binary_accuracy: 0.7807\n",
      "Epoch 3/20\n",
      "871/871 [==============================] - 2s 2ms/step - loss: 0.3527 - binary_accuracy: 0.7807\n",
      "Epoch 4/20\n",
      "871/871 [==============================] - 2s 2ms/step - loss: 0.3363 - binary_accuracy: 0.7807\n",
      "Epoch 5/20\n",
      "871/871 [==============================] - 2s 2ms/step - loss: 0.3276 - binary_accuracy: 0.7807\n",
      "Epoch 6/20\n",
      "871/871 [==============================] - 2s 2ms/step - loss: 0.3216 - binary_accuracy: 0.7807\n",
      "Epoch 7/20\n",
      "871/871 [==============================] - 2s 2ms/step - loss: 0.3195 - binary_accuracy: 0.7807\n",
      "Iteration no. 0\n",
      "7/7 [==============================] - 1s 1ms/step\n",
      "Epoch 1/20\n",
      "871/871 [==============================] - 3s 2ms/step - loss: 0.5134 - binary_accuracy: 0.7583\n",
      "Epoch 2/20\n",
      "871/871 [==============================] - 2s 2ms/step - loss: 0.4155 - binary_accuracy: 0.7635\n",
      "Epoch 3/20\n",
      "871/871 [==============================] - 2s 2ms/step - loss: 0.3784 - binary_accuracy: 0.7635\n",
      "Epoch 4/20\n",
      "871/871 [==============================] - 2s 2ms/step - loss: 0.3625 - binary_accuracy: 0.7635\n",
      "Epoch 5/20\n",
      "871/871 [==============================] - 2s 3ms/step - loss: 0.3539 - binary_accuracy: 0.7635\n",
      "Epoch 6/20\n",
      "871/871 [==============================] - 2s 2ms/step - loss: 0.3493 - binary_accuracy: 0.7646\n",
      "Epoch 7/20\n",
      "871/871 [==============================] - 2s 2ms/step - loss: 0.3421 - binary_accuracy: 0.7635\n",
      "Epoch 8/20\n",
      "871/871 [==============================] - 2s 2ms/step - loss: 0.3390 - binary_accuracy: 0.7635\n",
      "Epoch 9/20\n",
      "871/871 [==============================] - 2s 2ms/step - loss: 0.3363 - binary_accuracy: 0.7635\n",
      "Epoch 10/20\n",
      "871/871 [==============================] - 2s 2ms/step - loss: 0.3351 - binary_accuracy: 0.7635\n",
      "Epoch 11/20\n",
      "871/871 [==============================] - 2s 2ms/step - loss: 0.3308 - binary_accuracy: 0.7635\n",
      "Epoch 12/20\n",
      "871/871 [==============================] - 2s 2ms/step - loss: 0.3309 - binary_accuracy: 0.7646\n",
      "Iteration no. 1\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "Epoch 1/20\n",
      "871/871 [==============================] - 3s 2ms/step - loss: 0.5061 - binary_accuracy: 0.7710\n",
      "Epoch 2/20\n",
      "871/871 [==============================] - 2s 2ms/step - loss: 0.4021 - binary_accuracy: 0.7715\n",
      "Epoch 3/20\n",
      "871/871 [==============================] - 2s 2ms/step - loss: 0.3711 - binary_accuracy: 0.7991\n",
      "Epoch 4/20\n",
      "871/871 [==============================] - 2s 2ms/step - loss: 0.3511 - binary_accuracy: 0.7715\n",
      "Epoch 5/20\n",
      "871/871 [==============================] - 2s 2ms/step - loss: 0.3430 - binary_accuracy: 0.7715\n",
      "Epoch 6/20\n",
      "871/871 [==============================] - 2s 2ms/step - loss: 0.3346 - binary_accuracy: 0.7715\n",
      "Epoch 7/20\n",
      "871/871 [==============================] - 2s 2ms/step - loss: 0.3380 - binary_accuracy: 0.7727\n",
      "Epoch 8/20\n",
      "871/871 [==============================] - 2s 2ms/step - loss: 0.3289 - binary_accuracy: 0.7715\n",
      "Epoch 9/20\n",
      "871/871 [==============================] - 2s 2ms/step - loss: 0.3262 - binary_accuracy: 0.7715\n",
      "Iteration no. 2\n",
      "7/7 [==============================] - 1s 1ms/step\n",
      "Epoch 1/20\n",
      "871/871 [==============================] - 4s 2ms/step - loss: 0.5394 - binary_accuracy: 0.7480\n",
      "Epoch 2/20\n",
      "871/871 [==============================] - 2s 2ms/step - loss: 0.4226 - binary_accuracy: 0.7635\n",
      "Epoch 3/20\n",
      "871/871 [==============================] - 2s 2ms/step - loss: 0.3791 - binary_accuracy: 0.7635\n",
      "Epoch 4/20\n",
      "871/871 [==============================] - 2s 2ms/step - loss: 0.3644 - binary_accuracy: 0.7635\n",
      "Epoch 5/20\n",
      "871/871 [==============================] - 2s 2ms/step - loss: 0.3540 - binary_accuracy: 0.7635\n",
      "Epoch 6/20\n",
      "871/871 [==============================] - 2s 2ms/step - loss: 0.3475 - binary_accuracy: 0.7635\n",
      "Epoch 7/20\n",
      "871/871 [==============================] - 2s 2ms/step - loss: 0.3443 - binary_accuracy: 0.7635\n",
      "Epoch 8/20\n",
      "871/871 [==============================] - 2s 2ms/step - loss: 0.3373 - binary_accuracy: 0.7635\n",
      "Iteration no. 3\n",
      "7/7 [==============================] - 1s 2ms/step\n",
      "Epoch 1/20\n",
      "871/871 [==============================] - 4s 2ms/step - loss: 0.5502 - binary_accuracy: 0.7250\n",
      "Epoch 2/20\n",
      "871/871 [==============================] - 2s 2ms/step - loss: 0.4254 - binary_accuracy: 0.7681\n",
      "Epoch 3/20\n",
      "871/871 [==============================] - 2s 2ms/step - loss: 0.3785 - binary_accuracy: 0.7681\n",
      "Epoch 4/20\n",
      "871/871 [==============================] - 2s 2ms/step - loss: 0.3577 - binary_accuracy: 0.7681\n",
      "Epoch 5/20\n",
      "871/871 [==============================] - 2s 2ms/step - loss: 0.3466 - binary_accuracy: 0.7681\n",
      "Epoch 6/20\n",
      "871/871 [==============================] - 2s 2ms/step - loss: 0.3402 - binary_accuracy: 0.7681\n",
      "Epoch 7/20\n",
      "871/871 [==============================] - 2s 2ms/step - loss: 0.3349 - binary_accuracy: 0.7681\n",
      "Epoch 8/20\n",
      "871/871 [==============================] - 2s 2ms/step - loss: 0.3328 - binary_accuracy: 0.7681\n",
      "Iteration no. 4\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "Accuracy for ON class:  100.0 %\n",
      "Accuracy for OFF class:  72.49395830300352 %\n",
      "False Positive rate:  0.0 %\n",
      "False Negative rate:  27.50604169699647 %\n",
      "F1 score:  0.8016007681416115\n"
     ]
    }
   ],
   "source": [
    "#LSTM architecture\n",
    "from keras.layers import Conv1D\n",
    "from sklearn.metrics import f1_score\n",
    "ON_accuracy=[]\n",
    "OFF_accuracy=[]\n",
    "fscore=[]\n",
    "FPOS=[]\n",
    "FNEG= []\n",
    "\n",
    "weight_for_0 = (1 / lgOFF) * (lg / 2.0)\n",
    "weight_for_1 = (1 / lgON) * (lg / 2.0)\n",
    "\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "callback=tensorflow.keras.callbacks.EarlyStopping(monitor='binary_accuracy',patience=6, verbose=0, mode='auto',baseline=None, restore_best_weights=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(9,input_shape=(nbfeatures,binning),activation='tanh'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss=tensorflow.keras.losses.BinaryCrossentropy(reduction='sum'),optimizer='SGD', metrics=['binary_accuracy'])\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(data_matrix, Labels, test_size=0.2, random_state=i)\n",
    "    y_test2=y_test.copy()\n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n",
    "    \n",
    "    #fitting data\n",
    "    model.fit(x_train, y_train,epochs=20,batch_size=1,class_weight=class_weight,callbacks=callback)\n",
    "\n",
    "    print('Iteration no.',i)\n",
    "    #Obtain the accuracy of prediction for each class\n",
    "    prediction= model.predict(x_test)\n",
    "    predicted_labels=[]\n",
    "    \n",
    "    for i in range(len(prediction)):\n",
    "        if prediction[i][0]>0.5:\n",
    "            predicted_labels.append(0)\n",
    "        else:\n",
    "            predicted_labels.append(1)\n",
    "    on_score=0\n",
    "    on_nbs=0\n",
    "    off_nbs=0\n",
    "    off_score=0\n",
    "    foff=0\n",
    "    fon=0\n",
    "    for i in range(len(y_test)):\n",
    "        if y_test2[i]==1 :\n",
    "            on_nbs+=1\n",
    "            if predicted_labels[i]==1:\n",
    "                on_score+=1\n",
    "            else : \n",
    "                fon+=1\n",
    "        if y_test2[i]==0 :\n",
    "            off_nbs+=1\n",
    "            if predicted_labels[i]==0:\n",
    "                off_score+=1 \n",
    "            else:\n",
    "                foff+=1\n",
    "    if on_nbs>0:    \n",
    "        ON_accuracy.append(100*(on_score/on_nbs))\n",
    "        FPOS.append(100*(fon/on_nbs))\n",
    "    OFF_accuracy.append(100*(off_score/off_nbs))\n",
    "    FNEG.append(100*(foff/off_nbs))\n",
    "    f1= f1_score(y_test2,predicted_labels,average='weighted')\n",
    "    fscore.append(f1)\n",
    "print(\"Accuracy for ON class: \",np.mean(ON_accuracy) ,\"%\")\n",
    "print(\"Accuracy for OFF class: \",np.mean(OFF_accuracy) ,\"%\")\n",
    "print(\"False Positive rate: \",np.mean(FPOS) ,\"%\")\n",
    "print(\"False Negative rate: \",np.mean(FNEG) ,\"%\")\n",
    "print(\"F1 score: \",np.mean(fscore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2281ed5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "871/871 [==============================] - 5s 3ms/step - loss: 0.6355 - binary_accuracy: 0.5373\n",
      "Epoch 2/5\n",
      "871/871 [==============================] - 3s 3ms/step - loss: 0.5819 - binary_accuracy: 0.4328\n",
      "Epoch 3/5\n",
      "871/871 [==============================] - 3s 3ms/step - loss: 0.5650 - binary_accuracy: 0.4328\n",
      "Epoch 4/5\n",
      "871/871 [==============================] - 3s 3ms/step - loss: 0.5574 - binary_accuracy: 0.4328\n",
      "Epoch 5/5\n",
      "871/871 [==============================] - 2s 2ms/step - loss: 0.5502 - binary_accuracy: 0.4478\n",
      "Iteration no. 0\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "Epoch 1/5\n",
      "871/871 [==============================] - 3s 2ms/step - loss: 0.6200 - binary_accuracy: 0.4707\n",
      "Epoch 2/5\n",
      "871/871 [==============================] - 2s 2ms/step - loss: 0.5816 - binary_accuracy: 0.4374\n",
      "Epoch 3/5\n",
      "871/871 [==============================] - 2s 2ms/step - loss: 0.5649 - binary_accuracy: 0.4868\n",
      "Epoch 4/5\n",
      "871/871 [==============================] - 3s 3ms/step - loss: 0.5651 - binary_accuracy: 0.4374\n",
      "Epoch 5/5\n",
      "871/871 [==============================] - 3s 3ms/step - loss: 0.5569 - binary_accuracy: 0.4374\n",
      "Iteration no. 1\n",
      "7/7 [==============================] - 1s 2ms/step\n",
      "Epoch 1/5\n",
      "871/871 [==============================] - 3s 2ms/step - loss: 0.6450 - binary_accuracy: 0.4489\n",
      "Epoch 2/5\n",
      "871/871 [==============================] - 2s 2ms/step - loss: 0.5957 - binary_accuracy: 0.4271\n",
      "Epoch 3/5\n",
      "871/871 [==============================] - 2s 2ms/step - loss: 0.5752 - binary_accuracy: 0.4271\n",
      "Epoch 4/5\n",
      "871/871 [==============================] - 3s 3ms/step - loss: 0.5667 - binary_accuracy: 0.4271\n",
      "Epoch 5/5\n",
      "871/871 [==============================] - 3s 3ms/step - loss: 0.5626 - binary_accuracy: 0.4271\n",
      "Iteration no. 2\n",
      "7/7 [==============================] - 1s 2ms/step\n",
      "Accuracy for ON class:  100.0 %\n",
      "Accuracy for OFF class:  37.01650201650202 %\n",
      "False Positive rate:  0.0 %\n",
      "False Negative rate:  62.983497983497976 %\n",
      "F1 score:  0.5122611119668474\n"
     ]
    }
   ],
   "source": [
    "# Conv/LSTM architecture\n",
    "#LSTM architecture\n",
    "from keras.layers import Conv1D\n",
    "from sklearn.metrics import f1_score\n",
    "ON_accuracy=[]\n",
    "OFF_accuracy=[]\n",
    "fscore=[]\n",
    "FPOS=[]\n",
    "FNEG= []\n",
    "\n",
    "weight_for_0 = (1 / lgOFF) * (lg / 2.0)\n",
    "weight_for_1 = (1 / lgON) * (lg / 2.0)\n",
    "\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "callback=tensorflow.keras.callbacks.EarlyStopping(monitor='binary_accuracy',patience=6, verbose=0, mode='auto',baseline=None, restore_best_weights=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(a, activation='tanh', input_shape=(nbfeatures,a),input_dim=nbfeatures))\n",
    "    model.add(Conv1D(filters=15, kernel_size=1, activation='tanh'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(15,input_shape=(15, nbfeatures), activation='tanh'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss=tensorflow.keras.losses.BinaryCrossentropy(reduction='sum'),optimizer='SGD', metrics=['binary_accuracy'])\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data_matrix, Labels, test_size=0.2, random_state=i)\n",
    "    y_test2=y_test.copy()\n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n",
    "    \n",
    "    #fitting data\n",
    "    model.fit(x_train, y_train,epochs=5,batch_size=1,class_weight=class_weight,callbacks=callback)\n",
    "\n",
    "    print('Iteration no.',i)\n",
    "    #Obtain the accuracy of prediction for each class\n",
    "    prediction= model.predict(x_test)\n",
    "    predicted_labels=[]\n",
    "    \n",
    "    for i in range(len(prediction)):\n",
    "        if prediction[i][0]>0.5:\n",
    "            predicted_labels.append(0)\n",
    "        else:\n",
    "            predicted_labels.append(1)\n",
    "    on_score=0\n",
    "    on_nbs=0\n",
    "    off_nbs=0\n",
    "    off_score=0\n",
    "    foff=0\n",
    "    fon=0\n",
    "    for i in range(len(y_test)):\n",
    "        if y_test2[i]==1 :\n",
    "            on_nbs+=1\n",
    "            if predicted_labels[i]==1:\n",
    "                on_score+=1\n",
    "            else : \n",
    "                fon+=1\n",
    "        if y_test2[i]==0 :\n",
    "            off_nbs+=1\n",
    "            if predicted_labels[i]==0:\n",
    "                off_score+=1 \n",
    "            else:\n",
    "                foff+=1\n",
    "    if on_nbs>0:    \n",
    "        ON_accuracy.append(100*(on_score/on_nbs))\n",
    "        FPOS.append(100*(fon/on_nbs))\n",
    "    OFF_accuracy.append(100*(off_score/off_nbs))\n",
    "    FNEG.append(100*(foff/off_nbs))\n",
    "    f1= f1_score(y_test2,predicted_labels,average='weighted')\n",
    "    fscore.append(f1)\n",
    "print(\"Accuracy for ON class: \",np.mean(ON_accuracy) ,\"%\")\n",
    "print(\"Accuracy for OFF class: \",np.mean(OFF_accuracy) ,\"%\")\n",
    "print(\"False Positive rate: \",np.mean(FPOS) ,\"%\")\n",
    "print(\"False Negative rate: \",np.mean(FNEG) ,\"%\")\n",
    "print(\"F1 score: \",np.mean(fscore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab5bd759",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "871/871 [==============================] - 2s 1ms/step - loss: 0.4110 - binary_accuracy: 0.7635\n",
      "Epoch 2/20\n",
      "871/871 [==============================] - 1s 1ms/step - loss: 0.3509 - binary_accuracy: 0.7646\n",
      "Epoch 3/20\n",
      "871/871 [==============================] - 1s 1ms/step - loss: 0.3427 - binary_accuracy: 0.7635\n",
      "Epoch 4/20\n",
      "871/871 [==============================] - 1s 1ms/step - loss: 0.3403 - binary_accuracy: 0.7658\n",
      "Epoch 5/20\n",
      "871/871 [==============================] - 1s 1ms/step - loss: 0.3378 - binary_accuracy: 0.7646\n",
      "Epoch 6/20\n",
      "871/871 [==============================] - 1s 1ms/step - loss: 0.3336 - binary_accuracy: 0.7646\n",
      "Epoch 7/20\n",
      "871/871 [==============================] - 1s 2ms/step - loss: 0.3301 - binary_accuracy: 0.7646\n",
      "Epoch 8/20\n",
      "871/871 [==============================] - 1s 1ms/step - loss: 0.3310 - binary_accuracy: 0.7635\n",
      "Epoch 9/20\n",
      "871/871 [==============================] - 1s 1ms/step - loss: 0.3312 - binary_accuracy: 0.7658\n",
      "Epoch 10/20\n",
      "871/871 [==============================] - 1s 1ms/step - loss: 0.3299 - binary_accuracy: 0.7658\n",
      "Iteration no. 1\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "Epoch 1/20\n",
      "871/871 [==============================] - 2s 1ms/step - loss: 0.4099 - binary_accuracy: 0.7836\n",
      "Epoch 2/20\n",
      "871/871 [==============================] - 1s 1ms/step - loss: 0.3391 - binary_accuracy: 0.7715\n",
      "Epoch 3/20\n",
      "871/871 [==============================] - 1s 1ms/step - loss: 0.3361 - binary_accuracy: 0.7750\n",
      "Epoch 4/20\n",
      "871/871 [==============================] - 1s 1ms/step - loss: 0.3297 - binary_accuracy: 0.7704\n",
      "Epoch 5/20\n",
      "871/871 [==============================] - 1s 1ms/step - loss: 0.3254 - binary_accuracy: 0.7715\n",
      "Epoch 6/20\n",
      "871/871 [==============================] - 1s 2ms/step - loss: 0.3239 - binary_accuracy: 0.7727\n",
      "Epoch 7/20\n",
      "871/871 [==============================] - 1s 1ms/step - loss: 0.3194 - binary_accuracy: 0.7727\n",
      "Iteration no. 2\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "Epoch 1/20\n",
      "871/871 [==============================] - 2s 1ms/step - loss: 0.4429 - binary_accuracy: 0.7618\n",
      "Epoch 2/20\n",
      "871/871 [==============================] - 1s 1ms/step - loss: 0.3585 - binary_accuracy: 0.7635\n",
      "Epoch 3/20\n",
      "871/871 [==============================] - 1s 1ms/step - loss: 0.3467 - binary_accuracy: 0.7646\n",
      "Epoch 4/20\n",
      "871/871 [==============================] - 1s 1ms/step - loss: 0.3407 - binary_accuracy: 0.7635\n",
      "Epoch 5/20\n",
      "871/871 [==============================] - 1s 1ms/step - loss: 0.3383 - binary_accuracy: 0.7635\n",
      "Epoch 6/20\n",
      "871/871 [==============================] - 1s 1ms/step - loss: 0.3369 - binary_accuracy: 0.7635\n",
      "Epoch 7/20\n",
      "871/871 [==============================] - 1s 1ms/step - loss: 0.3337 - binary_accuracy: 0.7635\n",
      "Epoch 8/20\n",
      "871/871 [==============================] - 1s 1ms/step - loss: 0.3405 - binary_accuracy: 0.7635\n",
      "Epoch 9/20\n",
      "871/871 [==============================] - 1s 1ms/step - loss: 0.3347 - binary_accuracy: 0.7635\n",
      "Iteration no. 3\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "Epoch 1/20\n",
      "871/871 [==============================] - 1s 1ms/step - loss: 0.3894 - binary_accuracy: 0.7664\n",
      "Epoch 2/20\n",
      "871/871 [==============================] - 1s 1ms/step - loss: 0.3481 - binary_accuracy: 0.7681\n",
      "Epoch 3/20\n",
      "871/871 [==============================] - 1s 1ms/step - loss: 0.3390 - binary_accuracy: 0.7681\n",
      "Epoch 4/20\n",
      "871/871 [==============================] - 1s 1ms/step - loss: 0.3343 - binary_accuracy: 0.7681\n",
      "Epoch 5/20\n",
      "871/871 [==============================] - 1s 1ms/step - loss: 0.3335 - binary_accuracy: 0.7681\n",
      "Epoch 6/20\n",
      "871/871 [==============================] - 1s 1ms/step - loss: 0.3317 - binary_accuracy: 0.7681\n",
      "Epoch 7/20\n",
      "871/871 [==============================] - 1s 1ms/step - loss: 0.3330 - binary_accuracy: 0.7704\n",
      "Epoch 8/20\n",
      "871/871 [==============================] - 1s 1ms/step - loss: 0.3285 - binary_accuracy: 0.7681\n",
      "Epoch 9/20\n",
      "871/871 [==============================] - 1s 1ms/step - loss: 0.3270 - binary_accuracy: 0.7681\n",
      "Epoch 10/20\n",
      "871/871 [==============================] - 1s 1ms/step - loss: 0.3243 - binary_accuracy: 0.7681\n",
      "Epoch 11/20\n",
      "871/871 [==============================] - 1s 1ms/step - loss: 0.3210 - binary_accuracy: 0.7704\n",
      "Epoch 12/20\n",
      "871/871 [==============================] - 1s 1ms/step - loss: 0.3229 - binary_accuracy: 0.7704\n",
      "Epoch 13/20\n",
      "871/871 [==============================] - 1s 1ms/step - loss: 0.3228 - binary_accuracy: 0.7681\n",
      "Iteration no. 4\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "Accuracy for ON class:  100.0 %\n",
      "Accuracy for OFF class:  73.82257608388261 %\n",
      "False Positive rate:  0.0 %\n",
      "False Negative rate:  26.177423916117384 %\n",
      "F1 score:  0.811248697299463\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "ON_accuracy=[]\n",
    "OFF_accuracy=[]\n",
    "fscore=[]\n",
    "FPOS=[]\n",
    "FNEG=[]\n",
    "#Measuring the meaned accuracy of  correct prediction of labellisation for each class  over a defined number of steps\n",
    "for i in range(1,5):\n",
    "    nb_filters=20\n",
    "    \n",
    "    callback=tensorflow.keras.callbacks.EarlyStopping(monitor='binary_accuracy',patience=6, verbose=0, mode='auto',baseline=None, restore_best_weights=True)\n",
    "\n",
    "    #Conv1D Architecture\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(nb_filters, 1, padding=\"same\", activation=\"tanh\",input_shape=(nbfeatures,binning)))\n",
    "    model.add(Conv1D(15, 1, padding=\"same\", activation=\"tanh\",input_shape=(nb_filters,binning)))\n",
    "    model.add(Conv1D(9, 1, padding=\"same\", activation=\"tanh\",input_shape=(15,binning)))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(9, activation='tanh'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss=tensorflow.keras.losses.BinaryCrossentropy(reduction='sum'),optimizer='SGD', metrics=['binary_accuracy'])\n",
    "\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data_matrix, Labels, test_size=0.2, random_state=i)\n",
    "    y_test2=y_test.copy()\n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n",
    "    \n",
    "    #fitting data\n",
    "    model.fit(x_train, y_train,epochs=20,batch_size=1,class_weight=class_weight,callbacks=callback)\n",
    "\n",
    "    print('Iteration no.',i)\n",
    "    #Obtain the accuracy of prediction for each class\n",
    "    prediction= model.predict(x_test)\n",
    "    predicted_labels=[]\n",
    "    \n",
    "    for i in range(len(prediction)):\n",
    "        if prediction[i][0]>0.5:\n",
    "            predicted_labels.append(0)\n",
    "        else:\n",
    "            predicted_labels.append(1)\n",
    "    on_score=0\n",
    "    on_nbs=0\n",
    "    off_nbs=0\n",
    "    off_score=0\n",
    "    foff=0\n",
    "    fon=0\n",
    "    for i in range(len(y_test)):\n",
    "        if y_test2[i]==1 :\n",
    "            on_nbs+=1\n",
    "            if predicted_labels[i]==1:\n",
    "                on_score+=1\n",
    "            else : \n",
    "                fon+=1\n",
    "        if y_test2[i]==0 :\n",
    "            off_nbs+=1\n",
    "            if predicted_labels[i]==0:\n",
    "                off_score+=1 \n",
    "            else:\n",
    "                foff+=1\n",
    "    if on_nbs>0:    \n",
    "        ON_accuracy.append(100*(on_score/on_nbs))\n",
    "        FPOS.append(100*(fon/on_nbs))\n",
    "    OFF_accuracy.append(100*(off_score/off_nbs))\n",
    "    FNEG.append(100*(foff/off_nbs))\n",
    "    f1= f1_score(y_test2,predicted_labels,average='weighted')\n",
    "    fscore.append(f1)\n",
    "print(\"Accuracy for ON class: \",np.mean(ON_accuracy) ,\"%\")\n",
    "print(\"Accuracy for OFF class: \",np.mean(OFF_accuracy) ,\"%\")\n",
    "print(\"False Positive rate: \",np.mean(FPOS) ,\"%\")\n",
    "print(\"False Negative rate: \",np.mean(FNEG) ,\"%\")\n",
    "print(\"F1 score: \",np.mean(fscore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a6038e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
