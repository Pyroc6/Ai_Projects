{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff1139a5",
   "metadata": {},
   "source": [
    "# PYTS/BOSSVS classification algorithm test and measured accuracies\n",
    "# Tested on transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06b1435d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import sys \n",
    "import shutil\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import dstack\n",
    "from pandas import read_csv\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ShuffleSplit # or StratifiedShuffleSplit\n",
    "from pandas import DataFrame\n",
    "import sktime\n",
    "from sktime.classification.distance_based import KNeighborsTimeSeriesClassifier\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "527823ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathON=\"C:/Users/pierr/Documents_kanop/Shps/Pierre_points_for_louis/Bureau/interpo_ONOFF/ON_data/\"\n",
    "pathOFF=\"C:/Users/pierr/Documents_kanop/Shps/Pierre_points_for_louis/Bureau/interpo_ONOFF/OFF_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d6ba841",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing in a list the adresses of all the data files in the notebook\n",
    "\n",
    "data_files_ON=[]\n",
    "file_names_ON=[]\n",
    "\n",
    "data_files_OFF=[]\n",
    "file_names_OFF=[]\n",
    "data_files_ALL=[]\n",
    "file_names_ALL=[]\n",
    "\n",
    "#Storing in a list the adresses of all the data files in the notebook\n",
    "for filename in os.listdir(pathOFF):\n",
    "    f = os.path.join(pathOFF,filename)\n",
    "    if os.path.isfile(f):\n",
    "        data_files_OFF.append(f)\n",
    "        file_names_OFF.append(filename)\n",
    "        data_files_ALL.append(f)\n",
    "        file_names_ALL.append(filename)\n",
    "for filename in os.listdir(pathON):\n",
    "    f = os.path.join(pathON,filename)\n",
    "    if os.path.isfile(f):\n",
    "        data_files_ON.append(f)\n",
    "        file_names_ON.append(filename)\n",
    "        data_files_ALL.append(f)\n",
    "        file_names_ALL.append(filename)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e8e998f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using data_files with only the 36 binning\n",
    "Filter=True\n",
    "binning=37\n",
    "\n",
    "\n",
    "if Filter==True:\n",
    "    \n",
    "    idx_OFF=[]\n",
    "    idx_ON=[]\n",
    "    dataON=[]\n",
    "    dataOFF=[]\n",
    "    for i in range(len(data_files_OFF)):\n",
    "        dataframe = pd.read_csv(data_files_OFF[i])\n",
    "        lg = len(dataframe)\n",
    "        \n",
    "        if lg==binning:\n",
    "            idx_OFF.append(i)\n",
    "    \n",
    "    for i in range(len(data_files_ON)):\n",
    "        dataframe = pd.read_csv(data_files_ON[i])\n",
    "        lg = len(dataframe)\n",
    "\n",
    "        if lg==binning:\n",
    "            idx_ON.append(i)\n",
    "\n",
    "\n",
    "    for i in range(len(idx_OFF)):\n",
    "\n",
    "        a=idx_OFF[i]\n",
    "        dataOFF.append(data_files_OFF[a])\n",
    "    for i in range(len(idx_ON)):\n",
    "\n",
    "        a=idx_ON[i]\n",
    "        dataON.append(data_files_ON[a])\n",
    "\n",
    "idx = idx_OFF+idx_ON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9392c0f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1140, 3, 37)\n",
      "Accuracy for ON class:  100.0 %\n",
      "Accuracy for OFF class:  67.50103470217321 %\n",
      "False Positive rate:  0.0 %\n",
      "False Negative rate:  32.49896529782678 %\n",
      "F1 score:  0.7832726086419057\n",
      "[['4FGL J1443+2501.csv', 1], ['4FGL J1550+0528.csv', 1], ['4FGL J1558+5625.csv', 1], ['4FGL J1559+2319.csv', 2], ['4FGL J1600+8510.csv', 1], ['4FGL J1603+5009.csv', 1], ['4FGL J1604+5714.csv', 5], ['4FGL J1613+3411.csv', 1], ['4FGL J1615+2130.csv', 1], ['4FGL J1619+5536.csv', 2], ['4FGL J1621-1103.csv', 1], ['4FGL J1625+4134.csv', 1], ['4FGL J1630+5221.csv', 1], ['4FGL J1630+8234.csv', 3], ['4FGL J1635+3808.csv', 1], ['4FGL J1637+4717.csv', 1], ['4FGL J1638+5721.csv', 2], ['4FGL J1639+4129.csv', 1], ['4FGL J1640+3945.csv', 1], ['4FGL J1640+6850.csv', 1], ['4FGL J1642+3948.csv', 2], ['4FGL J1647+4950.csv', 1], ['4FGL J1649+5235.csv', 2], ['4FGL J1650+0831.csv', 1], ['4FGL J1651+7219.csv', 1], ['4FGL J1652+4024.csv', 1], ['4FGL J1656-2010.csv', 2], ['4FGL J1700+6830.csv', 2], ['4FGL J1701+3956.csv', 1], ['4FGL J1702+3114.csv', 1], ['4FGL J1704+7647.csv', 3], ['4FGL J1704-0527.csv', 1], ['4FGL J1705+5436.csv', 1], ['4FGL J1707+1649.csv', 2], ['4FGL J1709+4318.csv', 2], ['4FGL J1716+6836.csv', 1], ['4FGL J1719+1205.csv', 3], ['4FGL J1719+1745.csv', 1], ['4FGL J1722+1014.csv', 2], ['4FGL J1727+4530.csv', 3], ['4FGL J1728+1216.csv', 1], ['4FGL J1728+5013.csv', 2], ['4FGL J1730+3715.csv', 2], ['4FGL J1736+2033.csv', 1], ['4FGL J1739+4955.csv', 1], ['4FGL J1740+4737.csv', 2], ['4FGL J1740+5346.csv', 3], ['4FGL J1744+1935.csv', 3], ['4FGL J1748+3403.csv', 1], ['4FGL J1748+7005.csv', 1], ['4FGL J1749+4321.csv', 2], ['4FGL J1751+2921.csv', 2], ['4FGL J1754+3212.csv', 3], ['4FGL J1756+5522.csv', 1], ['4FGL J1801+4404.csv', 1], ['4FGL J1806+6949.csv', 2], ['4FGL J1808+3500.csv', 1], ['4FGL J1809+2910.csv', 3], ['4FGL J1813+3144.csv', 2], ['4FGL J1821+6819.csv', 2], ['4FGL J1823+6858.csv', 3], ['4FGL J1824+5651.csv', 3], ['4FGL J1828+3230.csv', 1], ['4FGL J1829+2729.csv', 2], ['4FGL J1829+4845.csv', 1], ['4FGL J1829+5402.csv', 2], ['4FGL J1830+1324.csv', 2], ['4FGL J1836+1948.csv', 2], ['4FGL J1837+5347.csv', 1], ['4FGL J1838+4802.csv', 2], ['4FGL J1841+2909.csv', 1], ['4FGL J1841+3218.csv', 1], ['4FGL J1842+6810.csv', 1], ['4FGL J1844+1547.csv', 1], ['4FGL J1844+5709.csv', 2], ['4FGL J1848+3217.csv', 3], ['4FGL J1848+3243.csv', 2], ['4FGL J1849+6705.csv', 1], ['4FGL J1852+4856.csv', 2], ['4FGL J1858+7318.csv', 1], ['4FGL J1911-1908.csv', 3], ['4FGL J1911-2006.csv', 2], ['4FGL J1913+4439.csv', 1], ['4FGL J1917-1921.csv', 1], ['4FGL J1921-1231.csv', 1], ['4FGL J1921-1607.csv', 1], ['4FGL J1923-2104.csv', 2], ['4FGL J1925+2815.csv', 1], ['4FGL J1926+6154.csv', 1], ['4FGL J1934+6002.csv', 2], ['4FGL J1934+6541.csv', 2], ['4FGL J1944+2117.csv', 1], ['4FGL J1944+3921.csv', 2], ['4FGL J1944-2143.csv', 2], ['4FGL J1949+0906.csv', 2], ['4FGL J1949+1314.csv', 1], ['4FGL J1950+1211.csv', 2], ['4FGL J1955+0214.csv', 2], ['4FGL J1955+1358.csv', 1], ['4FGL J1955-1604.csv', 1], ['4FGL J2000+6508.csv', 1], ['4FGL J2000-1328.csv', 1], ['4FGL J2000-1748.csv', 1], ['4FGL J2005+6424.csv', 1], ['4FGL J2005+7003.csv', 1], ['4FGL J2007+6607.csv', 1], ['4FGL J2010+7229.csv', 1], ['4FGL J2012+4629.csv', 2], ['4FGL J2012-1646.csv', 1], ['4FGL J2014-0047.csv', 2], ['4FGL J2016-0903.csv', 3], ['4FGL J2021+0629.csv', 2], ['4FGL J2023+3153.csv', 2], ['4FGL J2023-0123.csv', 1], ['4FGL J2023-1139.csv', 2], ['4FGL J2024-0847.csv', 1], ['4FGL J2025+3341.csv', 1], ['4FGL J2034+1154.csv', 1], ['4FGL J2039+5218.csv', 2], ['4FGL J2039-1046.csv', 2], ['4FGL J2040-1705.csv', 1], ['4FGL J2049+1002.csv', 1], ['4FGL J2053+2922.csv', 3], ['4FGL J2055-0020.csv', 3], ['4FGL J2056+4939.csv', 1], ['4FGL J2103-1112.csv', 1], ['4FGL J2104-0212.csv', 1], ['4FGL J2108+3655.csv', 2], ['4FGL J2108-0250.csv', 2], ['4FGL J2110+0808.csv', 4], ['4FGL J2112+0819.csv', 2], ['4FGL J2115+1218.csv', 1], ['4FGL J2119-1105.csv', 1], ['4FGL J2121+1901.csv', 1], ['4FGL J2133+6646.csv', 3], ['4FGL J2134-0154.csv', 3], ['4FGL J2134-2130.csv', 1], ['4FGL J2148-0733.csv', 2], ['4FGL J2149+0323.csv', 2], ['4FGL J2149+1917.csv', 2], ['4FGL J2150-1410.csv', 2], ['4FGL J2156+1818.csv', 1], ['4FGL J2156-0036.csv', 1], ['4FGL J2157+3127.csv', 2], ['4FGL J2200+2138.csv', 1], ['4FGL J2202+4216.csv', 2], ['4FGL J2203+1725.csv', 2], ['4FGL J2204+0438.csv', 2], ['4FGL J2206-0032.csv', 2], ['4FGL J2207+4316.csv', 2], ['4FGL J2209-0451.csv', 3], ['4FGL J2211-1325.csv', 1], ['4FGL J2212+2356.csv', 1], ['4FGL J2212+2800.csv', 1], ['4FGL J2225-0457.csv', 1], ['4FGL J2229-0832.csv', 2], ['4FGL J2232+1143.csv', 2], ['4FGL J2236+2828.csv', 3], ['4FGL J2236-1433.csv', 1], ['4FGL J2236-1706.csv', 1], ['4FGL J2241+4120.csv', 1], ['4FGL J2243-1231.csv', 2], ['4FGL J2244+4057.csv', 2], ['4FGL J2248+2106.csv', 1], ['4FGL J2252+4031.csv', 3], ['4FGL J2255+2411.csv', 1], ['4FGL J2256-2011.csv', 2], ['4FGL J2304+3704.csv', 1], ['4FGL J2311+0205.csv', 1], ['4FGL J2313+3945.csv', 1], ['4FGL J2321+3204.csv', 2], ['4FGL J2321+5111.csv', 1], ['4FGL J2321-1619.csv', 1], ['4FGL J2322+3436.csv', 1], ['4FGL J2323+4210.csv', 2], ['4FGL J2323-0317.csv', 1], ['4FGL J2323-0617.csv', 1], ['4FGL J2325+1821.csv', 1], ['4FGL J2329+3755.csv', 1], ['4FGL J2329+6101.csv', 1], ['4FGL J2330+7759.csv', 1], ['4FGL J2338+2124.csv', 1], ['4FGL J2340+8015.csv', 1], ['4FGL J2343+3438.csv', 2], ['4FGL J2346+0705.csv', 2], ['4FGL J2347+5141.csv', 2], ['4FGL J2347+5436.csv', 2], ['4FGL J2348-1630.csv', 1], ['4FGL J2352+1750.csv', 2], ['4FGL J2356+4036.csv', 1], ['4FGL J2357-1718.csv', 1], ['4FGL J2358+3830.csv', 2], ['4FGL J2358-1808.csv', 2], ['4FGL J2359-2049.csv', 1], ['4FGLJ0001+2113.csv', 1], ['4FGLJ0001-0747.csv', 1], ['4FGLJ0003-1149.csv', 2], ['4FGLJ0003-1928.csv', 1], ['4FGLJ0005+3824.csv', 2], ['4FGLJ0007+4008.csv', 1], ['4FGLJ0008+4711.csv', 1], ['4FGLJ0011+0057.csv', 3], ['4FGLJ0014+3212.csv', 1], ['4FGLJ0014+6118.csv', 1], ['4FGLJ0014-0500.csv', 2], ['4FGLJ0015+5551.csv', 2], ['4FGLJ0019+7327.csv', 3], ['4FGLJ0022+0608.csv', 1], ['4FGLJ0022-1854.csv', 1], ['4FGLJ0028+7505.csv', 1], ['4FGLJ0030-0212.csv', 2], ['4FGLJ0033-1921.csv', 2], ['4FGLJ0035+1514.csv', 2], ['4FGLJ0035+5950.csv', 1], ['4FGLJ0037+1239.csv', 1], ['4FGLJ0039-0946.csv', 2], ['4FGLJ0041+6052.csv', 2], ['4FGLJ0042+2319.csv', 1], ['4FGLJ0043+3425.csv', 2], ['4FGLJ0045+1217.csv', 2], ['4FGLJ0047+2233.csv', 1], ['4FGLJ0047+3947.csv', 2], ['4FGLJ0049+0237.csv', 2], ['4FGLJ0050-0929.csv', 1], ['4FGLJ0151+8601.csv', 1], ['4FGLJ0203+3042.csv', 1], ['4FGLJ0205+3212.csv', 1], ['4FGLJ0250+8435.csv', 1], ['4FGLJ0256+0334.csv', 1], ['4FGLJ0308+0407.csv', 1], ['4FGLJ0312+0134.csv', 1], ['4FGLJ0314+0620.csv', 2], ['4FGLJ0316+0905.csv', 1], ['4FGLJ0319+4130.csv', 2], ['4FGLJ0330+0438.csv', 1], ['4FGLJ0509+1012.csv', 1], ['4FGLJ0515+1527.csv', 2], ['4FGLJ0733+0455.csv', 1], ['4FGLJ1103+1157.csv', 1], ['4FGLJ1512+0202.csv', 1], ['4FGLJ1546+0819.csv', 3], ['4FGLJ1548+1456.csv', 1], ['4FGLJ1553+1257.csv', 1], ['4FGLJ1607+1550.csv', 1], ['4FGLJ1631+1046.csv', 1], ['4FGLJ1632+0854.csv', 3], ['4FGLJ1702+2642.csv', 3], ['4FGLJ1736+0628.csv', 1], ['4FGLJ2015+3710.csv', 1], ['4FGLJ2018+3852.csv', 2], ['4FGLJ2108+1434.csv', 2], ['4FGLJ2227+0036.csv', 1], ['4FGLJ2245+1544.csv', 1], ['4FGLJ2247-0001.csv', 2]]\n"
     ]
    }
   ],
   "source": [
    "from pyts.classification import BOSSVS\n",
    "from pyts.multivariate.classification import MultivariateClassifier\n",
    "\n",
    "iteration=5\n",
    "lgON=len(dataON)\n",
    "lgOFF=len(dataOFF)\n",
    "lg=lgON+lgOFF\n",
    "\n",
    "binning=37\n",
    "\n",
    "Labels = []\n",
    "Q_ON= []\n",
    "\n",
    "nbfeatures=3\n",
    "ON_accuracy=[]\n",
    "OFF_accuracy=[]\n",
    "fscore=[]\n",
    "FPOS=[]\n",
    "FNEG= []\n",
    "a=binning\n",
    "b=nbfeatures\n",
    "c=len(data_files_ALL)\n",
    "\n",
    "# multivariate\n",
    "y=np.zeros((2,lg))\n",
    "bigdata= np.zeros((b,c,a))\n",
    "\n",
    "for j in range(len(data_files_ALL)):\n",
    "\n",
    "\n",
    "    dataframe=pd.read_csv(data_files_ALL[j],index_col=[0])\n",
    "    dataframe.columns=['MJD','Flux','Delta Flux','Photon Index','Delta Index','TS','fratio']\n",
    "\n",
    "    bigdata[0][j]=dataframe['Flux']\n",
    "    bigdata[1][j]=dataframe['Photon Index']*dataframe['Flux']\n",
    "    bigdata[2][j]=dataframe['Photon Index']\n",
    "\n",
    "for i in range(lgOFF):\n",
    "    \n",
    "    Labels.append(int(0))\n",
    "    \n",
    "for i in range(lgON): \n",
    "    \n",
    "    \n",
    "    Labels.append(int(1))\n",
    "    \n",
    "    \n",
    "bigdata=bigdata.reshape(c,nbfeatures,binning)    \n",
    "print(bigdata.shape)\n",
    "for i in range(iteration):\n",
    "    \n",
    "    #3D shape of data, multivariate time series\n",
    "\n",
    "    sss = ShuffleSplit(n_splits=1, test_size=0.2,random_state=i)\n",
    "    X=bigdata\n",
    "    y=Labels\n",
    "\n",
    "    sss.get_n_splits(X, y)\n",
    "    train_index, test_index = next(sss.split(X, y)) \n",
    "\n",
    "    x_train, x_test = X[train_index], X[test_index] \n",
    "\n",
    "    y_train=[]\n",
    "    y_test=[]\n",
    "\n",
    "    for i in range(len(train_index)):\n",
    "        a=train_index[i]\n",
    "        y_train.append(y[a])\n",
    "    for i in range(len(test_index)):\n",
    "\n",
    "        a=test_index[i]\n",
    "        y_test.append(y[a])\n",
    "\n",
    "    y_test2=y_test.copy()\n",
    "\n",
    "    clf = MultivariateClassifier(BOSSVS(n_bins=6,anova=True,word_size=9,drop_sum=True))\n",
    "    clf.fit(x_train, y_train)\n",
    "    MultivariateClassifier(...)\n",
    "    clf.predict(x_test)\n",
    "\n",
    "    prediction=np.array(clf.predict(x_test))\n",
    "    on_score=0\n",
    "    on_nbs=0\n",
    "    off_nbs=0\n",
    "    off_score=0\n",
    "    foff=0\n",
    "    fon=0\n",
    "    for i in range(len(y_test)):\n",
    "        if y_test2[i]==1 :\n",
    "            on_nbs+=1\n",
    "            if prediction[i]==1:\n",
    "                on_score+=1\n",
    "                a=test_index[i]\n",
    "                Q_ON.append(a)\n",
    "            else : \n",
    "                fon+=1\n",
    "        if y_test2[i]==0 :\n",
    "            off_nbs+=1\n",
    "            if prediction[i]==0:\n",
    "                off_score+=1 \n",
    "            else:\n",
    "                foff+=1\n",
    "                a=test_index[i]\n",
    "                Q_ON.append(a)\n",
    "    if on_nbs>0:    \n",
    "        ON_accuracy.append(100*(on_score/on_nbs))\n",
    "        FPOS.append(100*(fon/on_nbs))\n",
    "    OFF_accuracy.append(100*(off_score/off_nbs))\n",
    "    FNEG.append(100*(foff/off_nbs))\n",
    "    f1= f1_score(y_test2,prediction,average='weighted')\n",
    "    fscore.append(f1)\n",
    "    \n",
    "print(\"Accuracy for ON class: \",np.mean(ON_accuracy) ,\"%\")\n",
    "print(\"Accuracy for OFF class: \",np.mean(OFF_accuracy) ,\"%\")\n",
    "print(\"False Positive rate: \",np.mean(FPOS) ,\"%\")\n",
    "print(\"False Negative rate: \",np.mean(FNEG) ,\"%\")\n",
    "print(\"F1 score: \",np.mean(fscore))\n",
    "\n",
    "\n",
    "\n",
    "gg = np.unique(Q_ON)\n",
    "Q_ON2=[]\n",
    "Q_ON3=[]\n",
    "errOFF=[]\n",
    "\n",
    "\n",
    "\n",
    "if len(gg)>0:\n",
    "    for i in range(len(Q_ON)):\n",
    "        aa=Q_ON[i]\n",
    "        Q_ON2.append(file_names_ALL[aa])\n",
    "    gg = np.unique(Q_ON2)    \n",
    "    for i in range(len(gg)):\n",
    "        bb=gg[i]\n",
    "        count = Q_ON2.count(bb)\n",
    "        if count>int(iteration/7):\n",
    "            Q_ON3.append([bb,count])\n",
    "            errOFF.append(bb)\n",
    "print(Q_ON3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "863aeb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1140, 37)\n",
      "Accuracy for ON class:  14.754689754689753 %\n",
      "Accuracy for OFF class:  87.80905901477242 %\n",
      "False Positive rate:  85.24531024531024 %\n",
      "False Negative rate:  12.19094098522759 %\n",
      "F1 score:  0.887195231690303\n",
      "[['4FGL J0056-2118.csv', 1], ['4FGL J0102+5824.csv', 1], ['4FGL J0115+2519.csv', 1], ['4FGL J0136+3906.csv', 1], ['4FGL J0205-1700.csv', 1], ['4FGL J0211+1051.csv', 1], ['4FGL J0217+7352.csv', 4], ['4FGL J0226-0553.csv', 2], ['4FGL J0231+1322.csv', 2], ['4FGL J0242+5216.csv', 2], ['4FGL J0319+1845.csv', 1], ['4FGL J0359+5057.csv', 2], ['4FGL J0426+6826.csv', 1], ['4FGL J0442-0017.csv', 1], ['4FGL J0444+3425.csv', 2], ['4FGL J0501-0158.csv', 1], ['4FGL J0505+0415.csv', 1], ['4FGL J0505+0459.csv', 2], ['4FGL J0521+2112.csv', 1], ['4FGL J0555+3947.csv', 2], ['4FGL J0608-1521.csv', 2], ['4FGL J0709-0255.csv', 2], ['4FGL J0723-0732.csv', 2], ['4FGL J0725+1425.csv', 1], ['4FGL J0730-1141.csv', 1], ['4FGL J0738+1742.csv', 1], ['4FGL J0739+0137.csv', 1], ['4FGL J0749+1058.csv', 1], ['4FGL J0808-0751.csv', 1], ['4FGL J0816+2050.csv', 2], ['4FGL J0829-1140.csv', 1], ['4FGL J0830+2410.csv', 1], ['4FGL J0839+0105.csv', 2], ['4FGL J0909+0121.csv', 1], ['4FGL J0909-0230.csv', 2], ['4FGL J1006-2159.csv', 2], ['4FGL J1023+3949.csv', 1], ['4FGL J1044+8053.csv', 1], ['4FGL J1058+0133.csv', 2], ['4FGL J1117+2013.csv', 1], ['4FGL J1118-1235.csv', 2], ['4FGL J1129-1447.csv', 2], ['4FGL J1131+3815.csv', 1], ['4FGL J1154+4037.csv', 1], ['4FGL J1208+5441.csv', 1], ['4FGL J1217+3007.csv', 1], ['4FGL J1218-0028.csv', 1], ['4FGL J1229+0202.csv', 2], ['4FGL J1250+0217.csv', 2], ['4FGL J1310+3221.csv', 1], ['4FGL J1312-0425.csv', 1], ['4FGL J1330+7002.csv', 1], ['4FGL J1332-1256.csv', 1], ['4FGL J1344-1723.csv', 2], ['4FGL J1349-1131.csv', 1], ['4FGL J1419-0838.csv', 1], ['4FGL J1424+0433.csv', 1], ['4FGL J1435+2021.csv', 2], ['4FGL J1443+2501.csv', 1], ['4FGL J1454+5124.csv', 1], ['4FGL J1506+3731.csv', 2], ['4FGL J1514-0949.csv', 2], ['4FGL J1522+3144.csv', 1], ['4FGL J1533+3416.csv', 1], ['4FGL J1604+5714.csv', 2], ['4FGL J1635+3808.csv', 1], ['4FGL J1640+3945.csv', 1], ['4FGL J1642+3948.csv', 2], ['4FGL J1707+1649.csv', 2], ['4FGL J1709+4318.csv', 2], ['4FGL J1740+5346.csv', 1], ['4FGL J1842+6810.csv', 1], ['4FGL J1844+1547.csv', 1], ['4FGL J1849+6705.csv', 1], ['4FGL J1858+7318.csv', 1], ['4FGL J1911-2006.csv', 2], ['4FGL J1925+2815.csv', 1], ['4FGL J2005+6424.csv', 1], ['4FGL J2021+0629.csv', 2], ['4FGL J2023+3153.csv', 1], ['4FGL J2110+0808.csv', 4], ['4FGL J2115+1218.csv', 1], ['4FGL J2149+0323.csv', 2], ['4FGL J2157+3127.csv', 2], ['4FGL J2211-1325.csv', 1], ['4FGL J2229-0832.csv', 1], ['4FGL J2232+1143.csv', 2], ['4FGL J2236-1433.csv', 1], ['4FGL J2243-1231.csv', 2], ['4FGL J2321+3204.csv', 1], ['4FGL J2323+4210.csv', 2], ['4FGL J2348-1630.csv', 1], ['4FGL J2359-2049.csv', 1], ['4FGLJ0028+7505.csv', 1], ['4FGLJ0205+3212.csv', 1], ['4FGLJ0319+4130.csv', 1], ['4FGLJ1103+1157.csv', 1], ['4FGLJ1736+0628.csv', 1], ['4FGLJ2015+3710.csv', 1], ['4FGLJ2247-0001.csv', 1]]\n"
     ]
    }
   ],
   "source": [
    "from pyts.classification import BOSSVS\n",
    "from pyts.multivariate.classification import MultivariateClassifier\n",
    "\n",
    "iteration=5\n",
    "lgON=len(dataON)\n",
    "lgOFF=len(dataOFF)\n",
    "lg=lgON+lgOFF\n",
    "\n",
    "binning=37\n",
    "\n",
    "Labels = []\n",
    "Q_ON= []\n",
    "\n",
    "nbfeatures=1\n",
    "ON_accuracy=[]\n",
    "OFF_accuracy=[]\n",
    "fscore=[]\n",
    "FPOS=[]\n",
    "FNEG= []\n",
    "a=binning\n",
    "b=nbfeatures\n",
    "c=len(data_files_ALL)\n",
    "\n",
    "# multivariate\n",
    "y=np.zeros((2,lg))\n",
    "bigdata= np.zeros((b,c,a))\n",
    "\n",
    "for j in range(len(data_files_ALL)):\n",
    "\n",
    "\n",
    "    dataframe=pd.read_csv(data_files_ALL[j],index_col=[0])\n",
    "    dataframe.columns=['MJD','Flux','Delta Flux','Photon Index','Delta Index','TS','fratio']\n",
    "\n",
    "    bigdata[0][j]=dataframe['Flux']\n",
    "    bigdata[0][j]=dataframe['Photon Index']*dataframe['Flux']\n",
    "    bigdata[0][j]=dataframe['Photon Index']\n",
    "\n",
    "for i in range(lgOFF):\n",
    "    \n",
    "    Labels.append(int(0))\n",
    "    \n",
    "for i in range(lgON): \n",
    "    \n",
    "    \n",
    "    Labels.append(int(1))\n",
    "    \n",
    "    \n",
    "bigdata=bigdata.reshape(c,binning)    \n",
    "print(bigdata.shape)\n",
    "for i in range(iteration):\n",
    "    \n",
    "    #3D shape of data, multivariate time series\n",
    "\n",
    "    sss = ShuffleSplit(n_splits=1, test_size=0.2,random_state=i)\n",
    "    X=bigdata\n",
    "    y=Labels\n",
    "\n",
    "    sss.get_n_splits(X, y)\n",
    "    train_index, test_index = next(sss.split(X, y)) \n",
    "\n",
    "    x_train, x_test = X[train_index], X[test_index] \n",
    "\n",
    "    y_train=[]\n",
    "    y_test=[]\n",
    "\n",
    "    for i in range(len(train_index)):\n",
    "        a=train_index[i]\n",
    "        y_train.append(y[a])\n",
    "    for i in range(len(test_index)):\n",
    "\n",
    "        a=test_index[i]\n",
    "        y_test.append(y[a])\n",
    "\n",
    "    y_test2=y_test.copy()\n",
    "\n",
    "    clf = BOSSVS(n_bins=6,anova=True,word_size=9,drop_sum=True)\n",
    "    clf.fit(x_train, y_train)\n",
    "    MultivariateClassifier(...)\n",
    "    clf.predict(x_test)\n",
    "\n",
    "    prediction=np.array(clf.predict(x_test))\n",
    "    on_score=0\n",
    "    on_nbs=0\n",
    "    off_nbs=0\n",
    "    off_score=0\n",
    "    foff=0\n",
    "    fon=0\n",
    "    for i in range(len(y_test)):\n",
    "        if y_test2[i]==1 :\n",
    "            on_nbs+=1\n",
    "            if prediction[i]==1:\n",
    "                on_score+=1\n",
    "                a=test_index[i]\n",
    "                Q_ON.append(a)\n",
    "            else : \n",
    "                fon+=1\n",
    "        if y_test2[i]==0 :\n",
    "            off_nbs+=1\n",
    "            if prediction[i]==0:\n",
    "                off_score+=1 \n",
    "            else:\n",
    "                foff+=1\n",
    "                a=test_index[i]\n",
    "                Q_ON.append(a)\n",
    "    if on_nbs>0:    \n",
    "        ON_accuracy.append(100*(on_score/on_nbs))\n",
    "        FPOS.append(100*(fon/on_nbs))\n",
    "    OFF_accuracy.append(100*(off_score/off_nbs))\n",
    "    FNEG.append(100*(foff/off_nbs))\n",
    "    f1= f1_score(y_test2,prediction,average='weighted')\n",
    "    fscore.append(f1)\n",
    "    \n",
    "print(\"Accuracy for ON class: \",np.mean(ON_accuracy) ,\"%\")\n",
    "print(\"Accuracy for OFF class: \",np.mean(OFF_accuracy) ,\"%\")\n",
    "print(\"False Positive rate: \",np.mean(FPOS) ,\"%\")\n",
    "print(\"False Negative rate: \",np.mean(FNEG) ,\"%\")\n",
    "print(\"F1 score: \",np.mean(fscore))\n",
    "\n",
    "\n",
    "\n",
    "gg = np.unique(Q_ON)\n",
    "Q_ON2=[]\n",
    "Q_ON3=[]\n",
    "errOFF=[]\n",
    "\n",
    "\n",
    "\n",
    "if len(gg)>0:\n",
    "    for i in range(len(Q_ON)):\n",
    "        aa=Q_ON[i]\n",
    "        Q_ON2.append(file_names_ALL[aa])\n",
    "    gg = np.unique(Q_ON2)    \n",
    "    for i in range(len(gg)):\n",
    "        bb=gg[i]\n",
    "        count = Q_ON2.count(bb)\n",
    "        if count>int(iteration/7):\n",
    "            Q_ON3.append([bb,count])\n",
    "            errOFF.append(bb)\n",
    "print(Q_ON3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b8a8771",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing in a list the adresses of all the data files in the notebook\n",
    "\n",
    "data_files_ON=[]\n",
    "file_names_ON=[]\n",
    "\n",
    "data_files_OFF=[]\n",
    "file_names_OFF=[]\n",
    "data_files_ALL=[]\n",
    "file_names_ALL=[]\n",
    "\n",
    "#Storing in a list the adresses of all the data files in the notebook\n",
    "for filename in os.listdir(pathOFF):\n",
    "    f = os.path.join(pathOFF,filename)\n",
    "    if os.path.isfile(f) and filename in errOFF :\n",
    "        data_files_OFF.append(f)\n",
    "        file_names_OFF.append(filename)\n",
    "        data_files_ALL.append(f)\n",
    "        file_names_ALL.append(filename)\n",
    "        \n",
    "for filename in os.listdir(pathON):\n",
    "    f = os.path.join(pathON,filename)\n",
    "    if os.path.isfile(f):\n",
    "        data_files_ON.append(f)\n",
    "        file_names_ON.append(filename)\n",
    "        data_files_ALL.append(f)\n",
    "        file_names_ALL.append(filename)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9033fed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using data_files with only the 36 binning\n",
    "Filter=True\n",
    "binning=37\n",
    "\n",
    "\n",
    "if Filter==True:\n",
    "    \n",
    "    idx_OFF=[]\n",
    "    idx_ON=[]\n",
    "    dataON=[]\n",
    "    dataOFF=[]\n",
    "    for i in range(len(data_files_OFF)):\n",
    "        dataframe = pd.read_csv(data_files_OFF[i])\n",
    "        lg = len(dataframe)\n",
    "        \n",
    "        if lg==binning:\n",
    "            idx_OFF.append(i)\n",
    "    \n",
    "    for i in range(len(data_files_ON)):\n",
    "        dataframe = pd.read_csv(data_files_ON[i])\n",
    "        lg = len(dataframe)\n",
    "\n",
    "        if lg==binning:\n",
    "            idx_ON.append(i)\n",
    "\n",
    "\n",
    "    for i in range(len(idx_OFF)):\n",
    "\n",
    "        a=idx_OFF[i]\n",
    "        dataOFF.append(data_files_OFF[a])\n",
    "    for i in range(len(idx_ON)):\n",
    "\n",
    "        a=idx_ON[i]\n",
    "        dataON.append(data_files_ON[a])\n",
    "\n",
    "idx = idx_OFF+idx_ON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2421f965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(366, 3, 37)\n",
      "Accuracy for ON class:  100.0 %\n",
      "Accuracy for OFF class:  93.19517316169927 %\n",
      "False Positive rate:  0.0 %\n",
      "False Negative rate:  6.80482683830072 %\n",
      "F1 score:  0.9532897723752309\n",
      "74\n",
      "24\n",
      "[['4FGLJ0047+2233.csv', 36], ['4FGLJ0203+7233.csv', 44], ['4FGLJ0205-1700.csv', 40], ['4FGLJ0209+4449.csv', 42], ['4FGLJ0209+7229.csv', 44], ['4FGLJ0211+1051.csv', 39], ['4FGLJ0212+2244.csv', 47], ['4FGLJ0214+5145.csv', 42], ['4FGLJ0215+0300.csv', 34], ['4FGLJ0216+2313.csv', 41], ['4FGLJ0216-1015.csv', 41], ['4FGLJ0217+0837.csv', 38], ['4FGLJ0217+7352.csv', 42], ['4FGLJ0217-0821.csv', 45], ['4FGLJ0219+2443.csv', 41], ['4FGLJ0219-1724.csv', 35]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyts.classification import BOSSVS\n",
    "from pyts.multivariate.classification import MultivariateClassifier\n",
    "\n",
    "iteration=200\n",
    "lgON=len(dataON)\n",
    "lgOFF=len(dataOFF)\n",
    "lg=lgON+lgOFF\n",
    "\n",
    "binning=37\n",
    "\n",
    "Labels = []\n",
    "Q_ON= []\n",
    "\n",
    "nbfeatures=3\n",
    "ON_accuracy=[]\n",
    "OFF_accuracy=[]\n",
    "fscore=[]\n",
    "FPOS=[]\n",
    "FNEG=[]\n",
    "a=binning\n",
    "b=nbfeatures\n",
    "c=len(data_files_ALL)\n",
    "\n",
    "# multivariate\n",
    "y=np.zeros((2,lg))\n",
    "bigdata= np.zeros((b,c,a))\n",
    "\n",
    "for j in range(len(data_files_ALL)):\n",
    "\n",
    "    dataframe=pd.read_csv(data_files_ALL[j],index_col=[0])\n",
    "    dataframe.columns=['MJD','Flux','Delta Flux','Photon Index','Delta Index','TS','fratio']\n",
    "\n",
    "    bigdata[0][j]=dataframe['Flux']\n",
    "    bigdata[1][j]=dataframe['Photon Index']*dataframe['Flux']\n",
    "    bigdata[2][j]=dataframe['Photon Index']\n",
    "\n",
    "for i in range(lgOFF):\n",
    "    \n",
    "    Labels.append(int(0))\n",
    "    \n",
    "for i in range(lgON): \n",
    "    \n",
    "    \n",
    "    Labels.append(int(1))\n",
    "    \n",
    "    \n",
    "bigdata=bigdata.reshape(c,nbfeatures,binning)    \n",
    "print(bigdata.shape)\n",
    "for i in range(iteration):\n",
    "    \n",
    "    #3D shape of data, multivariate time series\n",
    "\n",
    "    sss = ShuffleSplit(n_splits=1, test_size=0.2,random_state=i)\n",
    "    X=bigdata\n",
    "    y=Labels\n",
    "\n",
    "    sss.get_n_splits(X, y)\n",
    "    train_index, test_index = next(sss.split(X, y)) \n",
    "\n",
    "    x_train, x_test = X[train_index], X[test_index] \n",
    "\n",
    "    y_train=[]\n",
    "    y_test=[]\n",
    "\n",
    "    for i in range(len(train_index)):\n",
    "        a=train_index[i]\n",
    "        y_train.append(y[a])\n",
    "    for i in range(len(test_index)):\n",
    "\n",
    "        a=test_index[i]\n",
    "        y_test.append(y[a])\n",
    "\n",
    "    y_test2=y_test.copy()\n",
    "\n",
    "    clf = MultivariateClassifier(BOSSVS(n_bins=5,anova=True,word_size=9,drop_sum=True))\n",
    "    clf.fit(x_train, y_train)\n",
    "    MultivariateClassifier(...)\n",
    "    clf.predict(x_test)\n",
    "\n",
    "    prediction=np.array(clf.predict(x_test))\n",
    "    on_score=0\n",
    "    on_nbs=0\n",
    "    off_nbs=0\n",
    "    off_score=0\n",
    "    foff=0\n",
    "    fon=0\n",
    "    for i in range(len(y_test)):\n",
    "        if y_test2[i]==1 :\n",
    "            on_nbs+=1\n",
    "            if prediction[i]==1:\n",
    "                on_score+=1\n",
    "            else : \n",
    "                fon+=1\n",
    "        if y_test2[i]==0 :\n",
    "            off_nbs+=1\n",
    "            if prediction[i]==0:\n",
    "                off_score+=1 \n",
    "            else:\n",
    "                foff+=1\n",
    "                a=test_index[i]\n",
    "                Q_ON.append(a)\n",
    "    if on_nbs>0:    \n",
    "        ON_accuracy.append(100*(on_score/on_nbs))\n",
    "        FPOS.append(100*(fon/on_nbs))\n",
    "    OFF_accuracy.append(100*(off_score/off_nbs))\n",
    "    FNEG.append(100*(foff/off_nbs))\n",
    "    f1= f1_score(y_test2,prediction,average='weighted')\n",
    "    fscore.append(f1)\n",
    "    \n",
    "print(\"Accuracy for ON class: \",np.mean(ON_accuracy) ,\"%\")\n",
    "print(\"Accuracy for OFF class: \",np.mean(OFF_accuracy) ,\"%\")\n",
    "print(\"False Positive rate: \",np.mean(FPOS) ,\"%\")\n",
    "print(\"False Negative rate: \",np.mean(FNEG) ,\"%\")\n",
    "print(\"F1 score: \",np.mean(fscore))\n",
    "\n",
    "\n",
    "\n",
    "gg = np.unique(Q_ON)\n",
    "Q_ON2=[]\n",
    "Q_ON3=[]\n",
    "errOFF=[]\n",
    "print(len(test_index))\n",
    "print(len(np.unique(Q_ON)))\n",
    "\n",
    "if len(gg)>0:\n",
    "    for i in range(len(Q_ON)):\n",
    "        aa=Q_ON[i]\n",
    "        Q_ON2.append(file_names_ALL[aa])\n",
    "    gg = np.unique(Q_ON2)    \n",
    "    for i in range(len(gg)):\n",
    "        bb=gg[i]\n",
    "        count = Q_ON2.count(bb)\n",
    "        if count>int(iteration/8):\n",
    "            Q_ON3.append([bb,count])\n",
    "            errOFF.append(bb)\n",
    "print(Q_ON3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe57d85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b177a67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing in a list the adresses of all the data files in the notebook\n",
    "\n",
    "data_files_ON=[]\n",
    "file_names_ON=[]\n",
    "\n",
    "data_files_OFF=[]\n",
    "file_names_OFF=[]\n",
    "data_files_ALL=[]\n",
    "file_names_ALL=[]\n",
    "\n",
    "#Storing in a list the adresses of all the data files in the notebook\n",
    "for filename in os.listdir(pathOFF):\n",
    "    f = os.path.join(pathOFF,filename)\n",
    "    if os.path.isfile(f) and filename in errOFF :\n",
    "        data_files_OFF.append(f)\n",
    "        file_names_OFF.append(filename)\n",
    "        data_files_ALL.append(f)\n",
    "        file_names_ALL.append(filename)\n",
    "        \n",
    "for filename in os.listdir(pathON):\n",
    "    f = os.path.join(pathON,filename)\n",
    "    if os.path.isfile(f):\n",
    "        data_files_ON.append(f)\n",
    "        file_names_ON.append(filename)\n",
    "        data_files_ALL.append(f)\n",
    "        file_names_ALL.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "33637ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using data_files with only the 36 binning\n",
    "Filter=True\n",
    "binning=37\n",
    "\n",
    "\n",
    "if Filter==True:\n",
    "    \n",
    "    idx_OFF=[]\n",
    "    idx_ON=[]\n",
    "    dataON=[]\n",
    "    dataOFF=[]\n",
    "    for i in range(len(data_files_OFF)):\n",
    "        dataframe = pd.read_csv(data_files_OFF[i])\n",
    "        lg = len(dataframe)\n",
    "        \n",
    "        if lg==binning:\n",
    "            idx_OFF.append(i)\n",
    "    \n",
    "    for i in range(len(data_files_ON)):\n",
    "        dataframe = pd.read_csv(data_files_ON[i])\n",
    "        lg = len(dataframe)\n",
    "\n",
    "        if lg==binning:\n",
    "            idx_ON.append(i)\n",
    "\n",
    "\n",
    "    for i in range(len(idx_OFF)):\n",
    "\n",
    "        a=idx_OFF[i]\n",
    "        dataOFF.append(data_files_OFF[a])\n",
    "    for i in range(len(idx_ON)):\n",
    "\n",
    "        a=idx_ON[i]\n",
    "        dataON.append(data_files_ON[a])\n",
    "\n",
    "idx = idx_OFF+idx_ON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1c751043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124, 3, 37)\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [60]\u001b[0m, in \u001b[0;36m<cell line: 51>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    104\u001b[0m     ON_accuracy\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m100\u001b[39m\u001b[38;5;241m*\u001b[39m(on_score\u001b[38;5;241m/\u001b[39mon_nbs))\n\u001b[0;32m    105\u001b[0m     FPOS\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m100\u001b[39m\u001b[38;5;241m*\u001b[39m(fon\u001b[38;5;241m/\u001b[39mon_nbs))\n\u001b[1;32m--> 106\u001b[0m OFF_accuracy\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m100\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[43moff_score\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43moff_nbs\u001b[49m))\n\u001b[0;32m    107\u001b[0m FNEG\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m100\u001b[39m\u001b[38;5;241m*\u001b[39m(foff\u001b[38;5;241m/\u001b[39moff_nbs))\n\u001b[0;32m    108\u001b[0m f1\u001b[38;5;241m=\u001b[39m f1_score(y_test2,prediction,average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyts.classification import BOSSVS\n",
    "from pyts.multivariate.classification import MultivariateClassifier\n",
    "\n",
    "iteration=200\n",
    "lgON=len(dataON)\n",
    "lgOFF=len(dataOFF)\n",
    "lg=lgON+lgOFF\n",
    "\n",
    "binning=37\n",
    "\n",
    "Labels = []\n",
    "Q_ON= []\n",
    "\n",
    "nbfeatures=3\n",
    "ON_accuracy=[]\n",
    "OFF_accuracy=[]\n",
    "fscore= []\n",
    "FPOS=[]\n",
    "FNEG=[]\n",
    "a=binning\n",
    "b=nbfeatures\n",
    "c=len(data_files_ALL)\n",
    "\n",
    "# multivariate\n",
    "y=np.zeros((2,lg))\n",
    "bigdata= np.zeros((b,c,a))\n",
    "\n",
    "for j in range(len(data_files_ALL)):\n",
    "\n",
    "#         a=Q_ON2[j]\n",
    "\n",
    "    dataframe=pd.read_csv(data_files_ALL[j],index_col=[0])\n",
    "    dataframe.columns=['MJD','Flux','Delta Flux','Photon Index','Delta Index','TS','fratio']\n",
    "\n",
    "    bigdata[0][j]=dataframe['Flux']\n",
    "    bigdata[1][j]=dataframe['Photon Index']*dataframe['Flux']\n",
    "    bigdata[2][j]=dataframe['Photon Index']\n",
    "\n",
    "for i in range(lgOFF):\n",
    "    \n",
    "    Labels.append(int(0))\n",
    "    \n",
    "for i in range(lgON): \n",
    "    \n",
    "    \n",
    "    Labels.append(int(1))\n",
    "    \n",
    "    \n",
    "bigdata=bigdata.reshape(c,nbfeatures,binning)    \n",
    "print(bigdata.shape)\n",
    "for i in range(iteration):\n",
    "    #3D shape of data, multivariate time series\n",
    "\n",
    "    sss = ShuffleSplit(n_splits=1, test_size=0.2,random_state=i)\n",
    "    X=bigdata\n",
    "    y=Labels\n",
    "\n",
    "    sss.get_n_splits(X, y)\n",
    "    train_index, test_index = next(sss.split(X, y)) \n",
    "\n",
    "    x_train, x_test = X[train_index], X[test_index] \n",
    "\n",
    "    y_train=[]\n",
    "    y_test=[]\n",
    "\n",
    "    for i in range(len(train_index)):\n",
    "        a=train_index[i]\n",
    "        y_train.append(y[a])\n",
    "    for i in range(len(test_index)):\n",
    "\n",
    "        a=test_index[i]\n",
    "        y_test.append(y[a])\n",
    "\n",
    "    y_test2=y_test.copy()\n",
    "\n",
    "    clf = MultivariateClassifier(BOSSVS(n_bins=5,anova=True,word_size=9,drop_sum=True))\n",
    "    clf.fit(x_train, y_train)\n",
    "    MultivariateClassifier(...)\n",
    "    clf.predict(x_test)\n",
    "\n",
    "    prediction=np.array(clf.predict(x_test))\n",
    "    on_score=0\n",
    "    on_nbs=0\n",
    "    off_nbs=0\n",
    "    off_score=0\n",
    "    foff=0\n",
    "    fon=0\n",
    "    for i in range(len(y_test)):\n",
    "        if y_test2[i]==1 :\n",
    "            on_nbs+=1\n",
    "            if prediction[i]==1:\n",
    "                on_score+=1\n",
    "            else : \n",
    "                fon+=1\n",
    "        if y_test2[i]==0 :\n",
    "            off_nbs+=1\n",
    "            if prediction[i]==0:\n",
    "                off_score+=1 \n",
    "            else:\n",
    "                foff+=1\n",
    "                a=test_index[i]\n",
    "                Q_ON.append(a)\n",
    "    if on_nbs>0:    \n",
    "        ON_accuracy.append(100*(on_score/on_nbs))\n",
    "        FPOS.append(100*(fon/on_nbs))\n",
    "    OFF_accuracy.append(100*(off_score/off_nbs))\n",
    "    FNEG.append(100*(foff/off_nbs))\n",
    "    f1= f1_score(y_test2,prediction,average='weighted')\n",
    "    fscore.append(f1)\n",
    "    \n",
    "print(\"Accuracy for ON class: \",np.mean(ON_accuracy) ,\"%\")\n",
    "print(\"Accuracy for OFF class: \",np.mean(OFF_accuracy) ,\"%\")\n",
    "print(\"False Positive rate: \",np.mean(FPOS) ,\"%\")\n",
    "print(\"False Negative rate: \",np.mean(FNEG) ,\"%\")\n",
    "print(\"F1 score: \",np.mean(fscore))\n",
    "\n",
    "\n",
    "gg = np.unique(Q_ON)\n",
    "Q_ON2=[]\n",
    "Q_ON3=[]\n",
    "errOFF=[]\n",
    "\n",
    "if len(gg)>0:\n",
    "    for i in range(len(Q_ON)):\n",
    "        aa=Q_ON[i]\n",
    "        Q_ON2.append(file_names_ALL[aa])\n",
    "    gg = np.unique(Q_ON2)    \n",
    "    for i in range(len(gg)):\n",
    "        bb=gg[i]\n",
    "        count = Q_ON2.count(bb)\n",
    "        if count>int(iteration/5):\n",
    "            Q_ON3.append([bb,count])\n",
    "            errOFF.append(bb)\n",
    "print(Q_ON3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11d55ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing in a list the adresses of all the data files in the notebook\n",
    "\n",
    "data_files_ON=[]\n",
    "file_names_ON=[]\n",
    "\n",
    "data_files_OFF=[]\n",
    "file_names_OFF=[]\n",
    "data_files_ALL=[]\n",
    "file_names_ALL=[]\n",
    "\n",
    "#Storing in a list the adresses of all the data files in the notebook\n",
    "for filename in os.listdir(pathOFF):\n",
    "    f = os.path.join(pathOFF,filename)\n",
    "    if os.path.isfile(f) and filename in errOFF :\n",
    "        data_files_OFF.append(f)\n",
    "        file_names_OFF.append(filename)\n",
    "        data_files_ALL.append(f)\n",
    "        file_names_ALL.append(filename)\n",
    "        \n",
    "for filename in os.listdir(pathON):\n",
    "    f = os.path.join(pathON,filename)\n",
    "    if os.path.isfile(f):\n",
    "        data_files_ON.append(f)\n",
    "        file_names_ON.append(filename)\n",
    "        data_files_ALL.append(f)\n",
    "        file_names_ALL.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e182b57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using data_files with only the 36 binning\n",
    "Filter=True\n",
    "binning=37\n",
    "\n",
    "\n",
    "if Filter==True:\n",
    "    \n",
    "    idx_OFF=[]\n",
    "    idx_ON=[]\n",
    "    dataON=[]\n",
    "    dataOFF=[]\n",
    "    \n",
    "    for i in range(len(data_files_OFF)):\n",
    "        dataframe = pd.read_csv(data_files_OFF[i])\n",
    "        lg = len(dataframe)\n",
    "        \n",
    "        if lg==binning:\n",
    "            idx_OFF.append(i)\n",
    "    \n",
    "    for i in range(len(data_files_ON)):\n",
    "        dataframe = pd.read_csv(data_files_ON[i])\n",
    "        lg = len(dataframe)\n",
    "\n",
    "        if lg==binning:\n",
    "            idx_ON.append(i)\n",
    "\n",
    "\n",
    "    for i in range(len(idx_OFF)):\n",
    "\n",
    "        a=idx_OFF[i]\n",
    "        dataOFF.append(data_files_OFF[a])\n",
    "    for i in range(len(idx_ON)):\n",
    "\n",
    "        a=idx_ON[i]\n",
    "        dataON.append(data_files_ON[a])\n",
    "\n",
    "idx = idx_OFF+idx_ON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b39bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyts.classification import BOSSVS\n",
    "from pyts.multivariate.classification import MultivariateClassifier\n",
    "\n",
    "iteration=200\n",
    "lgON=len(dataON)\n",
    "lgOFF=len(dataOFF)\n",
    "lg=lgON+lgOFF\n",
    "\n",
    "binning=37\n",
    "\n",
    "Labels = []\n",
    "Q_ON= []\n",
    "\n",
    "nbfeatures=3\n",
    "ON_accuracy=[]\n",
    "OFF_accuracy=[]\n",
    "fscore=[]\n",
    "FPOS=[]\n",
    "FNEG=[]\n",
    "a=binning\n",
    "b=nbfeatures\n",
    "c=len(data_files_ALL)\n",
    "\n",
    "# multivariate\n",
    "y=np.zeros((2,lg))\n",
    "bigdata= np.zeros((b,c,a))\n",
    "\n",
    "for j in range(len(data_files_ALL)):\n",
    "\n",
    "\n",
    "    dataframe=pd.read_csv(data_files_ALL[j])\n",
    "    dataframe.columns=['Iteration2','Iteration','MJD','Flux','Delta Flux','Photon Index','Delta Index','TS','fratio']\n",
    "\n",
    "    bigdata[0][j]=dataframe['Flux']\n",
    "    bigdata[1][j]=dataframe['Photon Index']*dataframe['Flux']\n",
    "    bigdata[2][j]=dataframe['Photon Index']\n",
    "\n",
    "for i in range(lgOFF):\n",
    "    \n",
    "    Labels.append(int(0))\n",
    "    \n",
    "for i in range(lgON): \n",
    "    \n",
    "    \n",
    "    Labels.append(int(1))\n",
    "    \n",
    "    \n",
    "bigdata=bigdata.reshape(c,nbfeatures,binning)    \n",
    "print(bigdata.shape)\n",
    "for i in range(iteration):\n",
    "    #3D shape of data, multivariate time series\n",
    "\n",
    "    sss = ShuffleSplit(n_splits=1, test_size=0.2,random_state=i)\n",
    "    X=bigdata\n",
    "    y=Labels\n",
    "\n",
    "    sss.get_n_splits(X, y)\n",
    "    train_index, test_index = next(sss.split(X, y)) \n",
    "\n",
    "    x_train, x_test = X[train_index], X[test_index] \n",
    "\n",
    "    y_train=[]\n",
    "    y_test=[]\n",
    "\n",
    "    for i in range(len(train_index)):\n",
    "        a=train_index[i]\n",
    "        y_train.append(y[a])\n",
    "    for i in range(len(test_index)):\n",
    "\n",
    "        a=test_index[i]\n",
    "        y_test.append(y[a])\n",
    "\n",
    "    y_test2=y_test.copy()\n",
    "\n",
    "    clf = MultivariateClassifier(BOSSVS(n_bins=5,anova=True,word_size=9,drop_sum=True))\n",
    "    clf.fit(x_train, y_train)\n",
    "    MultivariateClassifier(...)\n",
    "    clf.predict(x_test)\n",
    "\n",
    "    prediction=np.array(clf.predict(x_test))\n",
    "    on_score=0\n",
    "    on_nbs=0\n",
    "    off_nbs=0\n",
    "    off_score=0\n",
    "    foff=0\n",
    "    fon=0\n",
    "    for i in range(len(y_test)):\n",
    "        if y_test2[i]==1 :\n",
    "            on_nbs+=1\n",
    "            if prediction[i]==1:\n",
    "                on_score+=1\n",
    "            else : \n",
    "                fon+=1\n",
    "        if y_test2[i]==0 :\n",
    "            off_nbs+=1\n",
    "            if prediction[i]==0:\n",
    "                off_score+=1 \n",
    "            else:\n",
    "                foff+=1\n",
    "    if on_nbs>0:    \n",
    "        ON_accuracy.append(100*(on_score/on_nbs))\n",
    "        FPOS.append(100*(fon/on_nbs))\n",
    "    OFF_accuracy.append(100*(off_score/off_nbs))\n",
    "    FNEG.append(100*(foff/off_nbs))\n",
    "    f1= f1_score(y_test2,prediction,average='weighted')\n",
    "    fscore.append(f1)\n",
    "    \n",
    "print(\"Accuracy for ON class: \",np.mean(ON_accuracy) ,\"%\")\n",
    "print(\"Accuracy for OFF class: \",np.mean(OFF_accuracy) ,\"%\")\n",
    "print(\"False Positive rate: \",np.mean(FPOS) ,\"%\")\n",
    "print(\"False Negative rate: \",np.mean(FNEG) ,\"%\")\n",
    "print(\"F1 score: \",np.mean(fscore))\n",
    "\n",
    "\n",
    "\n",
    "gg = np.unique(Q_ON)\n",
    "Q_ON2=[]\n",
    "Q_ON3=[]\n",
    "errOFF=[]\n",
    "\n",
    "if len(gg)>0:\n",
    "    for i in range(len(Q_ON)):\n",
    "        aa=Q_ON[i]\n",
    "        Q_ON2.append(file_names_ALL[aa])\n",
    "    gg = np.unique(Q_ON2)    \n",
    "    for i in range(len(gg)):\n",
    "        bb=gg[i]\n",
    "        count = Q_ON2.count(bb)\n",
    "        if count>1:\n",
    "            Q_ON3.append([bb,count])\n",
    "            errOFF.append(bb)\n",
    "print(Q_ON3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781b617f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
