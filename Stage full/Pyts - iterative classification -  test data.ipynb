{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff1139a5",
   "metadata": {},
   "source": [
    "# PYTS/BOSSVS classification algorithm test and measured accuracies\n",
    "# Tested on transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "06b1435d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import sys \n",
    "import shutil\n",
    "from keras.callbacks import CSVLogger\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import dstack\n",
    "from pandas import read_csv\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ShuffleSplit # or StratifiedShuffleSplit\n",
    "from pandas import DataFrame\n",
    "import sktime\n",
    "from sktime.classification.distance_based import KNeighborsTimeSeriesClassifier\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "527823ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathON=\"C:/Users/pierr/Documents_kanop/Shps/Pierre_points_for_louis/Bureau/interpo_ONOFF/ON_data/\"\n",
    "pathOFF=\"C:/Users/pierr/Documents_kanop/Shps/Pierre_points_for_louis/Bureau/interpo_ONOFF/OFF_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2d6ba841",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing in a list the adresses of all the data files in the notebook\n",
    "\n",
    "data_files_ON=[]\n",
    "file_names_ON=[]\n",
    "\n",
    "data_files_OFF=[]\n",
    "file_names_OFF=[]\n",
    "data_files_ALL=[]\n",
    "file_names_ALL=[]\n",
    "\n",
    "#Storing in a list the adresses of all the data files in the notebook\n",
    "for filename in os.listdir(pathOFF):\n",
    "    f = os.path.join(pathOFF,filename)\n",
    "    if os.path.isfile(f):\n",
    "        data_files_OFF.append(f)\n",
    "        file_names_OFF.append(filename)\n",
    "        data_files_ALL.append(f)\n",
    "        file_names_ALL.append(filename)\n",
    "for filename in os.listdir(pathON):\n",
    "    f = os.path.join(pathON,filename)\n",
    "    if os.path.isfile(f):\n",
    "        data_files_ON.append(f)\n",
    "        file_names_ON.append(filename)\n",
    "        data_files_ALL.append(f)\n",
    "        file_names_ALL.append(filename)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8e8e998f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using data_files with only the 36 binning\n",
    "Filter=True\n",
    "binning=37\n",
    "\n",
    "\n",
    "if Filter==True:\n",
    "    \n",
    "    idx_OFF=[]\n",
    "    idx_ON=[]\n",
    "    dataON=[]\n",
    "    dataOFF=[]\n",
    "    for i in range(len(data_files_OFF)):\n",
    "        dataframe = pd.read_csv(data_files_OFF[i])\n",
    "        lg = len(dataframe)\n",
    "        \n",
    "        if lg==binning:\n",
    "            idx_OFF.append(i)\n",
    "    \n",
    "    for i in range(len(data_files_ON)):\n",
    "        dataframe = pd.read_csv(data_files_ON[i])\n",
    "        lg = len(dataframe)\n",
    "\n",
    "        if lg==binning:\n",
    "            idx_ON.append(i)\n",
    "\n",
    "\n",
    "    for i in range(len(idx_OFF)):\n",
    "\n",
    "        a=idx_OFF[i]\n",
    "        dataOFF.append(data_files_OFF[a])\n",
    "    for i in range(len(idx_ON)):\n",
    "\n",
    "        a=idx_ON[i]\n",
    "        dataON.append(data_files_ON[a])\n",
    "\n",
    "idx = idx_OFF+idx_ON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9392c0f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1089, 3, 37)\n",
      "Accuracy for ON class:  100.0 %\n",
      "Accuracy for OFF class:  73.05418759122713 %\n",
      "False Positive rate:  0.0 %\n",
      "False Negative rate:  26.945812408772877 %\n",
      "F1 score:  0.8053957476427653\n",
      "[['4FGL J1059-1134.csv', 59], ['4FGL J1137-1708.csv', 48], ['4FGL J1215+0731.csv', 59], ['4FGL J1312-0425.csv', 51], ['4FGL J1443+4728.csv', 48], ['4FGL J1613+3411.csv', 50], ['4FGL J1615+2130.csv', 56], ['4FGL J1833-2103.csv', 59], ['4FGL J1836+3137.csv', 56], ['4FGL J1837+5347.csv', 52], ['4FGL J1838+4802.csv', 53], ['4FGL J1841+2909.csv', 50], ['4FGL J1841+3218.csv', 63], ['4FGL J1842+6810.csv', 57], ['4FGL J1844+1547.csv', 49], ['4FGL J1848+3217.csv', 69], ['4FGL J1848+3243.csv', 59], ['4FGL J1848+4247.csv', 56], ['4FGL J1849+6705.csv', 69], ['4FGL J1904+3627.csv', 65], ['4FGL J1911-1908.csv', 57], ['4FGL J1911-2006.csv', 56], ['4FGL J1913+4439.csv', 52], ['4FGL J1917-1921.csv', 65], ['4FGL J1921-1231.csv', 66], ['4FGL J1921-1607.csv', 65], ['4FGL J1923-2104.csv', 65], ['4FGL J1925+1227.csv', 54], ['4FGL J1925+2815.csv', 56], ['4FGL J1926+6154.csv', 55], ['4FGL J1931+0937.csv', 54], ['4FGL J1933+0726.csv', 57], ['4FGL J1934+6541.csv', 57], ['4FGL J1942+1033.csv', 70], ['4FGL J1944+2117.csv', 66], ['4FGL J1944+3921.csv', 62], ['4FGL J1944-2143.csv', 62], ['4FGL J1949+0906.csv', 60], ['4FGL J1950+1211.csv', 63], ['4FGL J1954-1122.csv', 56], ['4FGL J1955+0214.csv', 63], ['4FGL J1955+1358.csv', 72], ['4FGL J1955-1604.csv', 52], ['4FGL J2000+6508.csv', 66], ['4FGL J2000-1328.csv', 68], ['4FGL J2000-1748.csv', 55], ['4FGL J2001+7040.csv', 59], ['4FGL J2005+6424.csv', 71], ['4FGL J2005+7003.csv', 59], ['4FGL J2005+7752.csv', 58], ['4FGL J2007+6607.csv', 62], ['4FGL J2010+7229.csv', 68], ['4FGL J2012+4629.csv', 67], ['4FGL J2012-1646.csv', 64], ['4FGL J2014+0648.csv', 61], ['4FGL J2014-0047.csv', 65], ['4FGL J2015-0137.csv', 59], ['4FGL J2016-0903.csv', 57], ['4FGL J2021+0629.csv', 59], ['4FGL J2022+7612.csv', 43], ['4FGL J2023+3153.csv', 58], ['4FGL J2023-0123.csv', 61], ['4FGL J2023-1139.csv', 58], ['4FGL J2024-0847.csv', 70], ['4FGL J2025+0317.csv', 51], ['4FGL J2025+3341.csv', 61], ['4FGL J2025-0735.csv', 61], ['4FGL J2029+4925.csv', 68], ['4FGL J2032+1219.csv', 67], ['4FGL J2034+1154.csv', 71], ['4FGL J2035+1056.csv', 60], ['4FGL J2035+4901.csv', 75], ['4FGL J2036+6553.csv', 67], ['4FGL J2039+5218.csv', 60], ['4FGL J2039-1046.csv', 64], ['4FGL J2040-1705.csv', 52], ['4FGL J2050+0408.csv', 52], ['4FGL J2053+2922.csv', 65], ['4FGL J2055-0020.csv', 70], ['4FGL J2056+4939.csv', 54], ['4FGL J2056-0202.csv', 69], ['4FGL J2102+4702.csv', 66], ['4FGL J2103-1112.csv', 59], ['4FGL J2104-0212.csv', 67], ['4FGL J2108+3655.csv', 58], ['4FGL J2108-0250.csv', 62], ['4FGL J2110+0808.csv', 50], ['4FGL J2112+0819.csv', 57], ['4FGL J2115+1218.csv', 46], ['4FGL J2115-0113.csv', 64], ['4FGL J2119-1105.csv', 59], ['4FGL J2121+1901.csv', 52], ['4FGL J2123+0535.csv', 56], ['4FGL J2131-0916.csv', 76], ['4FGL J2133+6646.csv', 58], ['4FGL J2134-0154.csv', 53], ['4FGL J2134-2130.csv', 63], ['4FGL J2143+1743.csv', 69], ['4FGL J2146-1344.csv', 67], ['4FGL J2147+0931.csv', 63], ['4FGL J2148-0733.csv', 63], ['4FGL J2149+0323.csv', 60], ['4FGL J2149+1917.csv', 55], ['4FGL J2150-1410.csv', 67], ['4FGL J2151+4156.csv', 70], ['4FGL J2153-1137.csv', 59], ['4FGL J2156+1818.csv', 60], ['4FGL J2156-0036.csv', 52], ['4FGL J2157+3127.csv', 56], ['4FGL J2158-1501.csv', 62], ['4FGL J2200+2138.csv', 61], ['4FGL J2202+4216.csv', 48], ['4FGL J2204+0438.csv', 56], ['4FGL J2206-0032.csv', 50], ['4FGL J2207+4316.csv', 63], ['4FGL J2209-0451.csv', 61], ['4FGL J2211-1325.csv', 53], ['4FGL J2212+2800.csv', 63], ['4FGL J2219-0342.csv', 60], ['4FGL J2225-0457.csv', 49], ['4FGL J2228-1636.csv', 56], ['4FGL J2229-0832.csv', 55], ['4FGL J2236+2828.csv', 67], ['4FGL J2236-1433.csv', 54], ['4FGL J2236-1706.csv', 54], ['4FGL J2241+4120.csv', 60], ['4FGL J2243-1231.csv', 71], ['4FGL J2244+4057.csv', 61], ['4FGL J2247+4413.csv', 73], ['4FGL J2250+3825.csv', 60], ['4FGL J2250-1250.csv', 66], ['4FGL J2252+4031.csv', 57], ['4FGL J2255+2411.csv', 49], ['4FGL J2256-2011.csv', 59], ['4FGL J2259-1552.csv', 53], ['4FGL J2300+3136.csv', 54], ['4FGL J2301-0158.csv', 48], ['4FGL J2304+3704.csv', 55], ['4FGL J2311+0205.csv', 65], ['4FGL J2311+2604.csv', 62], ['4FGL J2311+3425.csv', 59], ['4FGL J2313+3945.csv', 55], ['4FGL J2314+1445.csv', 56], ['4FGL J2318+1915.csv', 65], ['4FGL J2321+5111.csv', 53], ['4FGL J2329+6101.csv', 73], ['4FGL J2347+5141.csv', 62], ['4FGL J2347+5436.csv', 67], ['4FGL J2352+1750.csv', 60], ['4FGL J2356+4036.csv', 69], ['4FGL J2357-1718.csv', 74], ['4FGL J2358+3830.csv', 62], ['4FGL J2358-1021.csv', 64], ['4FGL J2358-1808.csv', 63], ['4FGL J2359-2049.csv', 53], ['4FGLJ0001+2113.csv', 57], ['4FGLJ0001-0747.csv', 65], ['4FGLJ0003-1149.csv', 47], ['4FGLJ0003-1928.csv', 73], ['4FGLJ0005+3824.csv', 57], ['4FGLJ0007+4008.csv', 57], ['4FGLJ0008+1455.csv', 82], ['4FGLJ0008+4711.csv', 46], ['4FGLJ0009+0628.csv', 60], ['4FGLJ0009+5030.csv', 51], ['4FGLJ0011+0057.csv', 53], ['4FGLJ0013-1854.csv', 68], ['4FGLJ0014+3212.csv', 60], ['4FGLJ0014+6118.csv', 62], ['4FGLJ0014-0500.csv', 63], ['4FGLJ0015+5551.csv', 60], ['4FGLJ0016-0016.csv', 53], ['4FGLJ0017-0514.csv', 53], ['4FGLJ0017-0649.csv', 46], ['4FGLJ0018+2946.csv', 52], ['4FGLJ0019+2022.csv', 63], ['4FGLJ0019+7327.csv', 51], ['4FGLJ0022+0608.csv', 67], ['4FGLJ0022-1854.csv', 60], ['4FGLJ0023+4457.csv', 56], ['4FGLJ0028+2001.csv', 69], ['4FGLJ0028+7505.csv', 69], ['4FGLJ0030-0212.csv', 61], ['4FGLJ0030-1647.csv', 57], ['4FGLJ0033-1921.csv', 61], ['4FGLJ0035+1514.csv', 54], ['4FGLJ0035+5950.csv', 70], ['4FGLJ0037+1239.csv', 43], ['4FGLJ0038+0012.csv', 59], ['4FGLJ0039-0946.csv', 52], ['4FGLJ0041+6052.csv', 63], ['4FGLJ0042+2319.csv', 57], ['4FGLJ0043+3425.csv', 52], ['4FGLJ0045+1217.csv', 54], ['4FGLJ0045+2128.csv', 54], ['4FGLJ0047+2233.csv', 54], ['4FGLJ0047+3947.csv', 53], ['4FGLJ0047+5657.csv', 49], ['4FGLJ0049+0237.csv', 67], ['4FGLJ0050-0452.csv', 51], ['4FGLJ0050-0929.csv', 54], ['4FGLJ0051-0648.csv', 63], ['4FGLJ0055-1219.csv', 61], ['4FGLJ0056-0935.csv', 64], ['4FGLJ0056-2118.csv', 70], ['4FGLJ0058-0539.csv', 60], ['4FGLJ0059-0152.csv', 63], ['4FGLJ0102+4214.csv', 55], ['4FGLJ0102+5824.csv', 69], ['4FGLJ0103+5337.csv', 66], ['4FGLJ0105+3929.csv', 61], ['4FGLJ0108+0134.csv', 55], ['4FGLJ0109+6133.csv', 67], ['4FGLJ0110+6805.csv', 59], ['4FGLJ0112+2245.csv', 63], ['4FGLJ0112+3208.csv', 56], ['4FGLJ0113+4948.csv', 58], ['4FGLJ0114+1326.csv', 48], ['4FGLJ0115+0356.csv', 67], ['4FGLJ0115+2519.csv', 44], ['4FGLJ0115-0129.csv', 75], ['4FGLJ0116-1136.csv', 60], ['4FGLJ0117-2109.csv', 62], ['4FGLJ0118-2141.csv', 50], ['4FGLJ0119-1458.csv', 61], ['4FGLJ0123+3421.csv', 65], ['4FGLJ0127+0324.csv', 56], ['4FGLJ0127-0819.csv', 50], ['4FGLJ0129+1440.csv', 58], ['4FGLJ0131+5547.csv', 65], ['4FGLJ0131+6120.csv', 64], ['4FGLJ0132-1654.csv', 55], ['4FGLJ0134+2637.csv', 71], ['4FGLJ0136+3906.csv', 56], ['4FGLJ0137+4751.csv', 52], ['4FGLJ0137+5814.csv', 66], ['4FGLJ0138+2247.csv', 62], ['4FGLJ0141-0928.csv', 53], ['4FGLJ0144+2705.csv', 71], ['4FGLJ0148+0127.csv', 72], ['4FGLJ0148+5201.csv', 57], ['4FGLJ0151+8601.csv', 63], ['4FGLJ0152+0147.csv', 62], ['4FGLJ0152+2206.csv', 63], ['4FGLJ0153+0823.csv', 55], ['4FGLJ0153+7114.csv', 59], ['4FGLJ0153+7517.csv', 50], ['4FGLJ0155+4433.csv', 53], ['4FGLJ0156+3914.csv', 69], ['4FGLJ0159+1046.csv', 62], ['4FGLJ0203+3042.csv', 50], ['4FGLJ0203+7233.csv', 62], ['4FGLJ0204+1513.csv', 64], ['4FGLJ0205+3212.csv', 62], ['4FGLJ0205-1700.csv', 54], ['4FGLJ0206-1151.csv', 68], ['4FGLJ0208+3523.csv', 58], ['4FGLJ0209+4449.csv', 71], ['4FGLJ0209+7229.csv', 71], ['4FGLJ0211+1051.csv', 57], ['4FGLJ0212+2244.csv', 53], ['4FGLJ0214+5145.csv', 68], ['4FGLJ0215+0300.csv', 65], ['4FGLJ0216+2313.csv', 64], ['4FGLJ0216-1015.csv', 69], ['4FGLJ0217+0144.csv', 62], ['4FGLJ0217+0837.csv', 63], ['4FGLJ0217+7352.csv', 67], ['4FGLJ0217-0821.csv', 60], ['4FGLJ0218+3643.csv', 52], ['4FGLJ0219+2443.csv', 49], ['4FGLJ0219-1724.csv', 59], ['4FGLJ0221+3556.csv', 50], ['4FGLJ0227+0201.csv', 65], ['4FGLJ0245+2408.csv', 53], ['4FGLJ0250+8435.csv', 74], ['4FGLJ0253-0124.csv', 58], ['4FGLJ0256+0334.csv', 47], ['4FGLJ0259+0746.csv', 59], ['4FGLJ0308+0407.csv', 55], ['4FGLJ0309+1029.csv', 53], ['4FGLJ0310+3815.csv', 68], ['4FGLJ0312+0134.csv', 54], ['4FGLJ0312+3614.csv', 66], ['4FGLJ0314+0620.csv', 72], ['4FGLJ0316+0905.csv', 62], ['4FGLJ0318+2135.csv', 57], ['4FGLJ0319+4130.csv', 59], ['4FGLJ0324+3412.csv', 62], ['4FGLJ0330+0438.csv', 73], ['4FGLJ0336+3224.csv', 53], ['4FGLJ0409-0359.csv', 55], ['4FGLJ0449+1121.csv', 70], ['4FGLJ0505+0415.csv', 67], ['4FGLJ0505+0459.csv', 71], ['4FGLJ0506+0323.csv', 49], ['4FGLJ0509+0542.csv', 60], ['4FGLJ0509+1012.csv', 61], ['4FGLJ0510+1800.csv', 62], ['4FGLJ0515+1527.csv', 65], ['4FGLJ0519+0851.csv', 58], ['4FGLJ0530+1332.csv', 62], ['4FGLJ0653+1636.csv', 61], ['4FGLJ0654+5042.csv', 53], ['4FGLJ0700+1304.csv', 54], ['4FGLJ0721+0405.csv', 63], ['4FGLJ0725+0216.csv', 64], ['4FGLJ0725+1425.csv', 53], ['4FGLJ0733+0455.csv', 67], ['4FGLJ0738+1742.csv', 63], ['4FGLJ0749+1324.csv', 65], ['4FGLJ0805-0110.csv', 62], ['4FGLJ0922+0434.csv', 66], ['4FGLJ0948+0022.csv', 68], ['4FGLJ1050+0432.csv', 56], ['4FGLJ1058+2817.csv', 61], ['4FGLJ1103+1157.csv', 59], ['4FGLJ1104+0730.csv', 73], ['4FGLJ1107+1501.csv', 71], ['4FGLJ1110-1836.csv', 54], ['4FGLJ1125-2101.csv', 59], ['4FGLJ1424+1447.csv', 53], ['4FGLJ1428+1629.csv', 72], ['4FGLJ1533+1855.csv', 56], ['4FGLJ1534+0131.csv', 68], ['4FGLJ1540+1449.csv', 59], ['4FGLJ1546+0819.csv', 53], ['4FGLJ1548+1456.csv', 65], ['4FGLJ1550+0528.csv', 51], ['4FGLJ1552+0850.csv', 59], ['4FGLJ1553+1257.csv', 69], ['4FGLJ1555+1111.csv', 53], ['4FGLJ1607+1550.csv', 64], ['4FGLJ1608+1029.csv', 60], ['4FGLJ1631+1046.csv', 60], ['4FGLJ1632+0854.csv', 60], ['4FGLJ1640+1143.csv', 60], ['4FGLJ1644+2620.csv', 55], ['4FGLJ1650+0831.csv', 49], ['4FGLJ1702+2642.csv', 56], ['4FGLJ1709+4318.csv', 46], ['4FGLJ1719+1205.csv', 67], ['4FGLJ1724+4005.csv', 64], ['4FGLJ1725+1152.csv', 60], ['4FGLJ1728+1216.csv', 66], ['4FGLJ1734+3858.csv', 68], ['4FGLJ1736+0628.csv', 58], ['4FGLJ1751+0938.csv', 55], ['4FGLJ1841+6115.csv', 69], ['4FGLJ1844+5709.csv', 60], ['4FGLJ1903+5540.csv', 73], ['4FGLJ1927+6117.csv', 59], ['4FGLJ1934+6002.csv', 60], ['4FGLJ1949+1314.csv', 71], ['4FGLJ1959+3844.csv', 66], ['4FGLJ2001+4353.csv', 45], ['4FGLJ2015+3710.csv', 54], ['4FGLJ2018+3852.csv', 55], ['4FGLJ2049+1002.csv', 50], ['4FGLJ2108+1434.csv', 52], ['4FGLJ2203+1725.csv', 68], ['4FGLJ2212+2356.csv', 53], ['4FGLJ2216+2421.csv', 60], ['4FGLJ2219+1806.csv', 49], ['4FGLJ2227+0036.csv', 74], ['4FGLJ2232+1143.csv', 72], ['4FGLJ2243+2021.csv', 63], ['4FGLJ2245+1544.csv', 51], ['4FGLJ2247-0001.csv', 63], ['4FGLJ2248+2106.csv', 45], ['4FGLJ2253+1609.csv', 75]]\n"
     ]
    }
   ],
   "source": [
    "from pyts.classification import BOSSVS\n",
    "from pyts.multivariate.classification import MultivariateClassifier\n",
    "\n",
    "iteration=300\n",
    "lgON=len(dataON)\n",
    "lgOFF=len(dataOFF)\n",
    "lg=lgON+lgOFF\n",
    "\n",
    "binning=37\n",
    "\n",
    "Labels = []\n",
    "Q_ON= []\n",
    "\n",
    "nbfeatures=3\n",
    "ON_accuracy=[]\n",
    "OFF_accuracy=[]\n",
    "fscore=[]\n",
    "FPOS=[]\n",
    "FNEG= []\n",
    "a=binning\n",
    "b=nbfeatures\n",
    "c=len(data_files_ALL)\n",
    "\n",
    "# multivariate\n",
    "y=np.zeros((2,lg))\n",
    "bigdata= np.zeros((b,c,a))\n",
    "\n",
    "for j in range(len(data_files_ALL)):\n",
    "\n",
    "\n",
    "    dataframe=pd.read_csv(data_files_ALL[j],index_col=[0])\n",
    "    dataframe.columns=['MJD','Flux','Delta Flux','Photon Index','Delta Index','TS','fratio']\n",
    "\n",
    "    bigdata[0][j]=dataframe['Flux']\n",
    "    bigdata[1][j]=dataframe['Photon Index']*dataframe['Flux']\n",
    "    bigdata[2][j]=dataframe['Photon Index']\n",
    "\n",
    "for i in range(lgOFF):\n",
    "    \n",
    "    Labels.append(int(0))\n",
    "    \n",
    "for i in range(lgON): \n",
    "    \n",
    "    \n",
    "    Labels.append(int(1))\n",
    "    \n",
    "    \n",
    "bigdata=bigdata.reshape(c,nbfeatures,binning)    \n",
    "print(bigdata.shape)\n",
    "for i in range(iteration):\n",
    "    \n",
    "    #3D shape of data, multivariate time series\n",
    "\n",
    "    sss = ShuffleSplit(n_splits=1, test_size=0.2,random_state=i)\n",
    "    X=bigdata\n",
    "    y=Labels\n",
    "\n",
    "    sss.get_n_splits(X, y)\n",
    "    train_index, test_index = next(sss.split(X, y)) \n",
    "\n",
    "    x_train, x_test = X[train_index], X[test_index] \n",
    "\n",
    "    y_train=[]\n",
    "    y_test=[]\n",
    "\n",
    "    for i in range(len(train_index)):\n",
    "        a=train_index[i]\n",
    "        y_train.append(y[a])\n",
    "    for i in range(len(test_index)):\n",
    "\n",
    "        a=test_index[i]\n",
    "        y_test.append(y[a])\n",
    "\n",
    "    y_test2=y_test.copy()\n",
    "\n",
    "    clf = MultivariateClassifier(BOSSVS(n_bins=5,anova=True,word_size=9,drop_sum=True))\n",
    "    clf.fit(x_train, y_train)\n",
    "    MultivariateClassifier(...)\n",
    "    clf.predict(x_test)\n",
    "\n",
    "    prediction=np.array(clf.predict(x_test))\n",
    "    on_score=0\n",
    "    on_nbs=0\n",
    "    off_nbs=0\n",
    "    off_score=0\n",
    "    foff=0\n",
    "    fon=0\n",
    "    for i in range(len(y_test)):\n",
    "        if y_test2[i]==1 :\n",
    "            on_nbs+=1\n",
    "            if prediction[i]==1:\n",
    "                on_score+=1\n",
    "                a=test_index[i]\n",
    "                Q_ON.append(a)\n",
    "            else : \n",
    "                fon+=1\n",
    "        if y_test2[i]==0 :\n",
    "            off_nbs+=1\n",
    "            if prediction[i]==0:\n",
    "                off_score+=1 \n",
    "            else:\n",
    "                foff+=1\n",
    "                a=test_index[i]\n",
    "                Q_ON.append(a)\n",
    "    if on_nbs>0:    \n",
    "        ON_accuracy.append(100*(on_score/on_nbs))\n",
    "        FPOS.append(100*(fon/on_nbs))\n",
    "    OFF_accuracy.append(100*(off_score/off_nbs))\n",
    "    FNEG.append(100*(foff/off_nbs))\n",
    "    f1= f1_score(y_test2,prediction,average='weighted')\n",
    "    fscore.append(f1)\n",
    "    \n",
    "print(\"Accuracy for ON class: \",np.mean(ON_accuracy) ,\"%\")\n",
    "print(\"Accuracy for OFF class: \",np.mean(OFF_accuracy) ,\"%\")\n",
    "print(\"False Positive rate: \",np.mean(FPOS) ,\"%\")\n",
    "print(\"False Negative rate: \",np.mean(FNEG) ,\"%\")\n",
    "print(\"F1 score: \",np.mean(fscore))\n",
    "\n",
    "\n",
    "\n",
    "gg = np.unique(Q_ON)\n",
    "Q_ON2=[]\n",
    "Q_ON3=[]\n",
    "errOFF=[]\n",
    "\n",
    "\n",
    "\n",
    "if len(gg)>0:\n",
    "    for i in range(len(Q_ON)):\n",
    "        aa=Q_ON[i]\n",
    "        Q_ON2.append(file_names_ALL[aa])\n",
    "    gg = np.unique(Q_ON2)    \n",
    "    for i in range(len(gg)):\n",
    "        bb=gg[i]\n",
    "        count = Q_ON2.count(bb)\n",
    "        if count>int(iteration/7):\n",
    "            Q_ON3.append([bb,count])\n",
    "            errOFF.append(bb)\n",
    "print(Q_ON3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3b8a8771",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing in a list the adresses of all the data files in the notebook\n",
    "\n",
    "data_files_ON=[]\n",
    "file_names_ON=[]\n",
    "\n",
    "data_files_OFF=[]\n",
    "file_names_OFF=[]\n",
    "data_files_ALL=[]\n",
    "file_names_ALL=[]\n",
    "\n",
    "#Storing in a list the adresses of all the data files in the notebook\n",
    "for filename in os.listdir(pathOFF):\n",
    "    f = os.path.join(pathOFF,filename)\n",
    "    if os.path.isfile(f) and filename in errOFF :\n",
    "        data_files_OFF.append(f)\n",
    "        file_names_OFF.append(filename)\n",
    "        data_files_ALL.append(f)\n",
    "        file_names_ALL.append(filename)\n",
    "        \n",
    "for filename in os.listdir(pathON):\n",
    "    f = os.path.join(pathON,filename)\n",
    "    if os.path.isfile(f):\n",
    "        data_files_ON.append(f)\n",
    "        file_names_ON.append(filename)\n",
    "        data_files_ALL.append(f)\n",
    "        file_names_ALL.append(filename)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9033fed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using data_files with only the 36 binning\n",
    "Filter=True\n",
    "binning=37\n",
    "\n",
    "\n",
    "if Filter==True:\n",
    "    \n",
    "    idx_OFF=[]\n",
    "    idx_ON=[]\n",
    "    dataON=[]\n",
    "    dataOFF=[]\n",
    "    for i in range(len(data_files_OFF)):\n",
    "        dataframe = pd.read_csv(data_files_OFF[i])\n",
    "        lg = len(dataframe)\n",
    "        \n",
    "        if lg==binning:\n",
    "            idx_OFF.append(i)\n",
    "    \n",
    "    for i in range(len(data_files_ON)):\n",
    "        dataframe = pd.read_csv(data_files_ON[i])\n",
    "        lg = len(dataframe)\n",
    "\n",
    "        if lg==binning:\n",
    "            idx_ON.append(i)\n",
    "\n",
    "\n",
    "    for i in range(len(idx_OFF)):\n",
    "\n",
    "        a=idx_OFF[i]\n",
    "        dataOFF.append(data_files_OFF[a])\n",
    "    for i in range(len(idx_ON)):\n",
    "\n",
    "        a=idx_ON[i]\n",
    "        dataON.append(data_files_ON[a])\n",
    "\n",
    "idx = idx_OFF+idx_ON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2421f965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(371, 3, 37)\n",
      "Accuracy for ON class:  100.0 %\n",
      "Accuracy for OFF class:  93.26744604698347 %\n",
      "False Positive rate:  0.0 %\n",
      "False Negative rate:  6.732553953016548 %\n",
      "F1 score:  0.9535900117155339\n",
      "75\n",
      "23\n",
      "[['4FGLJ0205-1700.csv', 65], ['4FGLJ0216+2313.csv', 63], ['4FGLJ0216-1015.csv', 61], ['4FGLJ0217+7352.csv', 68], ['4FGLJ0219-1724.csv', 74]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyts.classification import BOSSVS\n",
    "from pyts.multivariate.classification import MultivariateClassifier\n",
    "\n",
    "iteration=300\n",
    "lgON=len(dataON)\n",
    "lgOFF=len(dataOFF)\n",
    "lg=lgON+lgOFF\n",
    "\n",
    "binning=37\n",
    "\n",
    "Labels = []\n",
    "Q_ON= []\n",
    "\n",
    "nbfeatures=3\n",
    "ON_accuracy=[]\n",
    "OFF_accuracy=[]\n",
    "fscore=[]\n",
    "FPOS=[]\n",
    "FNEG=[]\n",
    "a=binning\n",
    "b=nbfeatures\n",
    "c=len(data_files_ALL)\n",
    "\n",
    "# multivariate\n",
    "y=np.zeros((2,lg))\n",
    "bigdata= np.zeros((b,c,a))\n",
    "\n",
    "for j in range(len(data_files_ALL)):\n",
    "\n",
    "    dataframe=pd.read_csv(data_files_ALL[j],index_col=[0])\n",
    "    dataframe.columns=['MJD','Flux','Delta Flux','Photon Index','Delta Index','TS','fratio']\n",
    "\n",
    "    bigdata[0][j]=dataframe['Flux']\n",
    "    bigdata[1][j]=dataframe['Photon Index']*dataframe['Flux']\n",
    "    bigdata[2][j]=dataframe['Photon Index']\n",
    "\n",
    "for i in range(lgOFF):\n",
    "    \n",
    "    Labels.append(int(0))\n",
    "    \n",
    "for i in range(lgON): \n",
    "    \n",
    "    \n",
    "    Labels.append(int(1))\n",
    "    \n",
    "    \n",
    "bigdata=bigdata.reshape(c,nbfeatures,binning)    \n",
    "print(bigdata.shape)\n",
    "for i in range(iteration):\n",
    "    \n",
    "    #3D shape of data, multivariate time series\n",
    "\n",
    "    sss = ShuffleSplit(n_splits=1, test_size=0.2,random_state=i)\n",
    "    X=bigdata\n",
    "    y=Labels\n",
    "\n",
    "    sss.get_n_splits(X, y)\n",
    "    train_index, test_index = next(sss.split(X, y)) \n",
    "\n",
    "    x_train, x_test = X[train_index], X[test_index] \n",
    "\n",
    "    y_train=[]\n",
    "    y_test=[]\n",
    "\n",
    "    for i in range(len(train_index)):\n",
    "        a=train_index[i]\n",
    "        y_train.append(y[a])\n",
    "    for i in range(len(test_index)):\n",
    "\n",
    "        a=test_index[i]\n",
    "        y_test.append(y[a])\n",
    "\n",
    "    y_test2=y_test.copy()\n",
    "\n",
    "    clf = MultivariateClassifier(BOSSVS(n_bins=5,anova=True,word_size=9,drop_sum=True))\n",
    "    clf.fit(x_train, y_train)\n",
    "    MultivariateClassifier(...)\n",
    "    clf.predict(x_test)\n",
    "\n",
    "    prediction=np.array(clf.predict(x_test))\n",
    "    on_score=0\n",
    "    on_nbs=0\n",
    "    off_nbs=0\n",
    "    off_score=0\n",
    "    foff=0\n",
    "    fon=0\n",
    "    for i in range(len(y_test)):\n",
    "        if y_test2[i]==1 :\n",
    "            on_nbs+=1\n",
    "            if prediction[i]==1:\n",
    "                on_score+=1\n",
    "            else : \n",
    "                fon+=1\n",
    "        if y_test2[i]==0 :\n",
    "            off_nbs+=1\n",
    "            if prediction[i]==0:\n",
    "                off_score+=1 \n",
    "            else:\n",
    "                foff+=1\n",
    "                a=test_index[i]\n",
    "                Q_ON.append(a)\n",
    "    if on_nbs>0:    \n",
    "        ON_accuracy.append(100*(on_score/on_nbs))\n",
    "        FPOS.append(100*(fon/on_nbs))\n",
    "    OFF_accuracy.append(100*(off_score/off_nbs))\n",
    "    FNEG.append(100*(foff/off_nbs))\n",
    "    f1= f1_score(y_test2,prediction,average='weighted')\n",
    "    fscore.append(f1)\n",
    "    \n",
    "print(\"Accuracy for ON class: \",np.mean(ON_accuracy) ,\"%\")\n",
    "print(\"Accuracy for OFF class: \",np.mean(OFF_accuracy) ,\"%\")\n",
    "print(\"False Positive rate: \",np.mean(FPOS) ,\"%\")\n",
    "print(\"False Negative rate: \",np.mean(FNEG) ,\"%\")\n",
    "print(\"F1 score: \",np.mean(fscore))\n",
    "\n",
    "\n",
    "\n",
    "gg = np.unique(Q_ON)\n",
    "Q_ON2=[]\n",
    "Q_ON3=[]\n",
    "errOFF=[]\n",
    "print(len(test_index))\n",
    "print(len(np.unique(Q_ON)))\n",
    "\n",
    "if len(gg)>0:\n",
    "    for i in range(len(Q_ON)):\n",
    "        aa=Q_ON[i]\n",
    "        Q_ON2.append(file_names_ALL[aa])\n",
    "    gg = np.unique(Q_ON2)    \n",
    "    for i in range(len(gg)):\n",
    "        bb=gg[i]\n",
    "        count = Q_ON2.count(bb)\n",
    "        if count>int(iteration/5):\n",
    "            Q_ON3.append([bb,count])\n",
    "            errOFF.append(bb)\n",
    "print(Q_ON3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b177a67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing in a list the adresses of all the data files in the notebook\n",
    "\n",
    "data_files_ON=[]\n",
    "file_names_ON=[]\n",
    "\n",
    "data_files_OFF=[]\n",
    "file_names_OFF=[]\n",
    "data_files_ALL=[]\n",
    "file_names_ALL=[]\n",
    "\n",
    "#Storing in a list the adresses of all the data files in the notebook\n",
    "for filename in os.listdir(pathOFF):\n",
    "    f = os.path.join(pathOFF,filename)\n",
    "    if os.path.isfile(f) and filename in errOFF :\n",
    "        data_files_OFF.append(f)\n",
    "        file_names_OFF.append(filename)\n",
    "        data_files_ALL.append(f)\n",
    "        file_names_ALL.append(filename)\n",
    "        \n",
    "for filename in os.listdir(pathON):\n",
    "    f = os.path.join(pathON,filename)\n",
    "    if os.path.isfile(f):\n",
    "        data_files_ON.append(f)\n",
    "        file_names_ON.append(filename)\n",
    "        data_files_ALL.append(f)\n",
    "        file_names_ALL.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "33637ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using data_files with only the 36 binning\n",
    "Filter=True\n",
    "binning=37\n",
    "\n",
    "\n",
    "if Filter==True:\n",
    "    \n",
    "    idx_OFF=[]\n",
    "    idx_ON=[]\n",
    "    dataON=[]\n",
    "    dataOFF=[]\n",
    "    for i in range(len(data_files_OFF)):\n",
    "        dataframe = pd.read_csv(data_files_OFF[i])\n",
    "        lg = len(dataframe)\n",
    "        \n",
    "        if lg==binning:\n",
    "            idx_OFF.append(i)\n",
    "    \n",
    "    for i in range(len(data_files_ON)):\n",
    "        dataframe = pd.read_csv(data_files_ON[i])\n",
    "        lg = len(dataframe)\n",
    "\n",
    "        if lg==binning:\n",
    "            idx_ON.append(i)\n",
    "\n",
    "\n",
    "    for i in range(len(idx_OFF)):\n",
    "\n",
    "        a=idx_OFF[i]\n",
    "        dataOFF.append(data_files_OFF[a])\n",
    "    for i in range(len(idx_ON)):\n",
    "\n",
    "        a=idx_ON[i]\n",
    "        dataON.append(data_files_ON[a])\n",
    "\n",
    "idx = idx_OFF+idx_ON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1c751043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(116, 3, 37)\n",
      "Accuracy for ON class:  80.83881731667773 %\n",
      "Accuracy for OFF class:  34.31933842239186 %\n",
      "False Positive rate:  19.16118268332227 %\n",
      "False Negative rate:  65.68066157760813 %\n",
      "F1 score:  0.8219513211085087\n",
      "[['4FGLJ0217+7352.csv', 73], ['4FGLJ0217-0821.csv', 62]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyts.classification import BOSSVS\n",
    "from pyts.multivariate.classification import MultivariateClassifier\n",
    "\n",
    "iteration=300\n",
    "lgON=len(dataON)\n",
    "lgOFF=len(dataOFF)\n",
    "lg=lgON+lgOFF\n",
    "\n",
    "binning=37\n",
    "\n",
    "Labels = []\n",
    "Q_ON= []\n",
    "\n",
    "nbfeatures=3\n",
    "ON_accuracy=[]\n",
    "OFF_accuracy=[]\n",
    "fscore= []\n",
    "FPOS=[]\n",
    "FNEG=[]\n",
    "a=binning\n",
    "b=nbfeatures\n",
    "c=len(data_files_ALL)\n",
    "\n",
    "# multivariate\n",
    "y=np.zeros((2,lg))\n",
    "bigdata= np.zeros((b,c,a))\n",
    "\n",
    "for j in range(len(data_files_ALL)):\n",
    "\n",
    "#         a=Q_ON2[j]\n",
    "\n",
    "    dataframe=pd.read_csv(data_files_ALL[j],index_col=[0])\n",
    "    dataframe.columns=['MJD','Flux','Delta Flux','Photon Index','Delta Index','TS','fratio']\n",
    "\n",
    "    bigdata[0][j]=dataframe['Flux']\n",
    "    bigdata[1][j]=dataframe['Photon Index']*dataframe['Flux']\n",
    "    bigdata[2][j]=np.sin(dataframe['Photon Index'])\n",
    "\n",
    "for i in range(lgOFF):\n",
    "    \n",
    "    Labels.append(int(0))\n",
    "    \n",
    "for i in range(lgON): \n",
    "    \n",
    "    \n",
    "    Labels.append(int(1))\n",
    "    \n",
    "    \n",
    "bigdata=bigdata.reshape(c,nbfeatures,binning)    \n",
    "print(bigdata.shape)\n",
    "for i in range(iteration):\n",
    "    #3D shape of data, multivariate time series\n",
    "\n",
    "    sss = ShuffleSplit(n_splits=1, test_size=0.2,random_state=i)\n",
    "    X=bigdata\n",
    "    y=Labels\n",
    "\n",
    "    sss.get_n_splits(X, y)\n",
    "    train_index, test_index = next(sss.split(X, y)) \n",
    "\n",
    "    x_train, x_test = X[train_index], X[test_index] \n",
    "\n",
    "    y_train=[]\n",
    "    y_test=[]\n",
    "\n",
    "    for i in range(len(train_index)):\n",
    "        a=train_index[i]\n",
    "        y_train.append(y[a])\n",
    "    for i in range(len(test_index)):\n",
    "\n",
    "        a=test_index[i]\n",
    "        y_test.append(y[a])\n",
    "\n",
    "    y_test2=y_test.copy()\n",
    "\n",
    "    clf = MultivariateClassifier(BOSSVS(n_bins=5,anova=True,word_size=9,drop_sum=True))\n",
    "    clf.fit(x_train, y_train)\n",
    "    MultivariateClassifier(...)\n",
    "    clf.predict(x_test)\n",
    "\n",
    "    prediction=np.array(clf.predict(x_test))\n",
    "    on_score=0\n",
    "    on_nbs=0\n",
    "    off_nbs=0\n",
    "    off_score=0\n",
    "    foff=0\n",
    "    fon=0\n",
    "    for i in range(len(y_test)):\n",
    "        if y_test2[i]==1 :\n",
    "            on_nbs+=1\n",
    "            if prediction[i]==1:\n",
    "                on_score+=1\n",
    "            else : \n",
    "                fon+=1\n",
    "        if y_test2[i]==0 :\n",
    "            off_nbs+=1\n",
    "            if prediction[i]==0:\n",
    "                off_score+=1 \n",
    "            else:\n",
    "                foff+=1\n",
    "                a=test_index[i]\n",
    "                Q_ON.append(a)\n",
    "    if on_nbs>0:    \n",
    "        ON_accuracy.append(100*(on_score/on_nbs))\n",
    "        FPOS.append(100*(fon/on_nbs))\n",
    "    if off_nbs>0:  \n",
    "        OFF_accuracy.append(100*(off_score/off_nbs))\n",
    "        FNEG.append(100*(foff/off_nbs))\n",
    "    f1= f1_score(y_test2,prediction,average='weighted')\n",
    "    fscore.append(f1)\n",
    "    \n",
    "print(\"Accuracy for ON class: \",np.mean(ON_accuracy) ,\"%\")\n",
    "print(\"Accuracy for OFF class: \",np.mean(OFF_accuracy) ,\"%\")\n",
    "print(\"False Positive rate: \",np.mean(FPOS) ,\"%\")\n",
    "print(\"False Negative rate: \",np.mean(FNEG) ,\"%\")\n",
    "print(\"F1 score: \",np.mean(fscore))\n",
    "\n",
    "\n",
    "gg = np.unique(Q_ON)\n",
    "Q_ON2=[]\n",
    "Q_ON3=[]\n",
    "errOFF=[]\n",
    "\n",
    "if len(gg)>0:\n",
    "    for i in range(len(Q_ON)):\n",
    "        aa=Q_ON[i]\n",
    "        Q_ON2.append(file_names_ALL[aa])\n",
    "    gg = np.unique(Q_ON2)    \n",
    "    for i in range(len(gg)):\n",
    "        bb=gg[i]\n",
    "        count = Q_ON2.count(bb)\n",
    "        if count>int(iteration/5):\n",
    "            Q_ON3.append([bb,count])\n",
    "            errOFF.append(bb)\n",
    "print(Q_ON3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11d55ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing in a list the adresses of all the data files in the notebook\n",
    "\n",
    "data_files_ON=[]\n",
    "file_names_ON=[]\n",
    "\n",
    "data_files_OFF=[]\n",
    "file_names_OFF=[]\n",
    "data_files_ALL=[]\n",
    "file_names_ALL=[]\n",
    "\n",
    "#Storing in a list the adresses of all the data files in the notebook\n",
    "for filename in os.listdir(pathOFF):\n",
    "    f = os.path.join(pathOFF,filename)\n",
    "    if os.path.isfile(f) and filename in errOFF :\n",
    "        data_files_OFF.append(f)\n",
    "        file_names_OFF.append(filename)\n",
    "        data_files_ALL.append(f)\n",
    "        file_names_ALL.append(filename)\n",
    "        \n",
    "for filename in os.listdir(pathON):\n",
    "    f = os.path.join(pathON,filename)\n",
    "    if os.path.isfile(f):\n",
    "        data_files_ON.append(f)\n",
    "        file_names_ON.append(filename)\n",
    "        data_files_ALL.append(f)\n",
    "        file_names_ALL.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e182b57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using data_files with only the 36 binning\n",
    "Filter=True\n",
    "binning=37\n",
    "\n",
    "\n",
    "if Filter==True:\n",
    "    \n",
    "    idx_OFF=[]\n",
    "    idx_ON=[]\n",
    "    dataON=[]\n",
    "    dataOFF=[]\n",
    "    \n",
    "    for i in range(len(data_files_OFF)):\n",
    "        dataframe = pd.read_csv(data_files_OFF[i])\n",
    "        lg = len(dataframe)\n",
    "        \n",
    "        if lg==binning:\n",
    "            idx_OFF.append(i)\n",
    "    \n",
    "    for i in range(len(data_files_ON)):\n",
    "        dataframe = pd.read_csv(data_files_ON[i])\n",
    "        lg = len(dataframe)\n",
    "\n",
    "        if lg==binning:\n",
    "            idx_ON.append(i)\n",
    "\n",
    "\n",
    "    for i in range(len(idx_OFF)):\n",
    "\n",
    "        a=idx_OFF[i]\n",
    "        dataOFF.append(data_files_OFF[a])\n",
    "    for i in range(len(idx_ON)):\n",
    "\n",
    "        a=idx_ON[i]\n",
    "        dataON.append(data_files_ON[a])\n",
    "\n",
    "idx = idx_OFF+idx_ON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b39bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyts.classification import BOSSVS\n",
    "from pyts.multivariate.classification import MultivariateClassifier\n",
    "\n",
    "iteration=200\n",
    "lgON=len(dataON)\n",
    "lgOFF=len(dataOFF)\n",
    "lg=lgON+lgOFF\n",
    "\n",
    "binning=37\n",
    "\n",
    "Labels = []\n",
    "Q_ON= []\n",
    "\n",
    "nbfeatures=3\n",
    "ON_accuracy=[]\n",
    "OFF_accuracy=[]\n",
    "fscore=[]\n",
    "FPOS=[]\n",
    "FNEG=[]\n",
    "a=binning\n",
    "b=nbfeatures\n",
    "c=len(data_files_ALL)\n",
    "\n",
    "# multivariate\n",
    "y=np.zeros((2,lg))\n",
    "bigdata= np.zeros((b,c,a))\n",
    "\n",
    "for j in range(len(data_files_ALL)):\n",
    "\n",
    "\n",
    "    dataframe=pd.read_csv(data_files_ALL[j])\n",
    "    dataframe.columns=['Iteration2','Iteration','MJD','Flux','Delta Flux','Photon Index','Delta Index','TS','fratio']\n",
    "\n",
    "    bigdata[0][j]=dataframe['Flux']\n",
    "    bigdata[1][j]=dataframe['Photon Index']*dataframe['Flux']\n",
    "    bigdata[2][j]=dataframe['Photon Index']\n",
    "\n",
    "for i in range(lgOFF):\n",
    "    \n",
    "    Labels.append(int(0))\n",
    "    \n",
    "for i in range(lgON): \n",
    "    \n",
    "    \n",
    "    Labels.append(int(1))\n",
    "    \n",
    "    \n",
    "bigdata=bigdata.reshape(c,nbfeatures,binning)    \n",
    "print(bigdata.shape)\n",
    "for i in range(iteration):\n",
    "    #3D shape of data, multivariate time series\n",
    "\n",
    "    sss = ShuffleSplit(n_splits=1, test_size=0.2,random_state=i)\n",
    "    X=bigdata\n",
    "    y=Labels\n",
    "\n",
    "    sss.get_n_splits(X, y)\n",
    "    train_index, test_index = next(sss.split(X, y)) \n",
    "\n",
    "    x_train, x_test = X[train_index], X[test_index] \n",
    "\n",
    "    y_train=[]\n",
    "    y_test=[]\n",
    "\n",
    "    for i in range(len(train_index)):\n",
    "        a=train_index[i]\n",
    "        y_train.append(y[a])\n",
    "    for i in range(len(test_index)):\n",
    "\n",
    "        a=test_index[i]\n",
    "        y_test.append(y[a])\n",
    "\n",
    "    y_test2=y_test.copy()\n",
    "\n",
    "    clf = MultivariateClassifier(BOSSVS(n_bins=5,anova=True,word_size=9,drop_sum=True))\n",
    "    clf.fit(x_train, y_train)\n",
    "    MultivariateClassifier(...)\n",
    "    clf.predict(x_test)\n",
    "\n",
    "    prediction=np.array(clf.predict(x_test))\n",
    "    on_score=0\n",
    "    on_nbs=0\n",
    "    off_nbs=0\n",
    "    off_score=0\n",
    "    foff=0\n",
    "    fon=0\n",
    "    for i in range(len(y_test)):\n",
    "        if y_test2[i]==1 :\n",
    "            on_nbs+=1\n",
    "            if prediction[i]==1:\n",
    "                on_score+=1\n",
    "            else : \n",
    "                fon+=1\n",
    "        if y_test2[i]==0 :\n",
    "            off_nbs+=1\n",
    "            if prediction[i]==0:\n",
    "                off_score+=1 \n",
    "            else:\n",
    "                foff+=1\n",
    "    if on_nbs>0:    \n",
    "        ON_accuracy.append(100*(on_score/on_nbs))\n",
    "        FPOS.append(100*(fon/on_nbs))\n",
    "    OFF_accuracy.append(100*(off_score/off_nbs))\n",
    "    FNEG.append(100*(foff/off_nbs))\n",
    "    f1= f1_score(y_test2,prediction,average='weighted')\n",
    "    fscore.append(f1)\n",
    "    \n",
    "print(\"Accuracy for ON class: \",np.mean(ON_accuracy) ,\"%\")\n",
    "print(\"Accuracy for OFF class: \",np.mean(OFF_accuracy) ,\"%\")\n",
    "print(\"False Positive rate: \",np.mean(FPOS) ,\"%\")\n",
    "print(\"False Negative rate: \",np.mean(FNEG) ,\"%\")\n",
    "print(\"F1 score: \",np.mean(fscore))\n",
    "\n",
    "\n",
    "\n",
    "gg = np.unique(Q_ON)\n",
    "Q_ON2=[]\n",
    "Q_ON3=[]\n",
    "errOFF=[]\n",
    "\n",
    "if len(gg)>0:\n",
    "    for i in range(len(Q_ON)):\n",
    "        aa=Q_ON[i]\n",
    "        Q_ON2.append(file_names_ALL[aa])\n",
    "    gg = np.unique(Q_ON2)    \n",
    "    for i in range(len(gg)):\n",
    "        bb=gg[i]\n",
    "        count = Q_ON2.count(bb)\n",
    "        if count>1:\n",
    "            Q_ON3.append([bb,count])\n",
    "            errOFF.append(bb)\n",
    "print(Q_ON3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781b617f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
