{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70449042-90fe-4f99-9e2f-ef0b5e58ecf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import netCDF4\n",
    "from netCDF4 import Dataset\n",
    "import geopandas as gpd\n",
    "import xarray\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nbimporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99163c3-d71b-4d1b-8fcc-f50bc73d3391",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i Set1.py\n",
    "# aaa=coord_lister1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bbd929-87f0-46eb-abd3-8573d7b004ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa=coord_lister1(French_wine_regions['geometry'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97260734-84e5-4b54-b563-d906be5f2be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#France geojson files\n",
    "URL_French_regions='https://france-geojson.gregoiredavid.fr/repo/regions.geojson'\n",
    "French_regions = gpd.read_file(URL_French_regions) \n",
    "\n",
    "URL_French_departments='https://france-geojson.gregoiredavid.fr/repo/departements.geojson'\n",
    "French_departments = gpd.read_file(URL_French_departments)\n",
    "\n",
    "path_wine_regions ='Geojson_files/bassinsviticolesfranceaop.geojson'\n",
    "French_wine_regions = gpd.read_file(path_wine_regions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ada537-b8b5-495a-b9ff-6692f8543fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Picking all the regions related to the regions of agreste productivity data\n",
    "French_wine_region_choice=[3,4,6,8,10]\n",
    "# ALSACE ET EST,BOURGOGNE BEAUJOLAIS SAVOIE JURA,CHAMPAGNE,LANGUEDOC-ROUSSILLON,Sud_Ouest,Val de Loire\n",
    "\n",
    "French_departments_choice=[54,71]\n",
    "# Gironde(pour bordeaux),Charentes\n",
    "\n",
    "#geojson manquants : Sud-est\n",
    "#Données manquantes : Corse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16085633-a480-4f4c-864d-471552c6d6e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457f778a-7d4f-4fcb-b649-d79b312d2415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'netCDF4._netCDF4.Variable'>\n",
       "int16 sp(time, latitude, longitude)\n",
       "    scale_factor: 0.5523896357560313\n",
       "    add_offset: 86484.09880518212\n",
       "    _FillValue: -32767\n",
       "    missing_value: -32767\n",
       "    units: Pa\n",
       "    long_name: Surface pressure\n",
       "    standard_name: surface_air_pressure\n",
       "unlimited dimensions: \n",
       "current shape = (744, 101, 141)\n",
       "filling on"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name='era5-land'\n",
    "\n",
    "#Extraction des caractéristiques\n",
    "f= netCDF4.Dataset('era1/era5-land-2019-01.nc')\n",
    "f.variables.keys()\n",
    "np.asarray(f.variables['time'])\n",
    "f.variables['sp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198c3cef-9b04-4ab0-8f5e-ad184ac4b1cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(744, 101, 141)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(f['t2m']).shape\n",
    "\n",
    "# Shape of data is : hourly measurement/latitude/longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677197a4-7f2e-4d9e-ae2f-2029789b9499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LANGUEDOC-ROUSSILLON\n"
     ]
    }
   ],
   "source": [
    "print(French_wine_regions['Bassin'][6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf46ff1-ad62-4e1a-9b1b-ac7b81578aad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"100.0\" height=\"100.0\" viewBox=\"1.8489006600130489 42.31114120608248 2.528998632618709 1.7770907778199856\" preserveAspectRatio=\"xMinYMin meet\"><g transform=\"matrix(1,0,0,-1,0,86.39937318998494)\"><g><path fill-rule=\"evenodd\" fill=\"#66cc99\" stroke=\"#555555\" stroke-width=\"0.05057997265237418\" opacity=\"0.6\" d=\"M 2.674032174228978,42.4048078221054 L 2.673026061110764,42.40536633075506 L 2.671580671818615,42.413952277914746 L 2.675145186866001,42.418221268131994 L 2.670932986498765,42.42340640380519 L 2.666364154963733,42.42470477581432 L 2.662389109831504,42.424787663819075 L 2.650686756625279,42.42586718718476 L 2.649986969018951,42.430002093652725 L 2.65058434868289,42.43288496198568 L 2.6552052825044,42.43908718796492 L 2.659273752426178,42.443266300092056 L 2.660198118853537,42.44605509245997 L 2.658446404049504,42.45047037124995 L 2.659880115242959,42.454379631703624 L 2.661748611033927,42.45707779376158 L 2.664496557488049,42.458936813460255 L 2.665463144733761,42.46101116244593 L 2.664698678426976,42.46696740941868 L 2.663550631493871,42.469376122290896 L 2.659687875772157,42.47215977650957 L 2.660181949178423,42.47659905456376 L 2.665343668800974,42.482583630528936 L 2.662615485283103,42.49364626785948 L 2.660183745808991,42.4968666512045 L 2.657119592374859,42.50216515222323 L 2.661444980467895,42.5090988871088 L 2.665278091785233,42.50903663923308 L 2.665239464228016,42.51088881812129 L 2.662572366149465,42.51200393975582 L 2.659568399839369,42.51441091893007 L 2.653328701875875,42.517816950580446 L 2.651965957589866,42.51963311590767 L 2.648976364324316,42.52611738194265 L 2.650040867935997,42.526889967095784 L 2.66662736134198,42.53081431380174 L 2.678840855944869,42.53518720727089 L 2.683058446203811,42.53692807116872 L 2.68370163994724,42.53813340895467 L 2.682979394458808,42.544171000166045 L 2.683288414916545,42.548031390211015 L 2.686974202527287,42.55600886137262 L 2.695724691709896,42.55550860364224 L 2.697623730220524,42.558150792578914 L 2.699429343941605,42.565899377179264 L 2.702267121924138,42.572325254771755 L 2.699751839128604,42.574522193350695 L 2.699154459464664,42.5737621050191 L 2.693523819263803,42.57330168147223 L 2.678472546678381,42.57034192366959 L 2.66629139142572,42.571962067769334 L 2.664023145333318,42.573443910373236 L 2.663485054478131,42.57750821221875 L 2.657397171797652,42.589599663527785 L 2.654219830637722,42.59339574070015 L 2.652678321610172,42.59347179214864 L 2.644438973824228,42.59117962687149 L 2.640472013529557,42.59166571050945 L 2.638023206065047,42.59001764014599 L 2.635166563461547,42.586347683253535 L 2.631013651903062,42.5868801016442 L 2.623392345032592,42.592287362971085 L 2.620530312537387,42.59483541193559 L 2.618169539970721,42.59557804973549 L 2.616163601941282,42.59623801784211 L 2.611940621790636,42.60183093385934 L 2.61541081373319,42.60387872280412 L 2.620383887146076,42.60193474654263 L 2.622870423852519,42.602374460767216 L 2.625710896780904,42.60556940385991 L 2.630169235535989,42.606604815907204 L 2.634155958766912,42.60627290408547 L 2.632687213277377,42.61647078135959 L 2.628387876327581,42.62258474020413 L 2.62521502674407,42.630254599127426 L 2.618140793881629,42.62939406740105 L 2.608356343807,42.629275098945214 L 2.605945265584423,42.630374887560784 L 2.600845529716476,42.62966372839247 L 2.594070435843646,42.62757779087919 L 2.59221541478194,42.628968423654925 L 2.582349218016455,42.627376860380906 L 2.577955557961827,42.6270034187517 L 2.575425902121746,42.625416432078346 L 2.57242822401864,42.6225827571815 L 2.572799228230981,42.61925118997434 L 2.574821335935534,42.61800445053747 L 2.560927093436057,42.61861724658262 L 2.552431725794139,42.62002724953148 L 2.552474844927776,42.61916657642251 L 2.543045229390374,42.61780018384895 L 2.535758095805596,42.61846454392517 L 2.534410622879417,42.61763624116762 L 2.535129275106712,42.61528745044655 L 2.53560178894616,42.60205575091693 L 2.53751430218605,42.598147124704944 L 2.52987772395575,42.59421841404988 L 2.528339808189337,42.590452809026644 L 2.530011572933083,42.588992534270304 L 2.532704722154874,42.58116083040731 L 2.532428041047365,42.580590660350325 L 2.525712235983288,42.58275092909801 L 2.520546024784316,42.58297912213001 L 2.5112529531701,42.5812540943868 L 2.50898740202355,42.58400498831283 L 2.5101102961287,42.58704677081453 L 2.506479305750289,42.59505033473778 L 2.50384544533725,42.59602111526751 L 2.498960406822208,42.5919785215125 L 2.496277139068543,42.58853883686864 L 2.495236889969533,42.585464718507474 L 2.496236714880757,42.57385339549098 L 2.495067108380834,42.56786564825038 L 2.498158211273489,42.56115413670009 L 2.505977147506466,42.55124632455743 L 2.504617098166309,42.54310476773656 L 2.506369711285626,42.54030970951941 L 2.506245743776418,42.53918317897652 L 2.50191766073753,42.53293526494797 L 2.497730613198248,42.52961942664838 L 2.489034921247971,42.52759369315704 L 2.474889150468942,42.52658013975007 L 2.47214120401482,42.52483369105419 L 2.465470314714949,42.53402748697402 L 2.466970501239429,42.538357793815585 L 2.469863076454293,42.54229134788432 L 2.469530699799169,42.54562769176071 L 2.466799821335445,42.5516638965096 L 2.463021507250438,42.55656734542399 L 2.463032287033848,42.56160406344356 L 2.460911364648042,42.56678328059146 L 2.460260086067055,42.570202995511686 L 2.461106299064696,42.57587564363308 L 2.464292623377467,42.5823990471914 L 2.46355600484449,42.58417695649156 L 2.463665599309152,42.59323041114415 L 2.464884613149703,42.5987369720122 L 2.463256865854878,42.60173307206321 L 2.461144028306629,42.60344298646061 L 2.468529976572659,42.602744744023035 L 2.471674978382361,42.610498366553365 L 2.474840641443599,42.6141001304624 L 2.475949162504203,42.61947660520358 L 2.471089276817116,42.62829888808349 L 2.467452896547,42.63156916725151 L 2.473472507265885,42.6382261791658 L 2.478614463952185,42.64138822512859 L 2.486607673350281,42.64470010597947 L 2.486927473591427,42.646838990392816 L 2.481222273221984,42.650551633660854 L 2.478176086093535,42.650691043426555 L 2.474010598121072,42.65425282392384 L 2.469625921219285,42.65735657982722 L 2.467324437461371,42.66335950973161 L 2.460163966331654,42.67229247687563 L 2.469361816525754,42.68001366348293 L 2.475429936269981,42.68251255729244 L 2.48075694590481,42.684095446220695 L 2.485858478403325,42.686817366165826 L 2.485730019317696,42.688479382361955 L 2.488915445315184,42.69134835216391 L 2.476854664310595,42.69847497313536 L 2.470787442881651,42.7016035879333 L 2.46748343926666,42.70447459202083 L 2.46866741881113,42.70974820597914 L 2.471204261173483,42.71647077049188 L 2.462671164289632,42.71932920735619 L 2.455358877876899,42.719436783914375 L 2.448347527084346,42.71602922232498 L 2.438501093255112,42.70941750433914 L 2.43419816304418,42.707778491962465 L 2.427553324887548,42.703450050267804 L 2.417552380829445,42.69329877978164 L 2.417622449421606,42.6853593510999 L 2.415151184074993,42.680363673161104 L 2.411741179256476,42.6810286861177 L 2.408578211141091,42.68347074810687 L 2.405207732195074,42.68499154026373 L 2.39969207635058,42.686141189520775 L 2.389293178621613,42.68759786560289 L 2.376632323007232,42.68777350964051 L 2.375682803751918,42.69262267369125 L 2.374532960188245,42.694758589215404 L 2.367631203860355,42.69906981159265 L 2.365712402413476,42.70175080521448 L 2.36228532960456,42.70412537833557 L 2.363036321182083,42.70525552969446 L 2.375335155736964,42.712791766484855 L 2.387625007139003,42.71922493051442 L 2.392787625076838,42.722229749270475 L 2.394437830253765,42.72439236315454 L 2.399295919310284,42.725924031563984 L 2.403529679244339,42.72603753586092 L 2.405815891642423,42.72699439522522 L 2.406271337491472,42.73572884897737 L 2.405575143146279,42.74164767710456 L 2.406132996937717,42.74563305053258 L 2.408246732801251,42.747486119033184 L 2.406173421125503,42.749337153159026 L 2.409736139542321,42.75495918195357 L 2.413460554710281,42.75740559137146 L 2.416377384437817,42.76095993540213 L 2.423436345940428,42.765056840550905 L 2.429191851965782,42.77349843350571 L 2.423306988539514,42.77680854806763 L 2.420886927164097,42.78308738877723 L 2.419621200928772,42.786903963966495 L 2.41819467625759,42.7948600644724 L 2.416670235220439,42.79454563695868 L 2.409289676846114,42.79538872096777 L 2.400312812211907,42.793943144555 L 2.394149471047563,42.79460100795748 L 2.389062311593594,42.79763710855478 L 2.383163973438065,42.79859418103227 L 2.380265110016212,42.798314047714086 L 2.367315895195629,42.798924407150416 L 2.359189735135483,42.800999979665434 L 2.344027969770115,42.801328215624146 L 2.333441324146766,42.80191547681988 L 2.333774599117174,42.81237385612285 L 2.332463957117644,42.8196117668643 L 2.329293802479986,42.82304987302085 L 2.319077262753694,42.8239512076637 L 2.320529838568116,42.82843462514721 L 2.323682925215376,42.833942096291125 L 2.325155263966047,42.835390031747885 L 2.327199829552704,42.83741300031673 L 2.330409510062863,42.83803614824643 L 2.338597653877612,42.84112939843281 L 2.347354431267209,42.84165107272638 L 2.352252944511513,42.84313571258214 L 2.370866935513753,42.84630971866775 L 2.378277138292455,42.847418836940285 L 2.38215965695042,42.848744615491086 L 2.397014198488621,42.845502895060825 L 2.401356654572054,42.84386550472043 L 2.416048601043829,42.84103388940456 L 2.435679484947693,42.83935224764005 L 2.442786057160362,42.83734054084729 L 2.452484268967716,42.83696177405597 L 2.456854572824958,42.837399825874066 L 2.461941732278927,42.83829502286433 L 2.474654690179786,42.83941482424042 L 2.476437846018764,42.84154107358902 L 2.481923857458881,42.844702647359995 L 2.488799562643533,42.84629654612205 L 2.493776229317555,42.84692355618202 L 2.500431847257596,42.85055640096669 L 2.502749500690624,42.85143230872931 L 2.509045792517018,42.85002426447091 L 2.51579753019246,42.84534811544993 L 2.518910192651934,42.84746757445986 L 2.519592013952581,42.8506637497288 L 2.524512086763704,42.85821852645913 L 2.532787367161013,42.86504811310228 L 2.531459657171084,42.869108501284366 L 2.532380430337307,42.87197108889485 L 2.529432159574826,42.87572490666262 L 2.526041917692559,42.87833572279011 L 2.525378961012879,42.881301228387954 L 2.527019284721681,42.89137564167969 L 2.530028640923482,42.89493018860049 L 2.530880243812827,42.90100686388367 L 2.528935391222708,42.904154864747944 L 2.533036200494714,42.902350574550276 L 2.537932018793165,42.90153263858705 L 2.549745763094621,42.9012411270087 L 2.567946529066167,42.90935029384675 L 2.578168458684163,42.90797975901062 L 2.582714832337092,42.91112279826906 L 2.593457784819877,42.91298736000637 L 2.590216663274774,42.91915369679561 L 2.586945897325295,42.92875426982088 L 2.569417071186271,42.9336575823504 L 2.542270881615463,42.919076728294804 L 2.53602489544498,42.91886358425233 L 2.528732371968498,42.91988456383694 L 2.521534171596848,42.924377462378885 L 2.521099386999334,42.925905499467824 L 2.522605861730802,42.934678316765094 L 2.527266321424814,42.939216170884514 L 2.536361763676525,42.945909224090016 L 2.54895614395988,42.95609931197496 L 2.559741317261019,42.97485102773327 L 2.558230350953131,42.97962448593246 L 2.534690897248062,42.98691047534858 L 2.502760280474034,43.00252378075161 L 2.500916937511021,43.00709933133198 L 2.502350648704476,43.008745526464295 L 2.501340942325125,43.010380510601074 L 2.503477136070761,43.01221250821463 L 2.508514888184103,43.021556881967946 L 2.506048114413912,43.02847409961503 L 2.50982463186835,43.03462689836646 L 2.499105933898236,43.03404250749397 L 2.49554141885085,43.03350867141764 L 2.491378625824239,43.034800244701145 L 2.492881507294571,43.039438390543445 L 2.496363377335819,43.04331790661885 L 2.498782540395953,43.044871876482866 L 2.492143990446309,43.049665497472674 L 2.486202533157143,43.04912523271003 L 2.480544045182474,43.048465486175566 L 2.48146122508756,43.044776026502355 L 2.487813212461569,43.04009101203428 L 2.49001588153823,43.0349328805811 L 2.483939676956446,43.03589678024543 L 2.468538959725501,43.031149362397876 L 2.466144051178038,43.03085846441232 L 2.470286182953113,43.03475559493448 L 2.47143243325565,43.04645206656108 L 2.465645486195352,43.050582558113064 L 2.460177441060917,43.0520208124318 L 2.449867476545076,43.053291647148505 L 2.443650236463685,43.05811611562971 L 2.445278882073794,43.060446821491375 L 2.440682202764954,43.06282666232938 L 2.430880684699927,43.06696720767263 L 2.426769095644511,43.073996765540464 L 2.42228560406147,43.078652944329136 L 2.414932893460952,43.08426469186083 L 2.407566708131172,43.083080447934414 L 2.403380558907175,43.08308372842012 L 2.399812450598653,43.08613056769639 L 2.39853414794935,43.08864783543424 L 2.390919129285869,43.093599406575144 L 2.382528864532193,43.09516981332511 L 2.377731860914995,43.09381981696944 L 2.370541745380902,43.08798064162949 L 2.366113051030192,43.07997309298841 L 2.366325951752529,43.07725599660919 L 2.367482083523191,43.07475335304964 L 2.366001659934962,43.07172563453106 L 2.367287149106537,43.068017230192694 L 2.371201108799446,43.064263316893395 L 2.378820619039347,43.06016393851434 L 2.386303585356063,43.058544063757196 L 2.390957756843086,43.05696024694048 L 2.392316009552675,43.0555227648519 L 2.396385377789737,43.05439704188151 L 2.39406592772614,43.05137816439648 L 2.391416795953272,43.04273950727651 L 2.387850484275317,43.036843593376865 L 2.388382286923516,43.03639645251877 L 2.38642934949584,43.03536230369127 L 2.382428253220371,43.03616795637303 L 2.369995569688157,43.03661903846332 L 2.363214187608339,43.03842858626723 L 2.357874601559533,43.040145506271315 L 2.346089603347169,43.042259582683876 L 2.342577190586261,43.04359233254924 L 2.339345052194,43.043381589300594 L 2.329532754345562,43.041392294930596 L 2.32549392882816,43.041286591345816 L 2.320372633393395,43.042948939677245 L 2.318912871056701,43.041553804404096 L 2.316270027490821,43.034152819904975 L 2.318706258541353,43.02652700760995 L 2.319228179721427,43.02245728737437 L 2.322340842180901,43.01880961158184 L 2.327283372874127,43.015229371921514 L 2.336398578062088,43.01528454505738 L 2.344397177351888,43.01425332467999 L 2.349987393364963,43.01236489852663 L 2.352102925859064,43.01100979382016 L 2.353701927064797,43.00655475028608 L 2.345850651481593,43.006699928576076 L 2.34333357205549,43.00543601195401 L 2.336133575053272,43.00519623273708 L 2.333027200800787,43.00555425889449 L 2.330701462530202,43.01012498561626 L 2.327749598506585,43.01130866824733 L 2.321692258545767,43.01174088407725 L 2.31142002327186,43.01510457535989 L 2.309955769358745,43.017591920031805 L 2.302316496182593,43.011816423005776 L 2.298086329509674,43.01142033524251 L 2.298491469702812,43.00825351187129 L 2.296160341540522,43.006938388099634 L 2.292539232630236,43.0031859920779 L 2.290790212772055,42.998965586575615 L 2.282174470882065,42.99788941562503 L 2.278759974487127,42.996491285044364 L 2.276992090007979,42.99400045762354 L 2.278759974487127,42.99006065740827 L 2.28352553706938,42.98674685071842 L 2.289735590628499,42.98490686505209 L 2.288574967281416,42.98108280439608 L 2.285537763305808,42.97694632115743 L 2.294375389070976,42.972681390519334 L 2.307087448656551,42.97282467700976 L 2.310337553354496,42.972285050460705 L 2.315456153843409,42.96962431342701 L 2.320370836762827,42.970919202300095 L 2.325138195975649,42.97322692904282 L 2.329706129195397,42.97774682532958 L 2.338072139436402,42.97984924930231 L 2.342886211043998,42.98058202475828 L 2.351682514306097,42.98045847219935 L 2.359695486640443,42.98264229280791 L 2.362027513118017,42.98271721034516 L 2.365229108790619,42.981325963641495 L 2.375024338648658,42.975219089463565 L 2.378235815789386,42.97283782254262 L 2.37911346982197,42.96988855184749 L 2.36858521469209,42.966841853388225 L 2.356492992652556,42.96400404834685 L 2.355849798909127,42.95532285220823 L 2.353318346438479,42.94648787794959 L 2.357095762208201,42.93770161704527 L 2.350881217072662,42.93892154934805 L 2.345043066041169,42.93829941977588 L 2.343109891549744,42.93632316310644 L 2.334909171321017,42.93418373622332 L 2.330188524502969,42.93478946530299 L 2.324081777201525,42.93817841287097 L 2.31501148777777,42.941141698353725 L 2.311181071406284,42.94185849476953 L 2.303843632165596,42.94066952881618 L 2.299611668862109,42.93925299847746 L 2.298218381856439,42.93881566938903 L 2.301779303642689,42.93377662505762 L 2.305396819291839,42.930173014867 L 2.303281286797737,42.92825832619948 L 2.302550956471748,42.92492934928158 L 2.308642432413362,42.91703209394544 L 2.305490244081387,42.91281761748206 L 2.302386564774754,42.911460318389416 L 2.29610554430819,42.91363474580729 L 2.291828665240497,42.91706301395338 L 2.286278873415207,42.92358481520981 L 2.282567932976509,42.92661918439067 L 2.279678951022781,42.92803666310402 L 2.276237505169319,42.92850037875759 L 2.268317957624521,42.931207619184114 L 2.268508400464754,42.9345112647391 L 2.264499219351729,42.93476907712467 L 2.252008145326047,42.93304394971771 L 2.24229196721301,42.932634857899174 L 2.241562535202305,42.92902657668937 L 2.24216530475795,42.92703028946389 L 2.245476494895214,42.92263822983477 L 2.243953850488631,42.91683473182707 L 2.245147711501227,42.91462620661021 L 2.244324854700973,42.91241891808628 L 2.238433703067717,42.90908250571012 L 2.229943725317504,42.91111029748838 L 2.225704575491743,42.915539362767475 L 2.221478900395245,42.909902315351644 L 2.216597455141339,42.91016352046692 L 2.209190845623774,42.908729181499055 L 2.203967142246619,42.90693950075365 L 2.198799134417079,42.90866930699072 L 2.194771088683087,42.90880418904281 L 2.190462768580451,42.907402717224706 L 2.188236743306402,42.90578012836157 L 2.187783094087921,42.90094237556381 L 2.186043057382582,42.89815088792527 L 2.184397343782075,42.898129829546725 L 2.180628911165194,42.89678076218374 L 2.172058983354694,42.899037965588796 L 2.167637475526257,42.902526926175824 L 2.167263776368064,42.90444373052124 L 2.156583705955167,42.90871997004002 L 2.15285120594965,42.9106503985835 L 2.154075609681905,42.91361632435411 L 2.139168067541942,42.920120070901675 L 2.130345713136604,42.92303160064489 L 2.12170751336451,42.92229550840264 L 2.119084432734881,42.92097920356753 L 2.115687902645626,42.91581304382181 L 2.114160766662622,42.92058253047926 L 2.11638230036025,42.92485501915251 L 2.121667089176725,42.92749072388866 L 2.131323080165726,42.93491968641171 L 2.134403403274972,42.945556111051026 L 2.132718163801964,42.94688504176889 L 2.12756452901697,42.94850128796476 L 2.121689547058828,42.94961185727297 L 2.118063048256838,42.94875180890173 L 2.110421080134833,42.94513329333219 L 2.10545519324422,42.945841494930406 L 2.098601945941672,42.94416468129727 L 2.089413078900414,42.942705487483934 L 2.082920056026798,42.94287054449012 L 2.081085696216626,42.94601443428725 L 2.077452010892362,42.949019424629306 L 2.077563401987593,42.95122934688479 L 2.081485446518059,42.95715254656756 L 2.081845670946991,42.96474161415865 L 2.080649114988543,42.97173227341048 L 2.080464062040015,42.97338270240774 L 2.086069549412921,42.97347734932284 L 2.091308524149906,42.97113085159485 L 2.100998651119703,42.97431339074676 L 2.107183551850866,42.97524998025858 L 2.111163088559516,42.97529072979423 L 2.127279763071904,42.97635678063328 L 2.132120784138024,42.98046438695516 L 2.139341442391777,42.98042232645697 L 2.144619044685979,42.978769457535336 L 2.150288312444057,42.97877405802739 L 2.150187701132236,42.98354918317508 L 2.153904031462639,42.990491051473995 L 2.14891299174407,42.99767851544609 L 2.144120479703292,43.000378771449384 L 2.133095456221294,43.00159746126612 L 2.125923306992884,42.99590193193702 L 2.116865593983106,42.990473310098785 L 2.113937984472161,42.98739674677927 L 2.109073607208654,42.986179746721945 L 2.098384553642915,42.98504355159523 L 2.091890632454016,42.984755063775395 L 2.089483147492575,42.99048908021034 L 2.088217421257251,42.997114140378855 L 2.089104058442677,43.003143290173945 L 2.094985328607807,43.005739511976955 L 2.097797055447101,43.008338251648205 L 2.09987935027569,43.00811819074478 L 2.099382581923572,43.009958138394396 L 2.096802620427581,43.01258231680856 L 2.095866575901528,43.01639128141974 L 2.097261659537766,43.01944931688462 L 2.101912237763653,43.02199427967902 L 2.105696840055648,43.02235352141432 L 2.110385147523468,43.02175062458177 L 2.116898831648619,43.036302559090586 L 2.117270734176244,43.03658358234922 L 2.113723287119256,43.03942985520633 L 2.108601991684491,43.04031030180206 L 2.102001170976781,43.03975091437704 L 2.096938266035483,43.041690364928 L 2.085062537979423,43.03825131071979 L 2.08069403075275,43.03799984119429 L 2.077531062637365,43.03910748121987 L 2.07372040920213,43.038940712749515 L 2.069540548185122,43.03985071156841 L 2.064268335782624,43.04331987618733 L 2.064141673327563,43.044311874135715 L 2.068021497039676,43.04768165868128 L 2.06978219499655,43.050990209156474 L 2.075292460949339,43.05738557764597 L 2.077220245549059,43.06701511534171 L 2.072621769609652,43.075204807581954 L 2.074315992235501,43.07901513506301 L 2.076745935079044,43.08132929958988 L 2.080674267816499,43.08342949062883 L 2.081329139658622,43.084985724457404 L 2.081701042186248,43.08707398117324 L 2.071398264192681,43.0920479835896 L 2.066517717254059,43.09529707092186 L 2.065224143244928,43.10138479163833 L 2.066734211237532,43.10337541910809 L 2.064757917612469,43.10511216410851 L 2.06109907946025,43.104050971545746 L 2.062177956116478,43.098463338450514 L 2.058690696183526,43.09479328672198 L 2.053406805682335,43.09536725927703 L 2.050703774992419,43.09420159738251 L 2.052883986186977,43.08947968537739 L 2.048832584255599,43.092882411322975 L 2.041938912765265,43.09484970026292 L 2.038476805660269,43.096845787800405 L 2.032008037299324,43.102310261092605 L 2.031050433206453,43.104233303838754 L 2.034952714800668,43.11223309042303 L 2.033805566182847,43.11425221872341 L 2.02468676773375,43.11644439594371 L 2.01452502523979,43.117461439830436 L 2.009079437987457,43.120606890238484 L 2.001632404282107,43.12108816857402 L 1.991177811005524,43.120666558466596 L 1.985443864546989,43.121338641632256 L 1.982307845890127,43.12419474880822 L 1.980937016766561,43.12717856959818 L 1.978577142515179,43.128420310179834 L 1.980294721338415,43.12934143725067 L 1.985240845292778,43.13528352076146 L 1.986444587773498,43.13829564288127 L 1.990096239403444,43.14255890224576 L 1.995602013779812,43.146017665920155 L 1.995038770096669,43.14920883596933 L 1.992830711128303,43.152192747103946 L 1.993025645544958,43.15422300791998 L 1.996721314623825,43.15496942965529 L 1.999586042064882,43.15873154475921 L 1.996263173828924,43.16352674254807 L 1.996147291157273,43.16573156848094 L 1.99918000355646,43.16912022751489 L 1.991835377793499,43.17486387655761 L 1.981193934937819,43.17387399222463 L 1.981383479462768,43.1771724524545 L 1.982542306179282,43.17918616163412 L 1.979802444562718,43.18296447121194 L 1.971134600386249,43.18873432668816 L 1.965618046226471,43.19069722390602 L 1.962586232142567,43.1944584484971 L 1.965362924685781,43.199196488714506 L 1.964598458378995,43.20210003465036 L 1.962183786895282,43.20371218294736 L 1.966421140090473,43.20625669751094 L 1.972045492084346,43.20879193945011 L 1.969222985461643,43.213897460723516 L 1.965570435516412,43.21755643841464 L 1.958342590740387,43.21790929654686 L 1.95067187652929,43.22077398383118 L 1.946578253779557,43.22182660475834 L 1.946366251372506,43.229269713317585 L 1.94976906966875,43.23589122786079 L 1.944322584101133,43.238023415766655 L 1.942567276035964,43.24111030945175 L 1.944079140659137,43.24615946809756 L 1.949158215275549,43.24459097567472 L 1.953804301925015,43.2442919301909 L 1.965413230341691,43.245245991397354 L 1.968540265845711,43.2465147780421 L 1.972369783901913,43.25202149002042 L 1.970839054657773,43.24739682765663 L 1.971033090759143,43.24534479935157 L 1.974561673195165,43.24538471510218 L 1.986935966233911,43.241814442221035 L 1.988272659376681,43.241983930213344 L 1.989431486093195,43.23615628290433 L 1.992465096807667,43.22917153382389 L 1.993711958422024,43.22518531282946 L 1.993065171417459,43.22146002241831 L 1.988296913889352,43.21461041884784 L 1.999585143749598,43.21197655685884 L 2.012508307426941,43.20637848581918 L 2.014947233423326,43.2079708783136 L 2.021869651002751,43.20802849699955 L 2.032983607697878,43.20683289811344 L 2.042490478349714,43.20456735073783 L 2.048612497010989,43.20101826056942 L 2.054759668500219,43.20280396329348 L 2.058504744919714,43.20584222257555 L 2.058257708216581,43.20962673908285 L 2.059014988001093,43.2123065323788 L 2.061139503648036,43.21498882666092 L 2.063873077057611,43.215723376300886 L 2.066460225075876,43.2185737641099 L 2.066879738313559,43.225465469553676 L 2.071741420631215,43.23124112408044 L 2.07717892304599,43.234522740145714 L 2.08458643087884,43.238352595894284 L 2.088592917046013,43.23937939087112 L 2.096437904422228,43.237469106443704 L 2.10324174438415,43.234818561231926 L 2.105189291920121,43.23309662675269 L 2.11236772935552,43.228953574782636 L 2.125399589182241,43.23167178899414 L 2.128503268488875,43.236840185037174 L 2.133047845511235,43.23603324515095 L 2.134929816031466,43.24140478975978 L 2.141503687280652,43.24131317381647 L 2.1437863064176,43.2440053162926 L 2.144341465263186,43.24647944344428 L 2.142184610266015,43.25269932556851 L 2.137679559116155,43.25441808903193 L 2.132972387027369,43.2590572650844 L 2.135682604239558,43.266900731338716 L 2.138965946603015,43.26535828460562 L 2.147467602451922,43.26521764396431 L 2.151355511001591,43.264095777913205 L 2.163634582620221,43.26425539160412 L 2.171686182511785,43.26324079089505 L 2.17386369876049,43.264528827200415 L 2.174741352793075,43.26787602417937 L 2.174739556162506,43.27172471342998 L 2.173527728844229,43.276579706215514 L 2.170207555554123,43.2828756422494 L 2.167468592252843,43.2909448931578 L 2.162990490561507,43.29704189496548 L 2.169044237261189,43.29731649084868 L 2.17346035519792,43.301475813023316 L 2.178733465915702,43.30377502068511 L 2.183817930423818,43.30281272427739 L 2.190158239699134,43.29882674604055 L 2.189924677725263,43.30027877853983 L 2.19190636124203,43.30961253856083 L 2.192289941868349,43.31601616720987 L 2.189156618157341,43.31869520286897 L 2.180912778794976,43.32027877830564 L 2.177171295636618,43.31824227832549 L 2.171844286001789,43.32299226270791 L 2.173045333536657,43.32466917730126 L 2.18117329022737,43.32575267739006 L 2.18291781850913,43.33069353259653 L 2.184495260148045,43.3376104466996 L 2.188333761357087,43.341761131648006 L 2.19551309710777,43.34311479596007 L 2.200746681953051,43.34352637709823 L 2.206968413610863,43.337366087758134 L 2.210022685576869,43.336811376243794 L 2.216080025537687,43.33991416741921 L 2.221581308337635,43.33789857990998 L 2.227298186805772,43.331972299768296 L 2.228979833017643,43.33134108745221 L 2.227556901607598,43.326285924861125 L 2.227758124231241,43.32384052848625 L 2.240417183215053,43.3205441195541 L 2.246266114029955,43.32006375954743 L 2.251179898634089,43.317957319739605 L 2.251406274085687,43.32568079318174 L 2.258979970246099,43.32905925901748 L 2.261188927529749,43.3417833445299 L 2.263420342695501,43.34553393842722 L 2.265593367367787,43.35161566658472 L 2.265639181447276,43.35160129587234 L 2.266537496731396,43.3490223511191 L 2.270695798181586,43.34763942198225 L 2.278090729600458,43.350221691641224 L 2.283249754277156,43.344038563062206 L 2.288503102058687,43.34137501819657 L 2.299042136971977,43.34163177411546 L 2.301489147805919,43.342125683211364 L 2.304214636377937,43.34877411863917 L 2.303828360805766,43.3552572260557 L 2.304533538303799,43.362089704273565 L 2.302794399913744,43.36535185795595 L 2.300607002196913,43.36697993029707 L 2.300772292209191,43.367006705289256 L 2.311665263344425,43.36913299375932 L 2.312903141805942,43.36992314953208 L 2.317409091271085,43.35861249288849 L 2.319880356617697,43.35692018663177 L 2.331144331965272,43.35517949818968 L 2.334015347613319,43.35597179521792 L 2.332568161690602,43.35959219319459 L 2.349008229705273,43.35798482324532 L 2.352694017316016,43.35805340349106 L 2.360706989650362,43.363361279333255 L 2.365827386769843,43.36217264809205 L 2.369576056450474,43.36242017267147 L 2.381080780294192,43.36787917173778 L 2.382843274881635,43.369740304483535 L 2.386198482467822,43.367864151837715 L 2.38874071472188,43.36918719485604 L 2.392501960816488,43.36960904751954 L 2.396409632302408,43.36836503084688 L 2.402795755657213,43.36827752425167 L 2.411060256271113,43.371341485972316 L 2.421942447622937,43.37264551649836 L 2.430176405517177,43.37454829290729 L 2.43854421238875,43.37581895385773 L 2.443674490976357,43.379618371618676 L 2.446953341763392,43.383984894195706 L 2.451096371853752,43.38826297170726 L 2.454481223844314,43.39350227500543 L 2.456014648034306,43.3971452365043 L 2.459233311697306,43.397831898139835 L 2.468906370676706,43.39646117801247 L 2.470904223868587,43.39268566283448 L 2.47016131712862,43.382933771419125 L 2.472844584882285,43.38362712277481 L 2.475296087292647,43.38601527230263 L 2.477037920628555,43.39021944034006 L 2.480705741933615,43.39210599859676 L 2.492668606572235,43.39653950571218 L 2.497096402607661,43.39773464330515 L 2.505774128252255,43.401111057037845 L 2.508527464598081,43.403725641358236 L 2.511873689031426,43.40368321889864 L 2.513063956782885,43.400187506153266 L 2.51559451093825,43.39856293619589 L 2.512886090356629,43.39724379773066 L 2.504306281078003,43.38892623274718 L 2.498584911033447,43.38472589215691 L 2.496491836421448,43.381962282734236 L 2.496988604773566,43.38053701276161 L 2.502090137272081,43.37610821159722 L 2.508628974225187,43.37434064896719 L 2.518282270268335,43.37044164285053 L 2.518786225142726,43.36777468539941 L 2.517972351495314,43.36239470185107 L 2.520498414074258,43.35742180915478 L 2.521427272078038,43.352881582349426 L 2.525993408667217,43.35134262246773 L 2.528981205302199,43.35136548506053 L 2.533764734190135,43.34482185951411 L 2.539556172826853,43.34591087957165 L 2.544815808815373,43.34983563200082 L 2.545358391246981,43.35372028599132 L 2.553007545891259,43.36420767507597 L 2.557224237834916,43.36685454481061 L 2.552391301606354,43.37523978127959 L 2.552819797996878,43.37923249901309 L 2.554825736026317,43.38485646354625 L 2.557204474898666,43.38985974730809 L 2.55952572159283,43.39316740659723 L 2.565773504393881,43.39777837536421 L 2.567704882254739,43.39900155479599 L 2.573052553141102,43.39970124932354 L 2.576733849175424,43.39833840401714 L 2.5799821572428,43.39809363688602 L 2.586162566397542,43.39917647918538 L 2.592806506238891,43.40372955727609 L 2.594651645832472,43.409237697775794 L 2.593407479163966,43.412589325819184 L 2.58658297795051,43.41194459558017 L 2.579547372645286,43.41245750906129 L 2.577868421379267,43.41683863115452 L 2.571404144594743,43.42073922051157 L 2.565830098256781,43.42272791541909 L 2.570390844954256,43.42382401854487 L 2.574390144599156,43.42249890565258 L 2.581832686728086,43.423678525046064 L 2.590244511048581,43.42935186001671 L 2.596274901550876,43.43156468184792 L 2.601645928634627,43.4319495688947 L 2.602603532727498,43.43126655579215 L 2.603722833571511,43.42924291298435 L 2.609358863664077,43.42547074569766 L 2.617317937081376,43.42195410685743 L 2.622367367293411,43.414002748864114 L 2.624584409414619,43.41151194380832 L 2.623168664526846,43.40680412731752 L 2.621540018916737,43.40451338840267 L 2.626242699429103,43.40044988741219 L 2.629754213874727,43.4000758958703 L 2.63169367657314,43.398159560930615 L 2.631891305935647,43.393194170027314 L 2.62924037753221,43.377616508379326 L 2.630659715681119,43.37708697642342 L 2.636196931092432,43.38273268498513 L 2.638149868520108,43.38380600874919 L 2.645566359505798,43.38549299504183 L 2.649938459993608,43.38280646028026 L 2.657720565299935,43.38335226409945 L 2.667810442571166,43.388500597559826 L 2.674961930548041,43.39115358777168 L 2.674935879404802,43.3835696702331 L 2.676109977481146,43.38138708619596 L 2.680201803600311,43.3849041220333 L 2.67990086798013,43.38728830427404 L 2.682423337297938,43.38932118296993 L 2.684985332488247,43.38986758093588 L 2.692521299406725,43.38800576025649 L 2.695022209157715,43.38302843850292 L 2.700259387264131,43.38555762709683 L 2.70345828799088,43.385130009695125 L 2.706156827104376,43.383095684686445 L 2.707516876444533,43.37790575754414 L 2.711876400518365,43.37615979466087 L 2.715196573808471,43.37692178240084 L 2.721470407752761,43.37393515356259 L 2.729899300063655,43.373174434257194 L 2.740069127395171,43.37521888656234 L 2.747672467959959,43.382215602520745 L 2.751164219469332,43.382148355361025 L 2.756758028743544,43.37981359218754 L 2.759510466774087,43.37952631086477 L 2.758519625015703,43.38090263720486 L 2.763223203843352,43.39066856403299 L 2.765601942715701,43.39356885680759 L 2.773460404821179,43.39662697152389 L 2.777818132264442,43.397394576515076 L 2.776485032382809,43.40072336318662 L 2.768742452948983,43.404155085507455 L 2.764402691811402,43.406652066706364 L 2.761177739941413,43.411776886285814 L 2.759112513103222,43.413365211713156 L 2.760234508893087,43.417190333944255 L 2.771273905419632,43.42483528656528 L 2.77341279411112,43.4285624788781 L 2.773974241163695,43.432689980313086 L 2.773539456566181,43.43709246871353 L 2.776690746582872,43.43881384655042 L 2.778200814575477,43.43633123750999 L 2.787949332038743,43.43228096120445 L 2.791689916881816,43.42709328921625 L 2.792932286919753,43.42276119027036 L 2.79486366478061,43.42024856132773 L 2.80613752159631,43.41919415260679 L 2.807505655774024,43.41752702709439 L 2.808828874187533,43.4131577007321 L 2.81664152221352,43.41275507519665 L 2.826021730410296,43.41119153152975 L 2.829645534266434,43.41343372933378 L 2.839620427181297,43.40793967517716 L 2.854892685326613,43.40580561074845 L 2.861743237683309,43.40680869565617 L 2.866016523489865,43.408530282171455 L 2.866568987389599,43.410999674893745 L 2.863646767770358,43.412218019465044 L 2.86623122084277,43.415667361309325 L 2.854282729248696,43.423749640746124 L 2.867123247919901,43.435548470502 L 2.865758707003323,43.44084956213701 L 2.867559829147983,43.44859652618768 L 2.8708171203682,43.45477095078599 L 2.87941309932194,43.46398315751711 L 2.88049467092402,43.46421266230075 L 2.880264702211285,43.46588632566975 L 2.875809956717337,43.47127081291679 L 2.875531478979259,43.473109864964094 L 2.877446687165002,43.47706223585979 L 2.877674859247168,43.47957902411521 L 2.867730509051965,43.487727718642965 L 2.862946081848745,43.488737266799845 L 2.859631298450344,43.488268666493546 L 2.860566444661112,43.48936162706664 L 2.862615501824189,43.49834309772866 L 2.862346007238953,43.49972910471378 L 2.868105106525443,43.49839653159182 L 2.872604767783598,43.49830921281533 L 2.881225001250009,43.50178492389653 L 2.882021806907023,43.50760665004467 L 2.885251250353432,43.5097846848459 L 2.903764630043851,43.510472023216614 L 2.909878563867569,43.51169748576449 L 2.915911649315716,43.50983810858275 L 2.920080730549315,43.51054368837812 L 2.926772281100721,43.51432357928153 L 2.931183009145748,43.51513724768316 L 2.930785055474883,43.51788044757105 L 2.932285241999362,43.52320102688268 L 2.937648184245556,43.52726149611846 L 2.941615144540227,43.53457017297956 L 2.944242716746277,43.534839786428314 L 2.949472708330421,43.5369061316056 L 2.952582675844043,43.537339840196886 L 2.95923290389238,43.54327794780928 L 2.96410446767816,43.54702974367391 L 2.971754520637722,43.55551372316436 L 2.977107581415789,43.55698239612546 L 2.986926167471216,43.557954330666824 L 2.996080898531678,43.557773355429646 L 3.002002592884595,43.55354111406031 L 3.013951084478668,43.55362705085164 L 3.020814213249341,43.556477218330066 L 3.032173410017033,43.56054651343934 L 3.039301541796521,43.561027571755325 L 3.046588675381299,43.56250327066279 L 3.05566704964261,43.56047946441102 L 3.055098416067763,43.55163224404277 L 3.057120523772316,43.550008485055294 L 3.063269491892114,43.55439591926661 L 3.075478494918582,43.556116560457205 L 3.084025066531695,43.55627475542172 L 3.087126050892476,43.55702080508872 L 3.094689865584762,43.55887156730494 L 3.101751522033226,43.56121699961868 L 3.109227301827668,43.56139796451263 L 3.112180962481854,43.562874305283636 L 3.123339834941186,43.56687807911773 L 3.12938549680331,43.5712727460678 L 3.1323445473492,43.57019949281053 L 3.134790659867858,43.570571781734486 L 3.142266439662301,43.57452037592818 L 3.144085528112643,43.576482536348486 L 3.146042058801455,43.5814198690966 L 3.145688122579512,43.585805666676485 L 3.148635495026708,43.58601388650966 L 3.150707010071887,43.58816241285301 L 3.158221417423547,43.59107991336909 L 3.173350843438688,43.592059755813914 L 3.176824628642378,43.59257894873103 L 3.177510941519446,43.594849551841556 L 3.180750266433981,43.59685138501042 L 3.193544072710411,43.59434078804761 L 3.197898206892539,43.59432192076538 L 3.20089049510394,43.59385934520237 L 3.205917467433873,43.59186456934829 L 3.207984490902632,43.58902258270153 L 3.215612085980091,43.57676367648538 L 3.226085543877641,43.57515621401853 L 3.225971457836557,43.579155917692155 L 3.227485119090299,43.580735291447965 L 3.235788247261415,43.58513480350301 L 3.240082194319507,43.58203546174195 L 3.253968351981426,43.57609531572397 L 3.261054262942562,43.57229912016406 L 3.264307960901642,43.571142576891994 L 3.269155270174751,43.57230302516844 L 3.272806921804697,43.57511391179519 L 3.271975980166887,43.5859130301179 L 3.273049466931409,43.58734257235793 L 3.278268678732144,43.59014559987227 L 3.281555614356737,43.58930106168255 L 3.286231345410579,43.590696690437206 L 3.290885516897602,43.592873025940186 L 3.285816323749316,43.60102203636193 L 3.278849888720969,43.60792564721032 L 3.277523077046325,43.614401818742834 L 3.279841628794637,43.61671257063033 L 3.274765249124078,43.61801976780753 L 3.275209915189717,43.623062941330254 L 3.273683677521997,43.626378004061095 L 3.270217078840581,43.62907647840437 L 3.268221920594551,43.629506272410524 L 3.264493013850171,43.63291785650144 L 3.264847848387398,43.63784340792439 L 3.267597591472088,43.645847867976876 L 3.269591851402833,43.64919157411221 L 3.27028894406331,43.65579068734498 L 3.256054240071152,43.66058958697068 L 3.251164709979689,43.66340349786992 L 3.253266767744529,43.66508528279811 L 3.254701377253268,43.669041970393046 L 3.252696337539113,43.67546152934277 L 3.254324084833938,43.68047909305146 L 3.253883012029435,43.682506659668185 L 3.250850299630248,43.6844685461126 L 3.247556177483381,43.683318056908455 L 3.247730450648501,43.68863376911185 L 3.248269439818972,43.69218034672188 L 3.259476821303647,43.700452930560566 L 3.261365080030867,43.704061801869855 L 3.261515098683315,43.71082630787121 L 3.260960838153013,43.71138018819861 L 3.256593229241624,43.71378915094308 L 3.253326954868565,43.71952546885293 L 3.254611545724856,43.72320005965235 L 3.252698134169682,43.72972352608936 L 3.252607404325985,43.73205191151446 L 3.257165456077608,43.731783181380344 L 3.262920962102962,43.73598600859297 L 3.269145388706626,43.73384146766947 L 3.276869103519486,43.73497800603556 L 3.280335702200903,43.736676607118206 L 3.28241979366006,43.73904431302419 L 3.285499218454022,43.740237866114754 L 3.289594637834323,43.73917931097821 L 3.293292103543758,43.73582374274634 L 3.296056219672994,43.73744832856584 L 3.298016343622943,43.74255740586247 L 3.305412173357099,43.7528255140805 L 3.295422009082406,43.755887500242274 L 3.287683921225,43.76350058368669 L 3.284148152266706,43.76497390903624 L 3.270476691957691,43.7682195083282 L 3.26702895789724,43.76955843701606 L 3.2575562232262,43.7780726474165 L 3.253750061367385,43.78331834381929 L 3.250652670267741,43.786507823909886 L 3.243340383855009,43.791977794608314 L 3.241776416945357,43.79604541623093 L 3.244048256298894,43.803695529714815 L 3.245552036084511,43.80676269140308 L 3.245000470500061,43.81120334606762 L 3.245956277962365,43.81355578684874 L 3.248053844150784,43.81404713643743 L 3.256174614319224,43.815970361710825 L 3.263587512043779,43.81922356335211 L 3.267747610124536,43.823783192005706 L 3.271493584859314,43.825708047963566 L 3.279897324342253,43.827690519906604 L 3.286664333377524,43.829742914951524 L 3.286343634821093,43.828219340788024 L 3.288306453716895,43.82555574680719 L 3.291808086694393,43.82481303006832 L 3.2935103941578,43.82079338525979 L 3.291789222073426,43.81579794161533 L 3.297756730505832,43.817290071967996 L 3.305761618002622,43.82495561165985 L 3.305075305125554,43.827248536114894 L 3.306190114393147,43.83034300196837 L 3.30999178467554,43.83158074338898 L 3.318350608394272,43.82797761319684 L 3.326822619838804,43.82695366241034 L 3.330334134284427,43.82821804466415 L 3.344335276302714,43.84283329214035 L 3.350211156576139,43.844582605253 L 3.358235807009179,43.847764279934424 L 3.360191439382707,43.84764118836625 L 3.360636105448347,43.8487444684621 L 3.370005533861713,43.851851423592954 L 3.375785294399738,43.83052186043044 L 3.376751881645451,43.82467433673889 L 3.373394877428696,43.82425890292216 L 3.368243039274271,43.8210001423771 L 3.366169727598523,43.81195336273852 L 3.366034081990621,43.807542585019235 L 3.368172072366825,43.80457271124793 L 3.375970347348267,43.800454449428976 L 3.375504121715809,43.7921489865606 L 3.370269638555245,43.78510831293205 L 3.366006234216813,43.776998545987745 L 3.367424674050438,43.77294584464824 L 3.366278423747902,43.765432572269134 L 3.362209953826124,43.762977677458274 L 3.362965436980068,43.76170607500282 L 3.376566828696923,43.75249977545761 L 3.380417008004658,43.74923190901395 L 3.385817679492785,43.7494415094566 L 3.391353996588814,43.75200532419963 L 3.399924822714598,43.756751757875364 L 3.402714989987073,43.75936132491767 L 3.411560700589798,43.762908259042106 L 3.408010558586958,43.76576083545064 L 3.422795031532997,43.77109583269902 L 3.423037576659709,43.76953443516678 L 3.427074605546542,43.76826881049743 L 3.435464870300219,43.77008063702798 L 3.437949610376093,43.771147078541176 L 3.442379203042087,43.77747398094465 L 3.446449469594433,43.775025418088276 L 3.454623240364636,43.77241393964081 L 3.457388254809155,43.768763127368935 L 3.461418097173716,43.766237008484495 L 3.468574975042296,43.76293356109097 L 3.471454075527899,43.76350252997925 L 3.477077529206487,43.760894441141524 L 3.487261729582551,43.755393076974286 L 3.487524935960797,43.75059592103129 L 3.488683762677312,43.74870044206273 L 3.494569524418863,43.74879453606699 L 3.501553027437608,43.74287086782738 L 3.516550401105983,43.734331525406844 L 3.518487168858545,43.72832982694172 L 3.521605221209724,43.728666732502404 L 3.528769285600577,43.72202304653242 L 3.537367959500169,43.71477283699583 L 3.546381655061025,43.714677390937666 L 3.55301212017311,43.71336190801083 L 3.553822400559386,43.71956702025603 L 3.551103200194357,43.72460816024589 L 3.551878446284551,43.72861220456331 L 3.559556347017921,43.74108742257944 L 3.564302146663925,43.74367820330179 L 3.571897402391155,43.74602551477364 L 3.57844252755125,43.74896520274552 L 3.587792193028366,43.745953480872124 L 3.589619366316265,43.74628639358346 L 3.591519303142178,43.74921633495794 L 3.593154236959275,43.74981398645062 L 3.58999755705088,43.754468455569196 L 3.586595637069919,43.75436917957955 L 3.583353617209532,43.755521549554864 L 3.582888289892357,43.75874754837577 L 3.583556636463742,43.762543000067495 L 3.585459268235508,43.76471246293242 L 3.588719254401578,43.77054055847975 L 3.589406465593929,43.774900879231794 L 3.586627976420147,43.779005984379374 L 3.582601727316724,43.78277485708322 L 3.577602602760598,43.78606877788564 L 3.574038986028496,43.78743000461572 L 3.571930640056668,43.79090005219319 L 3.571788706241777,43.793339535210414 L 3.568078664118363,43.79322346424496 L 3.563723631620952,43.79066206433511 L 3.557714800685476,43.79042148164071 L 3.55495068455624,43.79286682020315 L 3.552496487200026,43.79187274475845 L 3.543731624972872,43.78605840157427 L 3.542045487184579,43.78635218019344 L 3.538213274182525,43.790973977384006 L 3.54423288490141,43.79512596213636 L 3.547722839780215,43.80361708195838 L 3.55315585061857,43.805442748500326 L 3.55754771404263,43.811004334012836 L 3.567266587101519,43.813118884214724 L 3.570015431870925,43.81643706022362 L 3.570708931270265,43.82246038464063 L 3.572543291080437,43.82405410141193 L 3.561131992026267,43.82863150675554 L 3.563807174942375,43.8333336264557 L 3.570497827178497,43.834864845945084 L 3.576599184588237,43.83835416887766 L 3.578447019127671,43.84366519410607 L 3.580951522139796,43.8461044659347 L 3.590472765836178,43.84884099640158 L 3.600533897018317,43.849445426189035 L 3.599631988473061,43.85277518679257 L 3.610756724951597,43.8546459901686 L 3.612137435543289,43.85701226508179 L 3.622131193079119,43.86219532919656 L 3.631103566136904,43.86493630266732 L 3.63948484773774,43.87039196738521 L 3.642612781557044,43.87153886281302 L 3.650809908524634,43.87336374747575 L 3.657636206368659,43.87754238039731 L 3.666124387488304,43.88036034767686 L 3.669964685327915,43.88051639390052 L 3.666167506621942,43.89505668370462 L 3.673195027089609,43.89732102133598 L 3.680474974152113,43.902372423661475 L 3.686856605930499,43.90392776019509 L 3.692593247334886,43.9065839713257 L 3.698102614972391,43.91085217150203 L 3.701679706433755,43.91281757637597 L 3.708400003074253,43.91166112111078 L 3.712831392370814,43.90937079447569 L 3.721007858086871,43.91130647895587 L 3.721059062058065,43.90724282796271 L 3.71878362944339,43.90628431211253 L 3.716633960968492,43.90095232643733 L 3.712583457352398,43.895790757995016 L 3.715227199233562,43.894343315362924 L 3.720727583718225,43.89312241287416 L 3.723088356284891,43.89129165958617 L 3.726174069285841,43.885964158119044 L 3.725856963990547,43.88156467723116 L 3.725229041606948,43.879432479326056 L 3.734114278082174,43.87709040541242 L 3.739475423697799,43.87677117173116 L 3.745276743802643,43.87739021155169 L 3.75340739543921,43.879243407380486 L 3.758795490513358,43.87791794439572 L 3.762075239615679,43.87775088346114 L 3.766255100632687,43.87860949397953 L 3.760801428542797,43.874291710330425 L 3.762110273911759,43.87065230357443 L 3.761600929145663,43.86926706883869 L 3.748852038633439,43.858454137450806 L 3.744223019974371,43.853733916075754 L 3.729194205271051,43.847252476483334 L 3.728209651719657,43.83947511511271 L 3.719721470600011,43.83534241250033 L 3.721389642082621,43.831108979005755 L 3.724460083723742,43.82676053554627 L 3.720718600565384,43.8267916432713 L 3.716651028958891,43.82785253660584 L 3.708871618598415,43.825376225370746 L 3.698831148667812,43.823119523576466 L 3.69756811737834,43.82328284891341 L 3.689355719050919,43.827093646592374 L 3.686859300876351,43.82783892722002 L 3.682923781616624,43.826337339260036 L 3.680351904958189,43.82364190383717 L 3.678598393523588,43.818168998374226 L 3.674955725046483,43.81481137796132 L 3.671302276785969,43.813335391260765 L 3.664324163658928,43.8061474564198 L 3.666928379667591,43.801204601072484 L 3.668119545734333,43.791105615874585 L 3.666406458487518,43.78872310887241 L 3.677542873064747,43.782121109928696 L 3.674679043938974,43.77752522132143 L 3.674799418187046,43.77321893062509 L 3.6773362605494,43.76910888726693 L 3.687480036737678,43.76705635457336 L 3.683077393530207,43.763279354686304 L 3.679001737086157,43.75946708070495 L 3.667059533699072,43.75507838287174 L 3.673595675706326,43.75022344890348 L 3.676666117347447,43.74661151707591 L 3.673665744298487,43.742886443534374 L 3.676245705794479,43.73573157554965 L 3.676389436239938,43.732598455555106 L 3.669534392306822,43.73314434551469 L 3.665910588450684,43.72782803537772 L 3.662475430804211,43.72744049102115 L 3.660727309261314,43.72585263653724 L 3.66106866906928,43.72288259948317 L 3.663983702166247,43.71960922087032 L 3.675050048151316,43.71836851040136 L 3.691788356840315,43.715538997124725 L 3.694758187169614,43.70749707407267 L 3.703666779842227,43.70749967155751 L 3.704597434476575,43.708573072531244 L 3.705473291878592,43.7140696492058 L 3.702623835797364,43.721488741164 L 3.70475553796658,43.72543067303385 L 3.708528462159882,43.72699451978932 L 3.712171130636987,43.72418943392821 L 3.716853149897817,43.72380381361427 L 3.716089581906316,43.72800784991434 L 3.714125864695231,43.7320415258312 L 3.716295296106379,43.73546805447225 L 3.719189667951812,43.737800110820345 L 3.730040418268692,43.729866983732265 L 3.732508988669453,43.72529369654986 L 3.740043158957363,43.72153938027332 L 3.749914745614552,43.7273139056378 L 3.752704912887027,43.72560140631709 L 3.763778445394369,43.72723730512206 L 3.769640850938533,43.730844563769864 L 3.768490109059576,43.7338563966399 L 3.769445018206595,43.738689941946745 L 3.770914662011414,43.74178055826683 L 3.774371379224706,43.74484375846238 L 3.7652418009922,43.74961931235266 L 3.762929537450877,43.7528307051199 L 3.758374180645106,43.754031769756246 L 3.749540148141075,43.75543135923674 L 3.748496305780928,43.756618746064376 L 3.758257399658171,43.75669595783456 L 3.760355864161874,43.75896944294738 L 3.765266953820155,43.76229841155011 L 3.778071539879995,43.76614683431807 L 3.778562020025124,43.76943323807475 L 3.781709716780679,43.77332206728481 L 3.785179010407949,43.776439434830664 L 3.782905374423842,43.78094071501547 L 3.784221406315077,43.7865045813366 L 3.791412420164454,43.78830223698706 L 3.796953228836903,43.78775231055083 L 3.802331442442926,43.786467615988634 L 3.806446624759479,43.78789692612991 L 3.808696006230913,43.79066984596943 L 3.814640158465932,43.79325912853208 L 3.816358635604452,43.7946811435828 L 3.813509179523226,43.798141692731384 L 3.815043502028502,43.79985146850541 L 3.821178097103754,43.79850543752222 L 3.821460168102968,43.79377982470027 L 3.824856698192224,43.79353925455625 L 3.833935072453536,43.79403725313486 L 3.838255070654866,43.79592092115906 L 3.840428993642435,43.799750322722225 L 3.841422530346672,43.80780254729516 L 3.844329478606082,43.810874035858795 L 3.851108165740049,43.81154302634909 L 3.855311382954443,43.81525475024022 L 3.855814439513551,43.8185403992281 L 3.847868840825514,43.821689759685526 L 3.841691126616623,43.8258655334315 L 3.837787946707123,43.83128589108647 L 3.83509839074647,43.83958915239343 L 3.838937790270797,43.84163465909685 L 3.83868536367596,43.84280284068008 L 3.841250952127405,43.84819185919751 L 3.843945897979763,43.850814278294074 L 3.842790664524385,43.85862643433255 L 3.83964925597582,43.861390241447715 L 3.836238352842018,43.862633168009964 L 3.830716408790535,43.866431738127076 L 3.837975694601505,43.8707196540452 L 3.848481491849283,43.868406380830464 L 3.851455813755003,43.86989007226878 L 3.852338857679292,43.8732789155379 L 3.856371394989704,43.876575615974176 L 3.866276219312407,43.87698097255815 L 3.876311299351306,43.878476105826884 L 3.885572929930578,43.87835502020646 L 3.897706474473181,43.877664762718865 L 3.903287707333415,43.879243407380486 L 3.910748215768027,43.886585044711076 L 3.913791707950624,43.88847291455052 L 3.917392155609375,43.88557958069646 L 3.919568773542797,43.881308921109124 L 3.924904766330468,43.88849881084012 L 3.932151475727459,43.8945770059333 L 3.947347377073625,43.89259740416121 L 3.948612204993665,43.89147227742337 L 3.956424853019653,43.8991936631915 L 3.956983605126375,43.901234536239826 L 3.972519069649938,43.90048369986539 L 3.978856683979402,43.89889137760063 L 3.982745490844354,43.89685883934511 L 3.989724502286679,43.89751715630672 L 3.997874018544211,43.899000122914984 L 4.0014609914737,43.89889202489475 L 4.007919878366519,43.89655783653975 L 4.012662983066671,43.89749903165004 L 4.017917229163486,43.89220704326492 L 4.017631564903136,43.893686258014235 L 4.020931975256991,43.899798877053456 L 4.031525807402613,43.90100216633916 L 4.034907964447323,43.901850732810715 L 4.040606876609777,43.90582219969948 L 4.041716295985665,43.90618528832602 L 4.046306687087516,43.90212711302438 L 4.052490689503395,43.90456593479747 L 4.059593668454927,43.90938567961827 L 4.063860666054495,43.91084764135326 L 4.064290060760305,43.913525546293535 L 4.070768710589374,43.92080213461665 L 4.070618691936926,43.92495026526361 L 4.066366965697188,43.92623133278875 L 4.067048786997836,43.92883285016695 L 4.078657715414511,43.92704719020112 L 4.088320892925785,43.927499433065854 L 4.102055235304689,43.935499487289036 L 4.100613439273677,43.94181156359405 L 4.100521811114697,43.94742317280509 L 4.113338075273231,43.9424719706485 L 4.117331985026425,43.940540534268585 L 4.124062163135049,43.946115396976644 L 4.131655622231711,43.942009492433115 L 4.137085938124214,43.93630292671595 L 4.141173272666958,43.93286720913357 L 4.143861031997043,43.92825898630619 L 4.149213194459827,43.92095548666322 L 4.152086006738442,43.92058989990859 L 4.156789585566091,43.92142201092303 L 4.161282958617257,43.920448841055766 L 4.158647301573651,43.91796213856823 L 4.154258133095443,43.90997137466468 L 4.148549339464863,43.904787935109 L 4.169352524814503,43.89729189232483 L 4.191350469492021,43.89512983260686 L 4.198477702956226,43.89405654147077 L 4.199886261321725,43.893687552715654 L 4.208787667472065,43.88702723632555 L 4.205173745084053,43.882573446368845 L 4.201679298628829,43.880673734699776 L 4.199273610297956,43.881272661924484 L 4.195777367212163,43.87832976702467 L 4.191780762513115,43.873549600230504 L 4.189469397287075,43.865628002900394 L 4.187521849751104,43.862309970735076 L 4.186290259496576,43.858210589129584 L 4.182548776338219,43.84844322254582 L 4.177575702925333,43.84386020994449 L 4.160731393032808,43.832569622014354 L 4.158440689058303,43.82956016587102 L 4.157608849105209,43.820549683161424 L 4.171364751050931,43.81948477109853 L 4.180768315445094,43.819312361150935 L 4.19470388044764,43.81517372485028 L 4.205862752906972,43.81049351176172 L 4.208585546533138,43.80736171284312 L 4.214611445459013,43.79987286470683 L 4.210797198762641,43.79605384557538 L 4.222904692162004,43.796772929120614 L 4.233639559807233,43.798170221814615 L 4.237564299283551,43.798017202026976 L 4.235955416609693,43.80537078613824 L 4.230971563413397,43.813155184852064 L 4.227569643432437,43.81495333523343 L 4.236505185563574,43.81651289838743 L 4.239701391344471,43.81905374529608 L 4.240846743331724,43.81714423115797 L 4.24906273492028,43.818252612380526 L 4.251317506283421,43.81902004089164 L 4.257371252983102,43.81787926521404 L 4.25952541303442,43.814172241953486 L 4.263565136867106,43.81250306646911 L 4.26992251413282,43.81540642947466 L 4.280705890803391,43.817925285563824 L 4.284232676608843,43.816660685301706 L 4.273832880564592,43.81042414850493 L 4.272874378156437,43.80833154434301 L 4.275916073708466,43.79960897768711 L 4.275670833635901,43.79858065007258 L 4.273878694644082,43.79483935930779 L 4.270262077310218,43.791861720998064 L 4.265058136869313,43.79021980645262 L 4.261978712075351,43.7902548240582 L 4.259114882949578,43.78504799962798 L 4.255792913028904,43.78390073820267 L 4.252824879330174,43.78265098292333 L 4.241429749951116,43.773043792493574 L 4.238997112161721,43.776811743769706 L 4.235879059810543,43.77995292707551 L 4.232714295064589,43.78016566282359 L 4.225340923212537,43.77901571331894 L 4.21666050262209,43.77864341810277 L 4.2169201157392,43.77561567684908 L 4.215699305268082,43.772023440514175 L 4.213662824518982,43.7693936673704 L 4.217366578435407,43.768157880559514 L 4.224565677122341,43.766734585792825 L 4.231002106133057,43.7638288574699 L 4.23415159951918,43.757934576730676 L 4.218109485175374,43.75191188413289 L 4.209640168676695,43.75134669839705 L 4.207671061573905,43.75047846926909 L 4.201974844357304,43.75020073711338 L 4.199669767338253,43.75137589870933 L 4.198815469503055,43.753620386744124 L 4.194133450242224,43.75701064343045 L 4.185577895476269,43.762397025608465 L 4.176245297989552,43.76201035376457 L 4.178919582590376,43.76492265790494 L 4.180254479102577,43.769010284272724 L 4.178384186681041,43.775430167580645 L 4.1771759526239,43.77705886741266 L 4.169623816030307,43.78354922690395 L 4.169366897859049,43.786637526697355 L 4.163385016382097,43.78924708698571 L 4.148347218525937,43.79200697508939 L 4.147080593975328,43.793420590220215 L 4.146718572915828,43.791859127171804 L 4.138708295527334,43.78381837319447 L 4.137142531987113,43.778049946197 L 4.136260386378108,43.774974175569355 L 4.136776019351193,43.77273567786917 L 4.143364263644925,43.76783741513878 L 4.136689781083917,43.76656980836813 L 4.13371815412405,43.763855456650134 L 4.126649311153313,43.75950795559105 L 4.121705882144803,43.75055698677102 L 4.121998732927427,43.74653948387957 L 4.111605225090163,43.74010481760108 L 4.115879409212003,43.7370089223671 L 4.122389500076019,43.73710303474897 L 4.126835262417126,43.73602949576515 L 4.127345505498506,43.730555056338055 L 4.128049784681256,43.728747875177014 L 4.127532355077602,43.722141203333784 L 4.124850883954505,43.713080107814 L 4.132441648105315,43.70891463468971 L 4.148113656552066,43.71284245961347 L 4.153772144526734,43.71487023077601 L 4.159466565112767,43.710591897422034 L 4.163642832868639,43.70666587314428 L 4.164426163796391,43.69875525068008 L 4.164228534433885,43.6928837913123 L 4.163649121075628,43.69015440618359 L 4.15882157473877,43.6870864602092 L 4.155055837067741,43.68231761357877 L 4.15500553141183,43.679753414141736 L 4.157983446578687,43.67321281259231 L 4.159294986893501,43.66905106741988 L 4.15970911023948,43.66232798860553 L 4.154517746212553,43.65690270919598 L 4.145440270266525,43.65185392551032 L 4.135751939927297,43.64356228833991 L 4.133830443534564,43.6411557052187 L 4.133427099971995,43.64271459961073 L 4.134752115016071,43.64604872960709 L 4.134757504907776,43.649975472060696 L 4.133320200453184,43.651788278127924 L 4.129114288292937,43.65354383342695 L 4.125091632450649,43.65764491220396 L 4.123518682388157,43.66173856236425 L 4.11923012522177,43.66608861272701 L 4.116370787672418,43.66677221910925 L 4.111660022322495,43.669869794189715 L 4.109391776230093,43.672099143569646 L 4.102993974776594,43.67001339511481 L 4.100699677540952,43.66876905894661 L 4.099301000643578,43.66597554400093 L 4.101711180550872,43.662847224788926 L 4.096569223864571,43.65986691697485 L 4.089229089678031,43.65772550112029 L 4.088100805681176,43.659284615398334 L 4.081081370051066,43.66345353620047 L 4.077974995798582,43.66262432419899 L 4.077312937434185,43.671104361058234 L 4.075775021667772,43.6752932520984 L 4.068650483149421,43.67450513656169 L 4.067339841149891,43.67908294919265 L 4.061914915149092,43.682935421786986 L 4.054910750878812,43.68543777229337 L 4.035610446999504,43.67970728743272 L 4.031705470459437,43.678545662280385 L 4.024375217741022,43.67622819189676 L 4.021856341684351,43.674787767808496 L 4.023160695476892,43.671372713549175 L 4.015989444563766,43.67397365626721 L 3.993164151509573,43.667076330473066 L 3.991334283275822,43.66563958566747 L 3.99036140782312,43.66184059135601 L 3.989075918651545,43.66150136053494 L 3.983695908414953,43.66429638337026 L 3.978553053413369,43.666185435777194 L 3.970774541368178,43.66759097696344 L 3.963042741717762,43.6758922909495 L 3.953948197781336,43.681153440338704 L 3.946392467926606,43.67691168279619 L 3.944797958297294,43.68111770921788 L 3.943830472736297,43.687281987173 L 3.940510299446191,43.693973691934794 L 3.948608611732528,43.69547535808015 L 3.949314687545847,43.69707505880407 L 3.94870473146793,43.70101599892583 L 3.940770810878586,43.707744483998 L 3.938825059973183,43.708496448053474 L 3.934830251904704,43.706818477303536 L 3.934822167067147,43.70407608885379 L 3.930105113510235,43.699661248793596 L 3.926437292205175,43.69777000278464 L 3.920195797611113,43.69357618577842 L 3.91070958821081,43.69241288056499 L 3.908328154392609,43.6910689783886 L 3.907678672442191,43.688305082986126 L 3.90925521576582,43.68556574535735 L 3.907183700720641,43.682381278651356 L 3.900846086391178,43.678130512266044 L 3.893129558100592,43.67702538062212 L 3.888603845699197,43.67544008863903 L 3.885027652553117,43.681477618080656 L 3.883329836666131,43.6876561499515 L 3.883263361335107,43.68735863876564 L 3.87951109839334,43.687681483805264 L 3.865325801741808,43.6835135961556 L 3.862495210281547,43.68196420623314 L 3.861883457573063,43.6791050382735 L 3.860349135067786,43.67779852012503 L 3.852471808341342,43.67631395353942 L 3.844375292685573,43.67288664095789 L 3.842017215064759,43.67085744971611 L 3.841073984016433,43.66776772322503 L 3.84267388353745,43.66421580327347 L 3.845404762001174,43.6614149278546 L 3.849802015316938,43.658754950019535 L 3.852673929280269,43.6557250442659 L 3.856826840838753,43.653294899978015 L 3.863342321594472,43.65067681155361 L 3.868619923888674,43.6477297006086 L 3.87120078369995,43.648085259599576 L 3.877888740990219,43.647941606220414 L 3.885797508751608,43.64553779870477 L 3.893976669413517,43.64851621767567 L 3.899100659794134,43.652167862000425 L 3.901550365573927,43.65463704852421 L 3.911801041281016,43.654187285864026 L 3.922809895087901,43.655565810756116 L 3.926228883059259,43.65396175331983 L 3.928557316275697,43.65129364554338 L 3.931694233247843,43.64073249546478 L 3.931993372237454,43.63830824574271 L 3.934691911350949,43.63320263406591 L 3.936770612918401,43.63121046317026 L 3.944152967923295,43.63654249304154 L 3.947391394522547,43.64008304797123 L 3.949772828340748,43.643831414008496 L 3.956343106328798,43.64436641145506 L 3.965507718857386,43.64723048443303 L 3.971811197206052,43.64981102333908 L 3.976872305516782,43.65245644813375 L 3.986124054627928,43.65448301169183 L 3.988153348854754,43.652003419281705 L 3.995218598564354,43.64939567456318 L 4.00196674297866,43.64806120905778 L 4.00701707150598,43.643257407804704 L 4.010202497503468,43.64006939584649 L 4.013440924102719,43.63899606909177 L 4.015932850700866,43.63836740666162 L 4.024825273698365,43.63371887123324 L 4.030747866366565,43.62768889251141 L 4.032226493324226,43.626932013977445 L 4.035706566734905,43.621299341901 L 4.039723832685488,43.62238664088011 L 4.047803280350859,43.62331069949489 L 4.050308681678269,43.61678150810668 L 4.051208793592956,43.61012607576226 L 4.052608368805614,43.60486910993301 L 4.059064560752581,43.59869832118731 L 4.064621539100144,43.60127899263555 L 4.072018267149585,43.60227818432159 L 4.077771078229087,43.60713209425525 L 4.082481843579009,43.60976248700123 L 4.083403515060515,43.61051047580089 L 4.088501454297893,43.60962784802939 L 4.092691196783028,43.60792629765926 L 4.093314627590206,43.60621103933762 L 4.09043912036574,43.601293304091946 L 4.086046358626395,43.59752862320979 L 4.088425097498743,43.59800092891221 L 4.086109240696283,43.593405223961675 L 4.074695246696261,43.58855606245597 L 4.052134058335599,43.56961046455006 L 4.045535932573741,43.5689426752676 L 4.035271782137391,43.56325445066553 L 4.03746546806121,43.55635157422587 L 4.03104970030203,43.55573571902447 L 4.013714910264376,43.55321754792086 L 4.007029647919958,43.55194735499258 L 3.993358187610943,43.5478201825113 L 3.984379526346168,43.54542278993043 L 3.976522860871259,43.54420517489666 L 3.971484210442632,43.54093963134118 L 3.969349813327565,43.540403713678565 L 3.970442164713054,43.54754997600228 L 3.972644833789715,43.54843546751238 L 3.968696738116009,43.565414854635485 L 3.96477828684668,43.56807570971517 L 3.959247359642357,43.57336062120008 L 3.951671866851376,43.57773203802084 L 3.941772432420379,43.58117258909493 L 3.938610362620278,43.58386072851416 L 3.936449016046687,43.58497278014141 L 3.935031474528346,43.5877479393317 L 3.93528749438432,43.590584130436724 L 3.937046395710627,43.59178324146745 L 3.93886817910682,43.59568945810481 L 3.937627605699451,43.59504602980249 L 3.929856280176533,43.59385739339922 L 3.9236471249327,43.59343189879977 L 3.915114926364132,43.59161538037388 L 3.908269763899141,43.59147419451795 L 3.900753559916914,43.58942468511632 L 3.892546551481197,43.58563128201201 L 3.886047240400593,43.58215975082523 L 3.877506956994468,43.578867630331004 L 3.871172935926142,43.57575885477298 L 3.866640935317759,43.572800911152356 L 3.860590781879214,43.56673813703222 L 3.857704494871338,43.57188063239407 L 3.849222601958682,43.58278900104181 L 3.842258861876187,43.58778567801218 L 3.836753087499818,43.59458801395785 L 3.830668798080477,43.59323541556877 L 3.820089338979401,43.59253470690264 L 3.816256227662063,43.59122435329768 L 3.817177899143569,43.58886187111446 L 3.815940020682053,43.58394466993307 L 3.814211662075407,43.572492417576804 L 3.805187186731142,43.56729269631833 L 3.796486104889161,43.56877475048188 L 3.793852244476122,43.569633244755614 L 3.790269763123054,43.57389494419455 L 3.785358673464772,43.57784527282856 L 3.782449030259509,43.57914550553157 L 3.778097591023234,43.57850320189469 L 3.775466425556048,43.57614412518353 L 3.766016148767111,43.57299355612796 L 3.765628974879656,43.56904095661104 L 3.763773055502665,43.56457452593202 L 3.759957910491008,43.55727143924784 L 3.757506408080647,43.557430933169954 L 3.751883852717342,43.54454050983315 L 3.743414536218663,43.53804705201988 L 3.741892790127365,43.535699415462496 L 3.742444355711815,43.532829378870275 L 3.738321088557706,43.53158089962827 L 3.731616063277038,43.53518754692264 L 3.730045808160397,43.53432856183591 L 3.731342975430666,43.53122204661352 L 3.731017785297814,43.527203528557806 L 3.723671362904285,43.521138771832135 L 3.723307545214216,43.519776706864505 L 3.715618864697437,43.517092873379084 L 3.712033688398517,43.517362564954524 L 3.704800453730786,43.50829401321846 L 3.702951720876068,43.501129415143986 L 3.698519433264222,43.4954158923936 L 3.695607095113107,43.491396292876445 L 3.694394369479545,43.4874285658838 L 3.698531111362916,43.48140543883595 L 3.698624536152465,43.479415413249875 L 3.692011139030776,43.47466985354645 L 3.689622518690303,43.47049501735504 L 3.683128597501403,43.46601933004265 L 3.67407627438333,43.46298297592979 L 3.666309440436832,43.45776402861608 L 3.665322191939586,43.459022511651874 L 3.658717777970739,43.46341395581349 L 3.652101685903198,43.46523238324642 L 3.65070839889753,43.46376147593294 L 3.644814552318421,43.465474923036325 L 3.642140267717597,43.46834555528348 L 3.632292037257795,43.48145758405585 L 3.625664267091561,43.48666666469217 L 3.624687798377723,43.48672010888729 L 3.61947756972983,43.48209766296314 L 3.61064174059523,43.47595275589393 L 3.606725984271753,43.4737937094628 L 3.600435980652348,43.47381782967084 L 3.597431116026968,43.4725290147984 L 3.595457517347758,43.462439845619 L 3.594087586539476,43.45894230859219 L 3.593803718909694,43.45863649269739 L 3.60010270568194,43.44516080027296 L 3.607635079339282,43.437227493299794 L 3.615585169603739,43.430851655817584 L 3.642998158813931,43.41493652964934 L 3.611980230368568,43.398919314050325 L 3.593729158741112,43.389352517753444 L 3.56740852091641,43.39756754738576 L 3.551610748329884,43.40497480631347 L 3.550003662286594,43.40631074471701 L 3.550960368064181,43.40425624592026 L 3.548067792849316,43.40083758301337 L 3.545232709812635,43.399572015308365 L 3.528596809066026,43.39570596324682 L 3.528625555155118,43.3930492557036 L 3.532455073211319,43.3916151074292 L 3.529826602689985,43.3879058783583 L 3.524858919168805,43.388425523558716 L 3.52044639449321,43.3858324757732 L 3.51452829340143,43.37842940602088 L 3.508283205546231,43.37497141420732 L 3.502074050302397,43.37053371739703 L 3.501332940192998,43.370099463878276 L 3.501523383033232,43.364461069035386 L 3.499020676651675,43.35727876877984 L 3.49682160083615,43.35454852712234 L 3.492869013586025,43.352758780531865 L 3.48537975906232,43.352415848608466 L 3.474773350502721,43.34771715950546 L 3.466688512945645,43.34744997718101 L 3.462797909450123,43.3439862993863 L 3.456230326407925,43.34112022117309 L 3.445610443119064,43.35018119142199 L 3.445610443119064,43.35301875429864 L 3.447432226515259,43.3559286861651 L 3.449009668154173,43.361259606741626 L 3.445019351662114,43.36336454477162 L 3.44506875900274,43.369857847792304 L 3.438624245154467,43.37586792535536 L 3.437680115790857,43.37817215284555 L 3.438316123012014,43.38269938814111 L 3.437080041181066,43.38366433632392 L 3.432993604953606,43.38549038364422 L 3.425527706627288,43.383151179053556 L 3.414194561002837,43.38173181303984 L 3.40904451947898,43.380284991772385 L 3.40677896833243,43.37774383041646 L 3.402247866039331,43.376189177399034 L 3.391904663857979,43.37591493795585 L 3.377958319072023,43.37617546545633 L 3.371567704140797,43.3744875669229 L 3.36813164817904,43.36549618542797 L 3.369148541080663,43.365250632601 L 3.37234025528514,43.36069923550326 L 3.376552455652376,43.35704624580696 L 3.38314968309895,43.35323365639571 L 3.387422968905507,43.349887237407955 L 3.390060422579682,43.34685420209053 L 3.388829730640438,43.34301157358618 L 3.389039936416922,43.340364970380236 L 3.387098677087939,43.33788485931191 L 3.382738254698824,43.334551968865334 L 3.378306865402262,43.333015811331705 L 3.370809526041,43.334759095598116 L 3.361292773921038,43.339126234206155 L 3.354094573549388,43.3369890935586 L 3.345727664993099,43.33229509110899 L 3.339116962817263,43.33541379437907 L 3.335476090970727,43.339754752237695 L 3.331240534406104,43.34124761981849 L 3.316995050630537,43.347288622004726 L 3.311502750983429,43.34668043325433 L 3.306041892371267,43.34266466672711 L 3.301447009692996,43.34354336292652 L 3.29321395011404,43.343848453725286 L 3.286620315928603,43.34074063695759 L 3.277897674519803,43.33570520493961 L 3.267602981363793,43.330774559010855 L 3.262929046940518,43.327311890283795 L 3.259945741881958,43.32282692138876 L 3.254301626951835,43.31776778232547 L 3.252426842953878,43.31285071377026 L 3.25495021058697,43.30980733072329 L 3.261458504820415,43.309283743662355 L 3.264875696161206,43.30983870658369 L 3.273147383297378,43.30836271627672 L 3.272743141419524,43.302527693341325 L 3.276065111340198,43.30053505442615 L 3.281679581865946,43.29911833211651 L 3.284829075252069,43.30009310860046 L 3.287870770804097,43.302729699349086 L 3.289943184164561,43.30602642097703 L 3.293429545782229,43.304941261675246 L 3.295830742536681,43.301869371370024 L 3.298796979604843,43.29476858570666 L 3.301120022929576,43.292872461789834 L 3.305557700433126,43.29142156015045 L 3.310786793701986,43.291724297329424 L 3.315017858690189,43.29506738329025 L 3.317739754001071,43.29636324553475 L 3.324817580124649,43.29570289536933 L 3.330547035006763,43.29229772841695 L 3.33258082081001,43.28826071092208 L 3.333119809980482,43.28543058490085 L 3.341165121665056,43.276216723385204 L 3.344943435750063,43.269397475971424 L 3.3414624640241,43.26804674787834 L 3.32697443512182,43.26140975688089 L 3.312357048818627,43.25446192438996 L 3.307517824383075,43.260120346188025 L 3.293973026529121,43.253751395144974 L 3.290134525320078,43.25290607699479 L 3.277456601715299,43.24782476107282 L 3.27592587247116,43.23988787375551 L 3.269041184133668,43.23601361144392 L 3.271444177518688,43.23384273888598 L 3.253921639586653,43.222542088583346 L 3.241120646787949,43.214017924389395 L 3.24005883812212,43.21277726817522 L 3.230940039673022,43.20808546087421 L 3.205848297156996,43.18852735753686 L 3.187868516745344,43.17358704724092 L 3.183839572696068,43.16940064176617 L 3.181167983041096,43.17124557819824 L 3.179778289296563,43.17063562846352 L 3.177370804335123,43.1657276372128 L 3.16038725557356,43.149959251228736 L 3.150752824151378,43.1396335150413 L 3.151715818135954,43.13839790165921 L 3.149665862657593,43.1385231028788 L 3.14478711234954,43.13338767598448 L 3.137374214624986,43.12489367483887 L 3.130935988983701,43.11612964005839 L 3.131632183328894,43.11124284726137 L 3.123014644808336,43.108948831687215 L 3.118284116522161,43.10338525730388 L 3.102051559338122,43.082055615599984 L 3.089838963050517,43.06430860148307 L 3.085101248242071,43.056885419937124 L 3.081895160993048,43.05101252813698 L 3.077753927533257,43.044502262377115 L 3.070120942564093,43.03067591373083 L 3.065754231967988,43.021969979887935 L 3.064978985877794,43.01827498622664 L 3.064817289126652,43.013048022864695 L 3.061107247003239,43.008476856767786 L 3.05578023736841,42.99646237599368 L 3.051598579720833,42.98605423400637 L 3.049814525566572,42.980812699350096 L 3.045568189218538,42.96740716374965 L 3.043534403415292,42.959821054637004 L 3.041657822786766,42.95207489567353 L 3.039035640472422,42.939035320778096 L 3.038928740953611,42.93531166344069 L 3.040046245167056,42.930113818925896 L 3.040940967190039,42.92953501117075 L 3.054213575512905,42.92597061944875 L 3.059516330635062,42.920279925742804 L 3.060531426906118,42.91711959094901 L 3.056413549643714,42.903984439886955 L 3.054884617030142,42.89749412635917 L 3.052483420275691,42.889571635685435 L 3.050513414857617,42.87910788702848 L 3.04924050210002,42.87728574824199 L 3.04586283663173,42.87656293406355 L 3.040240281268426,42.87175514855325 L 3.031319112181835,42.860824156888036 L 3.031746710257076,42.85895209135717 L 3.035705585714191,42.856245628255216 L 3.037432147690269,42.857145161756335 L 3.037097974404576,42.86250653105393 L 3.037913644682556,42.86531148349711 L 3.040708303531452,42.868972215628716 L 3.044547703055779,42.86991304139035 L 3.048324220510218,42.86946336928497 L 3.049493827010141,42.86761526812247 L 3.046573404021468,42.85638128348754 L 3.045554714489277,42.8512643724892 L 3.043840728927177,42.83813759033789 L 3.012587441877375,42.85233322946874 L 3.008223426227122,42.84001555637951 L 3.001945999021694,42.82393341829128 L 2.992882896120213,42.79911094128454 L 2.990438580232123,42.796647063459524 L 2.974308430990474,42.77765648918642 L 2.976881205964192,42.771116624094546 L 2.973466709569254,42.769427145613015 L 2.98169348094122,42.76226250625613 L 2.979370437616487,42.76137280386251 L 2.9748725729889,42.755908339592104 L 2.972218051324327,42.751845790086655 L 2.972263865403818,42.74855413157 L 2.974338075394849,42.73924682942145 L 2.972828007402245,42.73881072152802 L 2.967937578995498,42.73480907399254 L 2.960110557924965,42.73323341627745 L 2.958204332892063,42.73352374147806 L 2.958999341918509,42.73876057882022 L 2.956913453828783,42.74016587882685 L 2.950215615070388,42.74036578511337 L 2.947692247437296,42.73970800523361 L 2.9414696174642,42.73644869204963 L 2.934060313000782,42.7382274801564 L 2.92985350252525,42.735833098079056 L 2.924566917078207,42.73053398667847 L 2.926244071713659,42.72082338285534 L 2.928192517564914,42.713704607521414 L 2.935926113845899,42.715264260740106 L 2.938726162586499,42.718275871202785 L 2.945138337084544,42.719161572347744 L 2.947740756462639,42.720930296860544 L 2.947743451408491,42.72313915178231 L 2.951788565132881,42.723721875121655 L 2.957948313036089,42.72252738537712 L 2.957383272722378,42.71808579360148 L 2.967146163230189,42.71585101813906 L 2.969417104268443,42.71399304401619 L 2.974649790798439,42.7128122280001 L 2.982350149413912,42.713643883878 L 2.986705181911323,42.71465175665961 L 2.993192814893234,42.71337524704708 L 2.997095096487449,42.71084327128761 L 3.002304426820058,42.70962147022953 L 3.005236527907425,42.71113898181312 L 3.008336613952921,42.71468673819766 L 3.011351360046426,42.71538504477514 L 3.020551905186378,42.712801667218876 L 3.02513600808124,42.70973566443212 L 3.027738427459334,42.709040594732784 L 3.031140347440295,42.71030333191655 L 3.03287948583035,42.712443259640914 L 3.030919361880402,42.71617970546123 L 3.031266111580072,42.719463183041256 L 3.035128867301786,42.7217585441776 L 3.040060618211602,42.72291279231606 L 3.040543911834458,42.72100751241641 L 3.038427481025073,42.704589455828255 L 3.025399214459487,42.706002131251815 L 3.024456881726446,42.70384547785773 L 3.030096505080148,42.703057920921864 L 3.038750874527356,42.700992269390206 L 3.037060245162643,42.69452024185079 L 3.034957289082519,42.671301749672025 L 3.034728218685069,42.65889584787347 L 3.035335479817133,42.65324595284505 L 3.03585380773607,42.640745916571916 L 3.036125997267158,42.631510346043456 L 3.037141991853498,42.62023018684923 L 3.036899446726785,42.615030287849315 L 3.034417401596763,42.61094727617071 L 3.035362429275657,42.60703325723764 L 3.038975453348386,42.607129127193616 L 3.038124748774325,42.61393353364256 L 3.040424435901671,42.61728389515396 L 3.041531160331706,42.60543914965703 L 3.043559556243247,42.599816801325744 L 3.044414752393729,42.59369134885314 L 3.045569087533823,42.58595746067575 L 3.045631969603711,42.579995349622614 L 3.046409012324475,42.56868402746534 L 3.047518431700362,42.560174873160676 L 3.050561923882959,42.54724252140765 L 3.049930408238223,42.54246343108027 L 3.051254524967015,42.540346774808825 L 3.057875108610976,42.53646208041806 L 3.061117128471363,42.53538313962366 L 3.067699982873391,42.535871643961386 L 3.070415589977284,42.535475148185306 L 3.07577224401649,42.53322321624529 L 3.077587739205695,42.533592586310476 L 3.081220526214675,42.53216738833128 L 3.085339301792362,42.52893425943742 L 3.086630180855642,42.52549242175193 L 3.088498676646611,42.52457284501894 L 3.092329093018097,42.52494822462913 L 3.099046694712742,42.52363935155358 L 3.103755663432097,42.52562615316598 L 3.108942535882603,42.525097184166796 L 3.11383745586577,42.52177232277826 L 3.111549446837118,42.519699325822785 L 3.111795585224966,42.51769578273244 L 3.117298664655483,42.5196013351235 L 3.124897513643849,42.52033295116203 L 3.133032656856836,42.517591829801255 L 3.137404757344646,42.51464002408297 L 3.131448028695649,42.509906117244306 L 3.12484091978095,42.50613409764365 L 3.123295817492265,42.50454270860722 L 3.124513933017531,42.5023585390174 L 3.128690200773403,42.499955069566646 L 3.132649974545801,42.49680372880743 L 3.133008402344165,42.495030615863065 L 3.129787942050597,42.48990970001642 L 3.128727930015335,42.486826085121976 L 3.130604510643861,42.481664084946715 L 3.134236399337556,42.480186688745654 L 3.13787457623824,42.48074253831558 L 3.144084629797359,42.480109836688314 L 3.15123971103537,42.47784332107399 L 3.153533109955728,42.476300903962006 L 3.154886871088896,42.47053770502733 L 3.1625288392109,42.46534984209826 L 3.15734106844511,42.46017016470221 L 3.16230426038987,42.4590276090872 L 3.166862312141493,42.45637194829783 L 3.159941691192636,42.45211284712422 L 3.167939392167153,42.448649545598336 L 3.168419990844157,42.4434134655207 L 3.174385702645994,42.44214464952158 L 3.177231565466085,42.440027910261755 L 3.173575422259718,42.43600968767255 L 3.168428973996998,42.43553830364258 L 3.163932007684695,42.43603885900094 L 3.155561505867269,42.434468891846 L 3.145733038343718,42.433418026148196 L 3.141777756147739,42.43375616151309 L 3.132994029299619,42.435939411234756 L 3.127210675500458,42.4348163035071 L 3.123706347577107,42.43531222368659 L 3.120874857801562,42.43818357742185 L 3.117553786196173,42.43637963214949 L 3.112566339738741,42.435133215518256 L 3.110190295812245,42.43548460143949 L 3.107618419153811,42.43380522413996 L 3.102679481721721,42.42661117219743 L 3.098884099646317,42.42515502213696 L 3.091667932968984,42.4253221230422 L 3.085009620083091,42.42616093633285 L 3.080195548475494,42.42905459026591 L 3.073713305385288,42.43532216856983 L 3.06294160681341,42.44424143034022 L 3.057905651330636,42.44619363393549 L 3.047264208474957,42.45841191937218 L 3.044575550829586,42.46367060263923 L 3.040442402207353,42.47421712287845 L 3.029475769218822,42.473723498434886 L 3.013113854633869,42.46807668787807 L 3.009968852824166,42.46816150644949 L 3.000022705998395,42.473250410415105 L 2.987959230047954,42.47378644405032 L 2.982437285996471,42.47179799460075 L 2.979119807652217,42.467308676988 L 2.966554173457954,42.46650884890091 L 2.963217830492734,42.46973725557053 L 2.960055760692633,42.47397925601517 L 2.953268090405826,42.47667723604202 L 2.948386645151921,42.48002172171331 L 2.947122715547164,42.482142409335296 L 2.942535019391166,42.479056423974406 L 2.93279279013489,42.47439535678795 L 2.921375202873731,42.480050210027066 L 2.917684025371283,42.48433788461509 L 2.91168417758865,42.488400039451044 L 2.907106362900776,42.48804630064033 L 2.898149261202821,42.49134910987844 L 2.894636848441913,42.49205389857695 L 2.888831935075933,42.49144383288165 L 2.885937563230499,42.49199097133747 L 2.881824177544517,42.49499551076607 L 2.878284815325086,42.49635333506405 L 2.872620039143428,42.49472394236868 L 2.862217548153324,42.48966195655167 L 2.862428652245092,42.48347003847713 L 2.861513268970574,42.47875430988598 L 2.859957386898479,42.47778766750943 L 2.85223816366204,42.476622243995465 L 2.849411165462916,42.473798370580845 L 2.846016432004228,42.468337107032866 L 2.840972391683897,42.46407550625084 L 2.845040861605674,42.462598357512924 L 2.846430555350207,42.45954520792041 L 2.844122783385304,42.45809711335761 L 2.840616658831386,42.4587658257178 L 2.837910034880334,42.45809048584555 L 2.834148788785725,42.45248004528253 L 2.833134590829954,42.44732913218574 L 2.828133669643261,42.440165139183854 L 2.817719500554463,42.439939738863366 L 2.814503531837315,42.438117944160204 L 2.809182810409475,42.43338951661248 L 2.805824907877437,42.43166499708992 L 2.802354715934883,42.42732597301035 L 2.800958733983361,42.42440107322855 L 2.801260567918826,42.42087721994296 L 2.796629752629189,42.41854224306608 L 2.784601310974829,42.41746259357485 L 2.780483433712425,42.41430776185965 L 2.773879019743578,42.41199376400633 L 2.768084886161007,42.413816318037455 L 2.759794334403868,42.41860126513752 L 2.75762490299272,42.424208108594186 L 2.75322225978525,42.42640296337774 L 2.746535200810265,42.42513115054271 L 2.737614031723673,42.42395281082848 L 2.730322406562475,42.42258214245296 L 2.727356169494313,42.42278903771287 L 2.724159963713415,42.42273665083661 L 2.716597047336413,42.42135468249569 L 2.710619657435882,42.415983682345015 L 2.700816342740286,42.41118195433241 L 2.694201148988029,42.40733301106158 L 2.682461964855155,42.40682891001291 L 2.674032174228978,42.4048078221054 z M 2.307372214601617,43.17582754072926 L 2.307358739872356,43.17378358504648 L 2.311192749504978,43.168889605483635 L 2.320249564199471,43.165445895670594 L 2.324746530511773,43.161178990490434 L 2.329450109339423,43.159562442280325 L 2.335774248939624,43.159059842369764 L 2.336892651468353,43.157334458156654 L 2.331029347608905,43.15576302520563 L 2.32430096613085,43.154701400423676 L 2.317155766360963,43.154582130314296 L 2.314334158053544,43.14774271424616 L 2.313971238678759,43.14444725284974 L 2.315899921593764,43.13516027952615 L 2.315601680919436,43.12977675511053 L 2.314314395117293,43.12509233622129 L 2.309051165867637,43.121110462029144 L 2.311765874656246,43.11575258660608 L 2.315817276587625,43.112827884408 L 2.322392046152096,43.11138318733289 L 2.325701439658792,43.11124743782955 L 2.33461093064669,43.10905966517674 L 2.339584004059575,43.110466380489456 L 2.34279278625445,43.11430927009801 L 2.351467816953192,43.12141929111602 L 2.352206232116739,43.122490673357916 L 2.358407302523016,43.125927624875786 L 2.36337588435948,43.126911074134874 L 2.365237193628176,43.12244412041908 L 2.365088971606297,43.11952563940841 L 2.368656181599535,43.116873903966884 L 2.37363554321941,43.115516517175045 L 2.379797087753185,43.11524175744046 L 2.385191471034323,43.11233014650654 L 2.391644968035438,43.104594687462146 L 2.402577465043173,43.10610775326108 L 2.406414169621647,43.10986238221098 L 2.408943825461728,43.11129858985167 L 2.423545940405091,43.11567783138478 L 2.423934012607829,43.11629160839324 L 2.431567895892278,43.11449419487771 L 2.439675191331456,43.11367252128049 L 2.451071219025796,43.099802081832536 L 2.455820611932936,43.100917132369474 L 2.46095448378168,43.10078857463736 L 2.465276278613579,43.10401424267281 L 2.481181849034199,43.10954037852137 L 2.486474722688231,43.11281280150076 L 2.495855829200292,43.120191833810765 L 2.494829953145827,43.12330632755286 L 2.495481231726814,43.12461567895008 L 2.490021271429935,43.126659968260526 L 2.484457106560098,43.12943059899234 L 2.484799364683348,43.133792809577926 L 2.479950258779671,43.13583810324678 L 2.469851398355599,43.138913781476184 L 2.465820657675755,43.13666800097551 L 2.462380110137577,43.13708032354207 L 2.457322595087985,43.13889477200489 L 2.452449234671636,43.139793454682994 L 2.448513715411908,43.139801320556224 L 2.445852007225062,43.14080355562409 L 2.441888640191527,43.142895807669696 L 2.434250265330659,43.143517832455395 L 2.430131489752971,43.145776469873866 L 2.411195003563731,43.146144162379215 L 2.410521267100641,43.14632047047874 L 2.41126237721004,43.152334303951164 L 2.409863700312666,43.15695045033254 L 2.40804371354704,43.16020788720983 L 2.409950836895226,43.16456528117004 L 2.413941153387284,43.16869829336511 L 2.419241213563589,43.17047052823966 L 2.420598567957894,43.173125180908166 L 2.419423571566266,43.17248642384112 L 2.409419932562311,43.17223419485168 L 2.398429045061109,43.183961413356336 L 2.391232641320027,43.18951372953622 L 2.384377597386911,43.191574186945466 L 2.376370913259554,43.19025579039401 L 2.369617378953543,43.190636314130586 L 2.367471303739782,43.191002426811615 L 2.359338855472648,43.18761956683059 L 2.350914454738175,43.18672289738886 L 2.346317775429335,43.17600179788858 L 2.344873284452471,43.171233785457666 L 2.336539613561694,43.17617670964993 L 2.338365888534309,43.177182278782844 L 2.339301933060362,43.18097315827392 L 2.334191417409005,43.18383958030125 L 2.324889362641948,43.187265223504774 L 2.322556437849089,43.18562644074092 L 2.322900492602908,43.18398564893507 L 2.318624511850498,43.18125810341642 L 2.31311873747413,43.18332735479678 L 2.308403480547786,43.18200026463678 L 2.306784716405803,43.17909576232046 L 2.307372214601617,43.17582754072926 z M 2.918553594566311,43.20826617350379 L 2.921593493487772,43.211256365991794 L 2.923213155945039,43.21429944181119 L 2.920537074713647,43.21666806483594 L 2.921348253415207,43.21888406470828 L 2.927278930920965,43.2170726463021 L 2.936906175820873,43.212804765842385 L 2.944463702306171,43.21054009500363 L 2.948400119881183,43.21302802025253 L 2.965311803420017,43.21330037643213 L 2.968372363593012,43.217253986961815 L 2.972069829302447,43.220167802090856 L 2.975275018236186,43.22133499115182 L 2.977239633762556,43.22652390050988 L 2.984551920175289,43.22499024967705 L 2.987139966508837,43.22521084118242 L 2.995192464715684,43.232588741689355 L 2.996872314296988,43.23436632089363 L 2.999477428620934,43.234931129940634 L 3.005265273996516,43.234320507647546 L 3.010895914197378,43.23455873615109 L 3.029082307124378,43.2377158303424 L 3.032082680173337,43.237408243365486 L 3.032006323374187,43.23398541542897 L 3.035859197627775,43.228552344839926 L 3.048130184408847,43.225089745054255 L 3.054329458184557,43.22225864540081 L 3.058361097179685,43.21967159268146 L 3.067576015364183,43.21177032125226 L 3.071545670604707,43.21649654243325 L 3.075332069527271,43.21903659585811 L 3.077604807196094,43.22646957209563 L 3.076202537037582,43.233467720508145 L 3.077529348712227,43.238128125344055 L 3.089074496743732,43.246716315726665 L 3.092314719973551,43.247358221894785 L 3.099778821669299,43.24626154630485 L 3.101128989541331,43.24756368278658 L 3.09997734934709,43.25263520631271 L 3.095256702529042,43.25541582630721 L 3.097182690498194,43.2580792140757 L 3.086852963046104,43.26317733690396 L 3.078212068328158,43.26204823730279 L 3.071441466031749,43.26758952126368 L 3.054395933515581,43.27987653760376 L 3.069467867352539,43.28593672154257 L 3.083212989514851,43.29042572074264 L 3.090134508778993,43.293434112932985 L 3.110754437810672,43.301149589140394 L 3.111547650206549,43.304954335998985 L 3.110792167052605,43.30745736347762 L 3.107942710971377,43.309249752835825 L 3.109180589432895,43.31046622039055 L 3.116598877049154,43.31070676563208 L 3.124956802452601,43.30785872732109 L 3.142342796461451,43.30413457046114 L 3.152203603335231,43.30432349667299 L 3.152332960736143,43.30240544342486 L 3.154901244133442,43.29569570339736 L 3.153778350028292,43.2905107240863 L 3.151727496234647,43.28729816897391 L 3.15074473931382,43.279907928910774 L 3.146851440872447,43.27245596303778 L 3.141323208613975,43.25947792031875 L 3.141304343993009,43.25941184558321 L 3.144902995021191,43.257205169914876 L 3.147271852425415,43.25339416460281 L 3.146564878296812,43.250931441358844 L 3.148423492619656,43.246790910674605 L 3.153548381315558,43.24344582637296 L 3.157338373499258,43.242171740681115 L 3.159966844020591,43.24297598174602 L 3.165946030551691,43.24344190010976 L 3.168621213467799,43.24294129947813 L 3.175024404813003,43.243473310208316 L 3.178346374733677,43.247453754752335 L 3.181170677986949,43.24928128255474 L 3.187637649717325,43.251083238019966 L 3.193209899424718,43.25031378207748 L 3.192223549242756,43.26641667677565 L 3.1905410047156,43.271784888148076 L 3.195101751413074,43.27840962992488 L 3.196357596180274,43.28099288100631 L 3.193811770665079,43.289485445062226 L 3.19068293853049,43.29550544273907 L 3.190941653332317,43.299215745090365 L 3.187155254409753,43.30227077210932 L 3.183993184609653,43.30361485691844 L 3.17758819663388,43.30377894305777 L 3.178172101568558,43.308541824185774 L 3.177152513721082,43.31135387989864 L 3.167564794693674,43.314749482731635 L 3.156953894557655,43.3205879072842 L 3.155648642449829,43.325773589143765 L 3.151483154477367,43.33066739502045 L 3.147888096710321,43.33107644679649 L 3.148979549780525,43.3340677992042 L 3.144127748930996,43.337598686131365 L 3.137213416189129,43.341673586681374 L 3.132395751320395,43.34623620990802 L 3.128956102097501,43.348049012686594 L 3.129555278392009,43.354586411698676 L 3.129430412567516,43.359387763703424 L 3.130938683929553,43.36115380277829 L 3.132261902343061,43.360863167485036 L 3.147192800680412,43.35217546850505 L 3.153198936670035,43.35011390873997 L 3.16145355581581,43.34838739654243 L 3.165109699022176,43.34857683841795 L 3.167474064849978,43.35023867559608 L 3.164234739935443,43.353463581201 L 3.167017720685645,43.35669288756082 L 3.178036455960656,43.35881300617195 L 3.179385725517403,43.36093958184311 L 3.178980585324265,43.365118059783896 L 3.176973748979542,43.36888614959786 L 3.183544925282876,43.37189653494152 L 3.183676079314358,43.374611630697196 L 3.181943229131291,43.37778822983321 L 3.18313798845917,43.37890538680996 L 3.174726164138676,43.394463134707735 L 3.172887312752083,43.39720137073235 L 3.166289186990225,43.39572619816447 L 3.169863583505736,43.40128989141255 L 3.171365566660784,43.40602554685716 L 3.179239300126092,43.409973818894635 L 3.182670864511428,43.414993952456186 L 3.187714904831759,43.41710811789064 L 3.193471309172397,43.421519569214034 L 3.193346443347905,43.42244736202446 L 3.194349861520266,43.422264022940894 L 3.203177605817308,43.426070967910114 L 3.21092557514284,43.4223162191778 L 3.214068780321974,43.4224362703519 L 3.215034469252402,43.41891162536867 L 3.21826121775296,43.41179254787382 L 3.218712172025588,43.40665598243503 L 3.217943214142381,43.404042177234324 L 3.218105809208807,43.399769129705945 L 3.219169414505204,43.39355710708311 L 3.236246388056316,43.38922783133136 L 3.240189093838317,43.39430843210258 L 3.241276055332102,43.3989813209721 L 3.241612025248362,43.400187506153266 L 3.239743529457394,43.404179886140426 L 3.2372372298147,43.40710041599555 L 3.236459288778653,43.410026025726424 L 3.241003865801014,43.41119544696499 L 3.245340931992743,43.41554011893124 L 3.247650500588214,43.421702258092736 L 3.2500157647313,43.42620993155215 L 3.251232083625998,43.425616234889084 L 3.255911407940977,43.42474459934848 L 3.262796096278469,43.42997357196379 L 3.268052139005853,43.43213483568523 L 3.270915069816341,43.43256799244716 L 3.276176502435429,43.4364375625808 L 3.279443675123772,43.43970810877283 L 3.282098196788345,43.440353195634486 L 3.285254876696742,43.44464164172484 L 3.291796408595699,43.44237777141015 L 3.295061784653474,43.44258387777321 L 3.300448083097054,43.4441361752899 L 3.311077847854041,43.44819348416238 L 3.322778404429698,43.449594989281294 L 3.327593374352578,43.45195184462486 L 3.32494873415613,43.45865801071073 L 3.319127651115036,43.46196191299024 L 3.312693018734888,43.45971238398336 L 3.30297863725242,43.458583023661404 L 3.30342150668749,43.46430329037023 L 3.304974693813733,43.47324089773141 L 3.303211300911006,43.47518810243826 L 3.293818516300253,43.47543516543982 L 3.286626604135591,43.46945647880097 L 3.283016275008715,43.46499114646859 L 3.277687468743318,43.460929096151624 L 3.275633021688537,43.46153157478501 L 3.269374459104076,43.459900826001984 L 3.26805393563642,43.458380232131745 L 3.262188835146405,43.46287213340552 L 3.255916797832682,43.46873803176951 L 3.257746666066433,43.47305184538931 L 3.254457933811271,43.47722780867892 L 3.251595003000783,43.479277223647976 L 3.243327807441031,43.47402904405442 L 3.240520572178157,43.47099439614431 L 3.233637680471234,43.46710682540699 L 3.225309399472161,43.46377777607711 L 3.213153397047456,43.46060829339085 L 3.210484502338337,43.45735909145798 L 3.207182295353914,43.45086994595213 L 3.195514976443769,43.454544670687056 L 3.189177362114306,43.453691058888275 L 3.192370872949351,43.45714455843472 L 3.194275301351684,43.461045810554886 L 3.195700927707582,43.471031556019824 L 3.193780329630134,43.47361052594691 L 3.191760018556149,43.47424873230596 L 3.185432285694812,43.48124378836827 L 3.18458786932774,43.48536574018812 L 3.182662779673871,43.48598361764429 L 3.175632564260352,43.48415082346865 L 3.160186931265201,43.481670727174205 L 3.156282853040417,43.48293979310951 L 3.155610913207896,43.479300689829074 L 3.152028431854828,43.47574220011061 L 3.150825587689391,43.47256617373034 L 3.153791824757554,43.46572984956226 L 3.164782712258756,43.46081107744771 L 3.167418369302363,43.45695480366734 L 3.164620117192331,43.4537106223188 L 3.166905431275131,43.45236594779569 L 3.173732627434439,43.451340140580115 L 3.174265328397922,43.44679129438028 L 3.173325690610733,43.444719254906616 L 3.170209434890122,43.4421005702893 L 3.165909199625042,43.4356163107101 L 3.166034065449535,43.434577825349706 L 3.160849887944881,43.43448584808619 L 3.157467730900171,43.432790440723934 L 3.153197140039467,43.42963433853455 L 3.151644851228508,43.42761325073575 L 3.146122907177026,43.422999333279854 L 3.139685579851025,43.41519688943959 L 3.138727975758153,43.41489672517172 L 3.136804682734854,43.41336064386903 L 3.129500481159678,43.41377892645282 L 3.125582928205633,43.41106689004287 L 3.12575899800132,43.40650261620584 L 3.129103425804097,43.39805839033769 L 3.13127824710695,43.395166145368684 L 3.134828389109791,43.39372160301847 L 3.138185393326546,43.39434955561844 L 3.149134958324678,43.4022473642865 L 3.151327745933214,43.40224997496213 L 3.153304937873561,43.399010039945935 L 3.152144314526478,43.398133452406675 L 3.147426362654283,43.39464068320246 L 3.142082285029056,43.3896528086073 L 3.138231207406036,43.387285692953775 L 3.135833603912721,43.38486886781361 L 3.139390932437834,43.381929638354855 L 3.141170495015675,43.378488822208254 L 3.135234427618213,43.36980430031322 L 3.128568928210046,43.365632022735035 L 3.127510712805353,43.36293873012583 L 3.120108594864209,43.36567708408221 L 3.116134448047263,43.366339939161925 L 3.109047638770845,43.3665338963721 L 3.108439479323496,43.36455445881021 L 3.10722585537465,43.35910626381999 L 3.101901540685674,43.35536499982284 L 3.097222216370696,43.35375033279147 L 3.094967445007555,43.35469483983815 L 3.091490964858013,43.35819905461301 L 3.093107932369428,43.362922402817645 L 3.087592276524934,43.36462695007476 L 3.085146164006277,43.36266116528909 L 3.082814137528702,43.358603348944456 L 3.081129796370978,43.35739372362831 L 3.076092044257636,43.35698942123709 L 3.065966234375041,43.3572519894941 L 3.064057314396286,43.35629576530164 L 3.061981307774686,43.35430358317007 L 3.050881724124106,43.35652959747281 L 3.04652579331141,43.35691626768949 L 3.038378073684446,43.35115057634819 L 3.031517639859625,43.35389926018997 L 3.026153799298148,43.35061885364947 L 3.025869931668366,43.34490221379122 L 3.02474344430208,43.34144231056529 L 3.006406134407348,43.33262376150079 L 2.993114661463516,43.325287389007805 L 2.990189746898423,43.320180091979175 L 2.990904805864582,43.3191442026554 L 2.980142090445546,43.31746125332515 L 2.968618501980861,43.31431352278162 L 2.957563834094486,43.31235656657717 L 2.94594861747082,43.31188006369294 L 2.938171003740913,43.30946807819825 L 2.933454848499286,43.30755999259533 L 2.926943859319988,43.30582638704397 L 2.926125494096155,43.298899315800014 L 2.924142912264103,43.29182368372052 L 2.92248282561905,43.29444690264079 L 2.91788165473379,43.2973877548157 L 2.913834744378831,43.29623836798398 L 2.90684764809895,43.29352303510633 L 2.904059277457043,43.28854253927547 L 2.903416982028897,43.284581129056846 L 2.90852031115798,43.28241656516418 L 2.90883202656157,43.27245269271025 L 2.905132764221566,43.271595860844414 L 2.899270358677402,43.26862105627042 L 2.907078515126968,43.26674177871071 L 2.910737353279187,43.26528240411392 L 2.910538825601397,43.26416838911137 L 2.913759285894966,43.2612926577982 L 2.914017102381508,43.25873015947213 L 2.909330591544256,43.254587541957626 L 2.908095408028592,43.252523978872105 L 2.907958864105406,43.24405570285186 L 2.907394722106979,43.2408334966644 L 2.90995941224314,43.23801621697659 L 2.912264489262191,43.23557708705701 L 2.912580696242201,43.23261884839617 L 2.902824093941379,43.23368631788851 L 2.900253115598229,43.23315422169886 L 2.896245731115771,43.23391080480137 L 2.893527429066026,43.240325021671765 L 2.88715657707105,43.24297009230569 L 2.882226622791802,43.24316902419703 L 2.879983529527355,43.23825639433116 L 2.875917754551431,43.23496712570478 L 2.870844069826723,43.234348650074224 L 2.868260515069596,43.23491215034734 L 2.861783661871094,43.23602211938441 L 2.855660744894535,43.221789292013774 L 2.854448019260974,43.219694504814356 L 2.842335135969906,43.20586383034318 L 2.84351552225324,43.20327084357865 L 2.848405052344702,43.199851992329094 L 2.855602354401068,43.20515862743003 L 2.862932607119483,43.20601050104845 L 2.865499093886213,43.210730621288754 L 2.865288888109728,43.214905681892894 L 2.867284046355758,43.21581175734571 L 2.872133152259435,43.2147609966326 L 2.876030043961946,43.21668966876848 L 2.888042315941192,43.220827662701296 L 2.892383873709341,43.225418340475194 L 2.900236945923115,43.223012090771576 L 2.90229229129318,43.22186980896036 L 2.900525305129317,43.218316488365716 L 2.903774511511977,43.215502095789795 L 2.911580871330976,43.21370956311745 L 2.91346374016649,43.21281982408343 L 2.918553594566311,43.20826617350379 z M 2.929706178818655,42.653742783656114 L 2.933341660773487,42.64998606287544 L 2.930953040433013,42.644378308849326 L 2.932602347294656,42.63822089243402 L 2.934531030209661,42.63490271879687 L 2.934676557285689,42.63290221275743 L 2.936601646939557,42.632442226061976 L 2.940700659580994,42.62916207870154 L 2.949063974876147,42.628285008197814 L 2.957262000159022,42.631016642251936 L 2.961873950827691,42.63380036663439 L 2.966643106671082,42.62970867177746 L 2.969435968889409,42.62901336754196 L 2.977385160838583,42.62884482779826 L 2.983349974325137,42.63074632620097 L 2.988349098881262,42.633785166198315 L 2.992178616937463,42.63810722759222 L 2.998449755935902,42.64237216025403 L 3.006063876284099,42.644872567411355 L 3.004128905162105,42.650722757452854 L 3.002883840178316,42.6525476081465 L 2.997678103106843,42.65311051290623 L 2.990789821508214,42.650449223444326 L 2.977984337133091,42.648906442682396 L 2.971432923766007,42.645554479195084 L 2.96719018067911,42.645976044950395 L 2.96806873302698,42.64217788551183 L 2.961223570561989,42.64457125517235 L 2.958558269114006,42.64729292396396 L 2.953930148770222,42.6461022498168 L 2.946960120480739,42.647982077333154 L 2.936138116252951,42.65224897585347 L 2.929706178818655,42.653742783656114 z M 2.565172531468806,43.049544709652025 L 2.553560908106276,43.03535770743535 L 2.553481856361274,43.034794335175064 L 2.566793092241357,43.022446779437026 L 2.58682282813137,43.03077441236743 L 2.598388637414409,43.02674043554854 L 2.600284082663901,43.031417276252945 L 2.597935886511213,43.03506879923 L 2.591591085659477,43.043187914960214 L 2.584924687936025,43.047973133391665 L 2.575531005009988,43.04830924665355 L 2.565172531468806,43.049544709652025 z M 2.405769179247649,43.222985906900966 L 2.404733421725059,43.22278690911682 L 2.394924717137758,43.21434854355398 L 2.3912964217052,43.21472106082156 L 2.388750596190005,43.212059705620426 L 2.392741810997348,43.20986113567038 L 2.404351637729309,43.20769784357307 L 2.413544098031704,43.20888033054029 L 2.421628935588779,43.210376412152804 L 2.421420526442863,43.21387323702284 L 2.41983230502054,43.21864184707684 L 2.418793852552098,43.219232987334586 L 2.412204709943081,43.21902153915213 L 2.408295241826593,43.220069607937205 L 2.405769179247649,43.222985906900966 z M 3.47763628131321,43.582383600673545 L 3.478906499124955,43.576733740329544 L 3.478341458811244,43.57454250346681 L 3.472296695264403,43.56790973515698 L 3.477607535224118,43.56388064382132 L 3.479035856525868,43.56198446930016 L 3.480033884806525,43.56039679220542 L 3.477735994309747,43.55934482730605 L 3.479714982880662,43.55675519684205 L 3.482980358938436,43.55482559590303 L 3.482181756650855,43.550772842169366 L 3.479663778909468,43.54559859327778 L 3.480025799968968,43.54108549389001 L 3.477375769880815,43.535342540654156 L 3.48353731441459,43.53567466873737 L 3.487673157982677,43.53350017342293 L 3.487832159787966,43.53134644055008 L 3.494927053901942,43.53117450331346 L 3.50361645764523,43.533653868988246 L 3.50833081625629,43.53799495566767 L 3.50992712251617,43.54071823215525 L 3.503865290978931,43.547075972260316 L 3.520351173073093,43.54584015926324 L 3.522351721210828,43.55051111280086 L 3.515051112896788,43.55580798143006 L 3.510920659220406,43.55674543181067 L 3.509132113489724,43.562583336221685 L 3.514898399298487,43.56253061013389 L 3.519099819882315,43.56938070942381 L 3.518517711578205,43.571634614898336 L 3.521423761522331,43.57363006115891 L 3.523384783787565,43.57367431688229 L 3.528613877056424,43.573603377545275 L 3.531779540117661,43.57574974361857 L 3.532286189937905,43.57835938220837 L 3.536762494998672,43.58280917338049 L 3.538943604508515,43.5865058030048 L 3.538883417384479,43.589062923194476 L 3.543142330146489,43.592120263489726 L 3.548156726062444,43.594303704073475 L 3.548199845196082,43.59519501431874 L 3.544282292242037,43.597035495669175 L 3.533761223634429,43.60690053092795 L 3.53715236383198,43.61052218337773 L 3.540171601501906,43.61758729029507 L 3.544164612939817,43.62488501295067 L 3.546462503436595,43.63073386680636 L 3.549733269386074,43.626882595459925 L 3.551995227271487,43.6306655955307 L 3.553315750739143,43.635432044663986 L 3.540901033512611,43.638542288597804 L 3.53429122965206,43.63666601940302 L 3.520524547922928,43.62851338508583 L 3.515221792800771,43.63194583159139 L 3.511342867403942,43.632129184650395 L 3.508981196521992,43.62832026757124 L 3.505996094832863,43.625424730915505 L 3.495411245840083,43.62131820072312 L 3.494529998546361,43.61766728261078 L 3.498631706133651,43.614316619044615 L 3.500349284956887,43.611955693895936 L 3.500164232008359,43.60904310812826 L 3.498410720573758,43.603885573104165 L 3.500572067147349,43.60014707575668 L 3.498447551500407,43.59817722940187 L 3.491093942584604,43.59715780208514 L 3.488764611052882,43.594976416823485 L 3.489390736805914,43.5927201319953 L 3.493708040061392,43.58861982694275 L 3.49300555750921,43.58311045621511 L 3.492596824054936,43.58215910009772 L 3.486305922120247,43.580596032304314 L 3.484168830059327,43.5814211705676 L 3.483912810203353,43.58410539464885 L 3.47964042271208,43.58446133004265 L 3.47763628131321,43.582383600673545 z M 3.617652193072499,43.62288086118824 L 3.620524107035829,43.62226308516825 L 3.646708200937345,43.62559119716542 L 3.64945255413033,43.62547480089071 L 3.651245591437433,43.614952687160724 L 3.653426700947274,43.60977159300777 L 3.656767535488915,43.60400786559584 L 3.661275281584627,43.60099016069705 L 3.66810337605922,43.59860334099843 L 3.669963787012631,43.59645128393341 L 3.672827616138404,43.59843484837668 L 3.681419103515723,43.60109359391547 L 3.685204604123002,43.60391809772728 L 3.687107235894768,43.60820208737935 L 3.690060896548953,43.61019437036483 L 3.695517263584695,43.610855197942314 L 3.697298622793104,43.60945873585029 L 3.695986184163005,43.60578237973283 L 3.698656875502693,43.6056652946359 L 3.699979195600917,43.60088997992604 L 3.70754660355434,43.59868400911364 L 3.71286103677519,43.598789397939925 L 3.716049157718531,43.59966763098029 L 3.715768883349885,43.605424618998754 L 3.713497942311631,43.60975077927652 L 3.710424805724658,43.61348672864714 L 3.707876285263611,43.61330461950414 L 3.702562750358044,43.61108674604498 L 3.698111598125232,43.61043697818313 L 3.699268628211178,43.618383306648475 L 3.698086445297277,43.620931268533205 L 3.695577450708731,43.623467417386784 L 3.683030681135433,43.63265843466882 L 3.678690919997853,43.6340426550022 L 3.671567279794785,43.6337851885328 L 3.669020555964305,43.63628373699807 L 3.669685309274554,43.64082155828739 L 3.675706716624007,43.644103788759516 L 3.674530821917095,43.64912722382622 L 3.672444035512085,43.65393120561903 L 3.670341977747245,43.65551836558789 L 3.663283016244634,43.65792437330583 L 3.661755880261631,43.659746037870995 L 3.661166585435248,43.67020962749763 L 3.66744491095596,43.67793300639958 L 3.665438074611237,43.67792001256977 L 3.658937865215348,43.6811268044143 L 3.656496244273111,43.68368314960481 L 3.648704257498659,43.683237502348845 L 3.643238907310075,43.68821219310241 L 3.638846145570731,43.689967982439235 L 3.635433445806361,43.69026353200897 L 3.633840732807617,43.692495371404405 L 3.634840557718841,43.696656789572415 L 3.630049842308632,43.69479532621019 L 3.624008672022929,43.695548102197996 L 3.623604430145074,43.69313840634173 L 3.624910580568184,43.68702864646659 L 3.628986237012235,43.67873666930338 L 3.629036542668145,43.67694806612392 L 3.627303692485079,43.67049877669383 L 3.626713499343413,43.664432199578904 L 3.624546762878116,43.66063312885986 L 3.62162993315058,43.65141129177044 L 3.61860979716537,43.64762894767818 L 3.620542971656795,43.63796563136354 L 3.619521587178751,43.628823542219884 L 3.617652193072499,43.62288086118824 z M 3.347428175825938,43.41053112450023 L 3.351071742618326,43.40762315966651 L 3.359021832882784,43.409832860224 L 3.364977663216496,43.410852845425026 L 3.373658982122227,43.41324579510336 L 3.379490844946731,43.41121437156499 L 3.389653485755975,43.41542266419108 L 3.390770091654136,43.418601039631895 L 3.392310702366401,43.41920981227657 L 3.390530241473276,43.42294779007778 L 3.392114869634463,43.42756105906595 L 3.398253057970852,43.43395159440835 L 3.400057773376647,43.43504879893389 L 3.402848838964407,43.44010533698662 L 3.402581141009739,43.44475969198707 L 3.39364739550917,43.452105095735114 L 3.387245102479251,43.45547000095941 L 3.383224243267532,43.458668443684125 L 3.381265915948152,43.46893296515673 L 3.375689174664337,43.47149898618686 L 3.368914979106792,43.46720005701268 L 3.365380108463782,43.46361281841528 L 3.364550963456539,43.460764130716925 L 3.36126492614723,43.45490984852531 L 3.352442571741892,43.45321762193984 L 3.349497894240549,43.451557954883164 L 3.348073166199935,43.44735152201084 L 3.345057521791146,43.44339068611745 L 3.340241653552981,43.441448327347786 L 3.339770936344102,43.440346673051494 L 3.342202675818214,43.435377565025206 L 3.344979368361428,43.431185011210395 L 3.339178946571868,43.42777569702 L 3.338172833453654,43.422072201383685 L 3.339292134297667,43.41944535932101 L 3.343724421909513,43.41627420589576 L 3.347428175825938,43.41053112450023 z M 3.415882495421697,43.500052957895704 L 3.416743979779168,43.49498383738018 L 3.421052299881805,43.49091793750711 L 3.427395304102973,43.486235850612 L 3.430607679558984,43.48453407236807 L 3.438002610977856,43.482882434772385 L 3.44339609594371,43.48262171437186 L 3.448909056842352,43.47942584264097 L 3.450480210274277,43.47788488059062 L 3.4577817169036,43.48075687890259 L 3.46509759657747,43.48153058728809 L 3.467766491286588,43.48220455939069 L 3.46886153761793,43.48472374056686 L 3.464398707286424,43.488859141388104 L 3.465298819201112,43.49760739985404 L 3.463664783699299,43.50177319517423 L 3.464661913664671,43.507177939950125 L 3.46454603099302,43.510157998505605 L 3.466471120646888,43.51230206418176 L 3.471118105611638,43.51487862088579 L 3.477334447377745,43.5183175500689 L 3.479316130894513,43.520689964725435 L 3.480015020185558,43.52373905150751 L 3.480112936551528,43.52673001617294 L 3.47820760983391,43.53173655390176 L 3.474794011754256,43.53009206899945 L 3.467724270468235,43.52869698962454 L 3.460429950361184,43.52575757812094 L 3.455390401617274,43.52122475481547 L 3.453143715091691,43.51651179468057 L 3.448340423267504,43.51629616876248 L 3.447582245167707,43.51353661013455 L 3.447114222904681,43.50953124720608 L 3.445510730122527,43.506697754967675 L 3.44186087512315,43.502795540244804 L 3.439553103158246,43.502930418129694 L 3.43481089677338,43.50488578781662 L 3.4256175381557,43.503697978327054 L 3.415882495421697,43.500052957895704 z M 3.463088963602178,43.45335065418406 L 3.466513341465242,43.45002867321103 L 3.469200202480043,43.449141737339744 L 3.479119399847291,43.452035317618176 L 3.483765486496757,43.45285047751231 L 3.493736786150484,43.453265878768136 L 3.492330024415553,43.45942156919651 L 3.486640993721223,43.46420483828838 L 3.480474957611027,43.46679387910569 L 3.477197903454559,43.46939780377382 L 3.476828695872786,43.4741444293781 L 3.474727536423231,43.47692469273988 L 3.466487290322002,43.47887568980379 L 3.45629231016253,43.47800612522204 L 3.452044177183928,43.47572720694636 L 3.4455745105077,43.475096838804895 L 3.445021148292682,43.4717362854739 L 3.452289417256493,43.46715637508945 L 3.454301643492921,43.46509481320188 L 3.455217925082723,43.45863323239166 L 3.463088963602178,43.45335065418406 z M 3.901009579772888,43.74911510349521 L 3.903340707935178,43.747742621575334 L 3.909955901687434,43.74572764429464 L 3.917411020230342,43.744219449444735 L 3.925502145994407,43.74434470115227 L 3.929061271150088,43.74373596241599 L 3.935322528680401,43.74000162367224 L 3.939207742284218,43.73919488764562 L 3.949001175511689,43.740468266500095 L 3.946841625568666,43.73443862358368 L 3.947130883090152,43.72893612575877 L 3.945360303665152,43.722150941521306 L 3.943127990184115,43.72077069995651 L 3.949011056979814,43.717412152459104 L 3.954337168299359,43.72329679053516 L 3.958503554587105,43.73698685461477 L 3.96420246674956,43.73856922165023 L 3.968284411400599,43.73798314062245 L 3.973367977593431,43.740097029386234 L 3.981324356064877,43.74456535119129 L 3.986192326589522,43.747771823645984 L 3.993546833820607,43.748807514538726 L 3.994015754398918,43.74323494891891 L 3.99739162323664,43.737926024999574 L 4.003108501704777,43.73415172985504 L 4.009587151533846,43.737057601203766 L 4.019800996314285,43.740408557189575 L 4.020353460214019,43.74117893252605 L 4.023655667198442,43.74394298619896 L 4.037477146159905,43.74833185171108 L 4.042410693700289,43.7452078275555 L 4.049932287574222,43.74211024920448 L 4.052146634749577,43.74521237029312 L 4.056646296007731,43.74958102637221 L 4.061963424174435,43.75086781124286 L 4.064745506609353,43.75767958813706 L 4.066721800234416,43.7607244572757 L 4.067435062570007,43.76497585528087 L 4.064998831519475,43.769862675838226 L 4.061109126339237,43.77277784101678 L 4.054292709963338,43.77208376695774 L 4.051578899490013,43.77403947837338 L 4.053685448831274,43.777590082494896 L 4.051784613690076,43.77815437173534 L 4.045314947013848,43.77799286880704 L 4.033732968055694,43.777167186377795 L 4.026749465036949,43.77813167054687 L 4.022201294753453,43.780659232787144 L 4.016200548655535,43.78503243489447 L 4.012164418083985,43.78629900183077 L 4.006240028785216,43.7870071780976 L 4.001121428296304,43.786913144186265 L 3.998203700253483,43.78763039221233 L 3.995305735146914,43.79125670797191 L 3.995372210477939,43.79282791345165 L 4.001233717706818,43.79673986054505 L 4.003954714702417,43.80040517379689 L 3.999752395803306,43.80294929936519 L 3.996071998084268,43.806688785744186 L 3.995572534786297,43.810200500072284 L 3.989896080505946,43.80717111576381 L 3.982687100350887,43.80418631210068 L 3.974452244141363,43.80149571183554 L 3.979531318757775,43.79348478570995 L 3.975647901784527,43.78526331135909 L 3.976478843422337,43.78486641081819 L 3.975224795285706,43.78321911570276 L 3.97567485124305,43.780069672396834 L 3.977206478802473,43.77732739430781 L 3.981309983020332,43.77473547566203 L 3.985534759801546,43.76700705140477 L 3.982097805524504,43.7661260747785 L 3.977511905999074,43.76143228767275 L 3.969276151474267,43.75975125790673 L 3.963165810911686,43.757654932693804 L 3.95215246552838,43.75840107976752 L 3.944434140607226,43.75828818506855 L 3.939087368036146,43.75773344078043 L 3.939877885486172,43.75591475179968 L 3.922631130346361,43.74769330249033 L 3.921077044904834,43.74623577539583 L 3.910392482915516,43.74723839690869 L 3.90123685353977,43.75099759133795 L 3.901009579772888,43.74911510349521 z M 3.812179672902729,43.66710557186916 L 3.809026586255469,43.66608666326802 L 3.802385341359973,43.660274394741705 L 3.799840414160063,43.659191030689655 L 3.794427166257959,43.660301039922 L 3.790224847358848,43.662308492791524 L 3.787166982131705,43.66649474863611 L 3.780867995359459,43.665984641492514 L 3.782838000777533,43.66296809764905 L 3.775090929767286,43.66112183565876 L 3.766749174038952,43.656994997635124 L 3.763398458029186,43.65681497005541 L 3.757693257659743,43.6507184106731 L 3.750454633100309,43.64853961802569 L 3.74355108014185,43.64573151093239 L 3.741557718526388,43.64355903802253 L 3.743128871958314,43.64130457552937 L 3.748295981472569,43.63962797547454 L 3.749547334663347,43.63899411875313 L 3.748322032615809,43.63465640900556 L 3.752427333464235,43.63483325221612 L 3.755460944178706,43.635925507571166 L 3.761402401467873,43.635860492927165 L 3.764728862964967,43.63590925391676 L 3.764254552494953,43.63889465139909 L 3.767135449611123,43.64119146013047 L 3.773449707743199,43.64358504055661 L 3.783003290789811,43.6416172669895 L 3.795725231843512,43.637787497119355 L 3.802139202972125,43.63764056847672 L 3.807789606109237,43.63880753606486 L 3.808976280599559,43.640822858473605 L 3.81618795570047,43.643014932415156 L 3.819907879292009,43.642453269845014 L 3.822981914194266,43.64079360427699 L 3.824543186158066,43.63808915479945 L 3.826932704813824,43.63895966276024 L 3.836187148870823,43.64608448160678 L 3.838537141654079,43.65038041699324 L 3.833025079070723,43.65615399774244 L 3.831425179549705,43.65876144900955 L 3.830609509271725,43.66249175319326 L 3.827866054394024,43.66610225893817 L 3.827447439471624,43.66667279775227 L 3.818301691564004,43.66507293601707 L 3.815233944868735,43.66550832097219 L 3.812179672902729,43.66710557186916 z M 3.325723081931041,43.66864364922296 L 3.326722906842266,43.66811666560318 L 3.328235669780724,43.65956081941715 L 3.330949480254049,43.65399425085681 L 3.332929367140248,43.65561260595382 L 3.340878559089422,43.65875235042333 L 3.348349847307444,43.659668701112366 L 3.357483018801087,43.66263147262726 L 3.362689654187844,43.661971214207 L 3.364462030243412,43.659372350930255 L 3.370209451431208,43.65700084689761 L 3.381887550124762,43.657903576245374 L 3.386197666857968,43.6594828325288 L 3.387423867220791,43.66370502691848 L 3.385569744474368,43.66815045507398 L 3.384480986350015,43.669104999765 L 3.386034173476258,43.67272550371933 L 3.383254785987192,43.67754448967291 L 3.371187716775615,43.68184921846778 L 3.369966007989212,43.681721237475415 L 3.368451448420186,43.67521788453093 L 3.361897340107251,43.67306012290446 L 3.349439503747081,43.669862646623386 L 3.342509001330099,43.668557876618586 L 3.334725997708488,43.669058864870365 L 3.325723081931041,43.66864364922296 z M 3.282402725669662,43.61681077399257 L 3.280754317123303,43.61468213140188 L 3.283785232891922,43.61187374276355 L 3.287180864665893,43.61153422973976 L 3.299016168534168,43.61268089151052 L 3.311008677577163,43.60943792201077 L 3.317132492869006,43.61145032680059 L 3.322676894802592,43.61194463700364 L 3.3324739212912,43.611714393019795 L 3.33485086353298,43.612414878333794 L 3.334543639705811,43.62066138998543 L 3.332370615033526,43.62647424089188 L 3.334250788923188,43.62909338412682 L 3.334732285915476,43.631627888258805 L 3.332951825022351,43.63384370373634 L 3.333615680017316,43.63542944405821 L 3.329857128868559,43.63683115424754 L 3.324043232349738,43.64086381432493 L 3.316948338235762,43.634868360732774 L 3.310669114399766,43.6311298386758 L 3.30520735747232,43.62901145635062 L 3.297432438688265,43.62799515250875 L 3.293158254566425,43.62968508141702 L 3.290943009075786,43.62806212635544 L 3.286517907986213,43.619767205417915 L 3.282402725669662,43.61681077399257 z\" /><path fill-rule=\"evenodd\" fill=\"#66cc99\" stroke=\"#555555\" stroke-width=\"0.05057997265237418\" opacity=\"0.6\" d=\"M 3.825792742718276,43.4667808396413 L 3.826432343200569,43.47107719092341 L 3.82986929747761,43.47021990090685 L 3.82613140758039,43.46655330053505 L 3.825792742718276,43.4667808396413 z\" /><path fill-rule=\"evenodd\" fill=\"#66cc99\" stroke=\"#555555\" stroke-width=\"0.05057997265237418\" opacity=\"0.6\" d=\"M 3.799660751103239,43.89116801050004 L 3.79809319093245,43.89030504925919 L 3.789701129548206,43.888030733668195 L 3.777993386450276,43.88752510260416 L 3.783358125327038,43.89564057774773 L 3.785344300420226,43.89930499711234 L 3.78693252184255,43.904183419198965 L 3.789471160835471,43.907946981021524 L 3.783210801620442,43.91711509595906 L 3.781796853363239,43.92272644769107 L 3.778094896077381,43.927065305861234 L 3.774796282354095,43.92827321974963 L 3.773129009186769,43.92706077694672 L 3.759394666807866,43.925129486976196 L 3.758166669814474,43.92560697756757 L 3.75044834489332,43.92482409803018 L 3.74632507773921,43.92562379965944 L 3.746486774490352,43.92799890187635 L 3.742011367744869,43.93818533889845 L 3.73504672934709,43.93758310285839 L 3.731191160147649,43.937963463172295 L 3.723489903216893,43.93649828635297 L 3.724908343050518,43.94104377457411 L 3.723923789499122,43.94290792583992 L 3.719600198036655,43.944281099117106 L 3.718196131247576,43.94624151904454 L 3.717646362293695,43.950146643265064 L 3.717870042799441,43.95519097511442 L 3.716683368309119,43.96077351654821 L 3.719193261212949,43.96213075854691 L 3.724577763025961,43.96638333556997 L 3.726841517541942,43.97054380315806 L 3.732169425492055,43.97201266631588 L 3.737330246799322,43.968745172122624 L 3.743763082548901,43.966603812104815 L 3.748085775696085,43.968941718207866 L 3.754429678232537,43.969936722783814 L 3.758988628299444,43.96758398526857 L 3.767673540466311,43.96832427683794 L 3.776518352753752,43.96682299471101 L 3.787688903311778,43.96680812394029 L 3.800262622343599,43.96575099657015 L 3.804396669281117,43.96673894247976 L 3.806967647624267,43.969264983380896 L 3.799107388888222,43.97249042691638 L 3.790178134964074,43.97544288922796 L 3.789592433398827,43.97864407947021 L 3.792010698143678,43.98338413566449 L 3.796873278776617,43.98840625601757 L 3.796608275767801,43.99456536787955 L 3.801149259529025,43.99278168245143 L 3.804739825719651,43.991450346068866 L 3.807044902738702,43.98467105409625 L 3.814145186744383,43.98259426030971 L 3.815104587467822,43.98113406207722 L 3.825372331165308,43.981654411294265 L 3.830486440077801,43.98075268522071 L 3.838102357056565,43.97588183485224 L 3.845570950328736,43.97588054193912 L 3.848252421451832,43.977122372026166 L 3.851216861889427,43.982508937629376 L 3.851839394381322,43.98511898048596 L 3.86083422532121,43.98265178841134 L 3.864979950357422,43.9776854212544 L 3.873694506928666,43.976086761227094 L 3.882313842079792,43.97836740829013 L 3.895255870378103,43.971338364367995 L 3.898676654980029,43.97090908282123 L 3.906892646568587,43.972978527485004 L 3.916221650794168,43.969516482468634 L 3.924662221203755,43.968593236380755 L 3.927929393892097,43.96589582736483 L 3.92790603769471,43.963248083621004 L 3.930949529877307,43.96286788508643 L 3.935051237464597,43.96080714083126 L 3.932564700758154,43.9565095446049 L 3.932349105089965,43.9498206870959 L 3.937815353593833,43.94195904004651 L 3.938279782595723,43.93970157534352 L 3.934227482349059,43.936859894053796 L 3.932710227834181,43.93431630270098 L 3.932840483550379,43.930096366299246 L 3.923128797013762,43.91642334633529 L 3.923222221803311,43.91323045199505 L 3.926098627343062,43.90988594767991 L 3.926256730833067,43.90679949203018 L 3.923945365607028,43.90076138097277 L 3.889884843294352,43.913137264017834 L 3.881069675411287,43.919310005040714 L 3.879330537021231,43.91999848578274 L 3.868761857703565,43.92675022198625 L 3.862683856491212,43.92356823421089 L 3.837591215659902,43.91999783871811 L 3.82985402611778,43.92071478200836 L 3.823724820934233,43.91762176995441 L 3.819566519484044,43.91073050452967 L 3.809216130780419,43.89990503157797 L 3.799660751103239,43.89116801050004 z\" /><path fill-rule=\"evenodd\" fill=\"#66cc99\" stroke=\"#555555\" stroke-width=\"0.05057997265237418\" opacity=\"0.6\" d=\"M 3.899913635126262,43.5402246392787 L 3.898761096616737,43.52559800038601 L 3.89887967423424,43.523793114249315 L 3.907427144162638,43.51714303349765 L 3.891550319831109,43.50953711081388 L 3.872362305362316,43.49862004101904 L 3.86750691125165,43.496222647476806 L 3.853072781266417,43.48759085155138 L 3.845429016513845,43.48267451034382 L 3.836277878714519,43.475783268324086 L 3.830428049584333,43.470715370384276 L 3.829972603735284,43.47138359581223 L 3.833193962344137,43.477030946457184 L 3.837493299293933,43.4787766096136 L 3.83888389135375,43.47838941310175 L 3.844879247559963,43.48358181168943 L 3.860375186211025,43.49245335356635 L 3.859311580914627,43.4939274726624 L 3.845284387753102,43.50640586317945 L 3.837344178956769,43.50756039119925 L 3.832013576060804,43.50689256588917 L 3.832532802295024,43.50826404304423 L 3.831837506265116,43.5142441010753 L 3.833375422031529,43.51630463746866 L 3.831996508070406,43.51884324142732 L 3.828859591098261,43.52210477057705 L 3.821025383505454,43.52586114058834 L 3.810422568206991,43.53156657159964 L 3.817126695172375,43.534756427809604 L 3.818044773392745,43.539675691531436 L 3.815916664484666,43.543317667801325 L 3.820231272794292,43.54955468412483 L 3.824143435856633,43.55156778932734 L 3.827907376897094,43.55453133284022 L 3.833516457531136,43.55590433116938 L 3.835331952720341,43.554669350207895 L 3.843807557426009,43.55543625227582 L 3.847643363689199,43.55348512487274 L 3.848228166939161,43.55155021075663 L 3.854238794505205,43.544765801771995 L 3.857776360094068,43.54460171639693 L 3.862690144698201,43.545678030177676 L 3.86671100390992,43.54881375072856 L 3.877762078535158,43.5443321465969 L 3.878979295745141,43.541991917533146 L 3.885854102614507,43.54037636416826 L 3.897676830068804,43.540963724732954 L 3.899913635126262,43.5402246392787 z\" /></g></g></svg>"
      ],
      "text/plain": [
       "<MULTIPOLYGON (((2.674 42.405, 2.673 42.405, 2.672 42.414, 2.675 42.418, 2.6...>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "French_wine_regions['geometry'][6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38302a1b-1445-4f4c-9593-9fbe8e26f775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coord_lister(geom):\n",
    "    \n",
    "    if geom.geom_type=='MultiPolygon':\n",
    "        coords = [list(x.exterior.coords) for x in geom.geoms]\n",
    "    else:\n",
    "        coords = list(geom.exterior.coords)        \n",
    "    return (coords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e35629-9f21-4202-a898-3aac338c7dda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36c4ed2-717c-42c1-85ed-1a83fa1968e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def data_period_pick(path,year_start,year_finish,month_start,month_finish):\n",
    "    \n",
    "    delta_year=year_finish-year_start\n",
    "    delta_month=month_finish-month_start\n",
    "    data_files=[]\n",
    "    year_size_data=[]\n",
    "    \n",
    "    for i in range(year_start,year_finish+1):\n",
    "        month_size_data=[]\n",
    "        for j in range(month_start,month_finish+1):\n",
    "            year_actual=str(i)\n",
    "            month_actual=str(j)\n",
    "            if j <= 9 :\n",
    "                data_files.append(path+'-'+year_actual+'-'+'0'+month_actual+'.nc')\n",
    "            else:\n",
    "                 data_files.append(path+'-'+year_actual+'-'+month_actual+'.nc')\n",
    "                \n",
    "            data=netCDF4.Dataset(data_files[-1])\n",
    "            month_size_data.append(len(data.variables['t2m']))\n",
    "            \n",
    "        year_size_data.append(np.sum(month_size_data))\n",
    "    return data_files,delta_year,delta_month,year_size_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e741c1-3dfc-4f90-844f-5ae20de200f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_year_sep(data,year_size_data):\n",
    "    \n",
    "    data_matrix_year=[]\n",
    "    _iter=0\n",
    "    \n",
    "    for i in range(len(year_size_data)):\n",
    "                                   \n",
    "        size_year_actual = year_size_data[i]\n",
    "        _iter_end=_iter+size_year_actual\n",
    "        data_matrix_year.append(data[int(_iter):int(_iter_end)])                          \n",
    "        _iter+=year_size_data[i]\n",
    "            \n",
    "    return data_matrix_year\n",
    "                                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf032f8-1495-4dd9-9e43-6e222c01ba81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coordinates_region(geojson_file,region_choice):\n",
    "    \n",
    "        if geojson_file[region_choice].geom_type=='MultiPolygon':\n",
    "\n",
    "            coordinates = coord_lister(geojson_file[region_choice])\n",
    "            nb_polygons=len(coordinates)\n",
    "            \n",
    "            data_coords=[]\n",
    "            for j in range(nb_polygons):\n",
    "\n",
    "                coords = coordinates[j]\n",
    "                coords=np.round(coords,1)\n",
    "                min_lat=min(coords[:,1])\n",
    "                max_lat=max(coords[:,1])\n",
    "                lat_list_iter=np.arange(min_lat,max_lat,0.1)\n",
    "            \n",
    "                for latitude in lat_list_iter:\n",
    "\n",
    "                    idx_long=np.where(coords[:,1]==np.round(latitude,1))[0]\n",
    "                    long_values_at_iter_lat=coords[:,0][idx_long]\n",
    "                    long_list_iter=np.arange(min(long_values_at_iter_lat),max(long_values_at_iter_lat),0.1)\n",
    "                    \n",
    "                    for longitude in long_list_iter:\n",
    "                        \n",
    "                        data_coords.append([latitude,longitude])\n",
    "                # Long_list_department.append(long_list)\n",
    "                \n",
    "            # return np.hstack(Lat_list_department),Long_list_department\n",
    "            return data_coords\n",
    "\n",
    "        else:    \n",
    "            \n",
    "            data_coords=[]\n",
    "            coordinates = coord_lister(geojson_file[region_choice])\n",
    "            coordinates=np.round(coordinates,1)\n",
    "            min_lat=min(coordinates[:,1])\n",
    "            max_lat=max(coordinates[:,1])\n",
    "            lat_list=np.arange(min_lat,max_lat,0.1)\n",
    "            \n",
    "            for latitude in lat_list:\n",
    "\n",
    "                idx_long=np.where(coordinates[:,1]==np.round(latitude,1))[0]\n",
    "                long_values_at_iter_lat=coordinates[:,0][idx_long]\n",
    "                long_list_iter=np.arange(min(long_values_at_iter_lat),max(long_values_at_iter_lat),0.1)\n",
    "                 \n",
    "                for longitude in long_list_iter:\n",
    "                        \n",
    "                        data_coords.append([latitude,longitude])\n",
    "                        \n",
    "            return data_coords\n",
    "        \n",
    "def data_ready_model(data_coords,path_picked_files,data_feature):\n",
    "\n",
    "        data_variable=[]\n",
    "\n",
    "        for file in range(len(path_picked_files)):\n",
    "\n",
    "            main_data= netCDF4.Dataset(path_picked_files[file])\n",
    "            data=np.asarray(main_data.variables[data_feature]).T\n",
    "            # print(data.shape)\n",
    "            lat_data = np.asarray(main_data['latitude'])\n",
    "            long_data = np.asarray(main_data['longitude'])\n",
    "            meta_data_region_variable=[]\n",
    "            missing_data_coords_nb=0\n",
    "            for kk in range(len(data_coords)):\n",
    "                    \n",
    "                    latitude_iter=data_coords[kk][0]\n",
    "                    longitude_iter=data_coords[kk][1]\n",
    "                    ID_lat = np.where(lat_data==latitude_iter)[0]\n",
    "                    ID_long = np.where(long_data==longitude_iter)[0]\n",
    "                    if len(ID_lat)==0 or len(ID_long)==0:\n",
    "                        missing_data_coords_nb+=1\n",
    "                        continue\n",
    "                        \n",
    "                    data_at_coord=data[ID_long][0][ID_lat]\n",
    "                    \n",
    "                    if np.isnan(data_at_coord[0].any())==True:\n",
    "                        for i in range (len(data_at_coord)):\n",
    "                            if np.isnan(data_at_coord[0][i])==True:\n",
    "                                if i ==0:\n",
    "                                    data_at_coord[0][i]==data_at_coord[0][i+1]\n",
    "                                else:\n",
    "                                    data_at_coord[0][i]==data_at_coord[0][i-1]   \n",
    "                    if data_at_coord[0].any()==-32767:\n",
    "                        for i in range(len(data_at_coord)):\n",
    "                            if data_at_coord[0][i]==-32767:\n",
    "                                if i ==0:\n",
    "                                    data_at_coord[0][i]==data_at_coord[0][i+1]\n",
    "                                else:\n",
    "                                    data_at_coord[0][i]==data_at_coord[0][i-1]                    \n",
    "                    if data_at_coord[0].any()==0:\n",
    "                        for i in range(len(data_at_coord)):\n",
    "                            if data_at_coord[0][i]==0:\n",
    "                                if i ==0:\n",
    "                                    data_at_coord[0][i]==data_at_coord[0][i+1]\n",
    "                                else:\n",
    "                                    data_at_coord[0][i]==data_at_coord[0][i-1]   \n",
    "                                    \n",
    "                    meta_data_region_variable.append(data_at_coord[0])\n",
    "            \n",
    "            meta_data_region_variable=np.asarray(meta_data_region_variable)\n",
    "            nb_data_coords=len(data_coords)- missing_data_coords_nb\n",
    "            meta_data_region_variable=meta_data_region_variable.reshape((nb_data_coords,len(data_at_coord[0])))\n",
    "\n",
    "            data_variable=np.append(data_variable,np.mean(meta_data_region_variable,axis=0))\n",
    "            # print(data_variable.shape)\n",
    "            \n",
    "        return data_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3e11f0-d890-4028-9e7e-7b5f755d399c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaler(array):\n",
    "    scaler = MinMaxScaler()\n",
    "    array=array.reshape(-1,1)\n",
    "    scaler.fit(array)\n",
    "    array=scaler.transform(array)\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de3c571-998e-47c1-9f21-3f375184fad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def data_gen_region(data_features,geojson_file,region_choice,scaling,year_start,year_finish):\n",
    "    \n",
    "    #Picking the period of data we want \n",
    "    \n",
    "    path_picked_files,nb_years,nb_months,year_size_data=data_period_pick('era1/era5-land',year_start,year_finish,1,12)\n",
    "    data_region_year=[]           \n",
    "    data_coords=coordinates_region(geojson_file,region_choice)\n",
    "    \n",
    "    for variable in range(len(data_features)):\n",
    "        \n",
    "        data = data_ready_model(data_coords,path_picked_files,data_features[variable])\n",
    "        if scaling==True:\n",
    "            data=scaler(data)\n",
    "        \n",
    "        data_variable_region_year= data_year_sep(data,year_size_data)   \n",
    "        data_region_year.append(data_variable_region_year)\n",
    "        \n",
    "    return data_region_year,nb_months,nb_years,year_size_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64caad58-d4f0-4629-8ca4-98ac582d5de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_features=['u10', 'v10', 't2m', 'evabs', 'evatc', 'evavt', 'lai_hv', 'lai_lv', 'src', 'stl1', 'sp', 'e', 'tp', 'swvl1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bd0f18-a541-4f34-a903-b9575ab8c2e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Alsace</th>\n",
       "      <th>Bordelais</th>\n",
       "      <th>Bourgogne Beaujolais</th>\n",
       "      <th>Champagne</th>\n",
       "      <th>Charentes</th>\n",
       "      <th>Corse</th>\n",
       "      <th>Jura</th>\n",
       "      <th>Languedoc Roussillon</th>\n",
       "      <th>Savoie</th>\n",
       "      <th>Sud est</th>\n",
       "      <th>Sud ouest</th>\n",
       "      <th>Val de Loire</th>\n",
       "      <th>BBJS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000</td>\n",
       "      <td>1268,81</td>\n",
       "      <td>7227,31</td>\n",
       "      <td>3211,82</td>\n",
       "      <td>2742,79</td>\n",
       "      <td>8211,92</td>\n",
       "      <td>378</td>\n",
       "      <td>110,12</td>\n",
       "      <td>19958,96</td>\n",
       "      <td>148,43</td>\n",
       "      <td>7370,87</td>\n",
       "      <td>4545,93</td>\n",
       "      <td>3586,81</td>\n",
       "      <td>1514,83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001</td>\n",
       "      <td>1265,22</td>\n",
       "      <td>6913,59</td>\n",
       "      <td>3035,74</td>\n",
       "      <td>2338,92</td>\n",
       "      <td>8508,88</td>\n",
       "      <td>340</td>\n",
       "      <td>92,97</td>\n",
       "      <td>18156,23</td>\n",
       "      <td>135,9</td>\n",
       "      <td>6201,98</td>\n",
       "      <td>4183,71</td>\n",
       "      <td>3272,45</td>\n",
       "      <td>1490,98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2002</td>\n",
       "      <td>1264,14</td>\n",
       "      <td>5903,98</td>\n",
       "      <td>3007,01</td>\n",
       "      <td>2538,24</td>\n",
       "      <td>8486,97</td>\n",
       "      <td>75,69</td>\n",
       "      <td>89,26</td>\n",
       "      <td>16578,97</td>\n",
       "      <td>132,79</td>\n",
       "      <td>5950,69</td>\n",
       "      <td>3499,03</td>\n",
       "      <td>3376,16</td>\n",
       "      <td>1469,92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003</td>\n",
       "      <td>1015,31</td>\n",
       "      <td>5974,61</td>\n",
       "      <td>1983,34</td>\n",
       "      <td>1642,83</td>\n",
       "      <td>7731,28</td>\n",
       "      <td>299,26</td>\n",
       "      <td>72,18</td>\n",
       "      <td>15276,84</td>\n",
       "      <td>116,52</td>\n",
       "      <td>6169,92</td>\n",
       "      <td>3284,55</td>\n",
       "      <td>3019,23</td>\n",
       "      <td>1238,87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2004</td>\n",
       "      <td>1353,94</td>\n",
       "      <td>7968,91</td>\n",
       "      <td>3194,11</td>\n",
       "      <td>3278,85</td>\n",
       "      <td>10102,93</td>\n",
       "      <td>364,12</td>\n",
       "      <td>123,56</td>\n",
       "      <td>17781,25</td>\n",
       "      <td>151,38</td>\n",
       "      <td>6037,22</td>\n",
       "      <td>4251,09</td>\n",
       "      <td>3752,56</td>\n",
       "      <td>1616,55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2005</td>\n",
       "      <td>1195,26</td>\n",
       "      <td>6481,59</td>\n",
       "      <td>2792,99</td>\n",
       "      <td>3002,12</td>\n",
       "      <td>9459,55</td>\n",
       "      <td>351,15</td>\n",
       "      <td>96,88</td>\n",
       "      <td>15955,94</td>\n",
       "      <td>139,05</td>\n",
       "      <td>5765,12</td>\n",
       "      <td>3977,23</td>\n",
       "      <td>3338,56</td>\n",
       "      <td>1423,9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2006</td>\n",
       "      <td>1109,2</td>\n",
       "      <td>6081,5</td>\n",
       "      <td>2752,95</td>\n",
       "      <td>3080,24</td>\n",
       "      <td>9335,26</td>\n",
       "      <td>347,41</td>\n",
       "      <td>85,89</td>\n",
       "      <td>16313,25</td>\n",
       "      <td>131,76</td>\n",
       "      <td>5974,05</td>\n",
       "      <td>3858,65</td>\n",
       "      <td>3183,57</td>\n",
       "      <td>1312,73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2007</td>\n",
       "      <td>1205,22</td>\n",
       "      <td>5932,02</td>\n",
       "      <td>2625,39</td>\n",
       "      <td>2994,1</td>\n",
       "      <td>6482,06</td>\n",
       "      <td>311,73</td>\n",
       "      <td>102</td>\n",
       "      <td>14656,93</td>\n",
       "      <td>117,64</td>\n",
       "      <td>5718,29</td>\n",
       "      <td>3063,79</td>\n",
       "      <td>2890,39</td>\n",
       "      <td>1415,8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2008</td>\n",
       "      <td>1169,27</td>\n",
       "      <td>4898,15</td>\n",
       "      <td>2279,44</td>\n",
       "      <td>3026,11</td>\n",
       "      <td>6977,02</td>\n",
       "      <td>292,13</td>\n",
       "      <td>93,4</td>\n",
       "      <td>12984,83</td>\n",
       "      <td>108,58</td>\n",
       "      <td>5137,99</td>\n",
       "      <td>2814,93</td>\n",
       "      <td>2261,31</td>\n",
       "      <td>1383,23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2009</td>\n",
       "      <td>1214,82</td>\n",
       "      <td>6069,09</td>\n",
       "      <td>2550,19</td>\n",
       "      <td>2893,92</td>\n",
       "      <td>8656,6</td>\n",
       "      <td>291,93</td>\n",
       "      <td>82,8</td>\n",
       "      <td>12135,64</td>\n",
       "      <td>120,56</td>\n",
       "      <td>5130,18</td>\n",
       "      <td>3735,27</td>\n",
       "      <td>3159,11</td>\n",
       "      <td>1418,96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2010</td>\n",
       "      <td>921,67</td>\n",
       "      <td>5983,02</td>\n",
       "      <td>2305,89</td>\n",
       "      <td>2478,76</td>\n",
       "      <td>8719,14</td>\n",
       "      <td>335,75</td>\n",
       "      <td>89,68</td>\n",
       "      <td>11966,2</td>\n",
       "      <td>121,34</td>\n",
       "      <td>5090,39</td>\n",
       "      <td>3464,76</td>\n",
       "      <td>3152,11</td>\n",
       "      <td>1137,844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2011</td>\n",
       "      <td>1201,247</td>\n",
       "      <td>6187,886</td>\n",
       "      <td>2591,061</td>\n",
       "      <td>3028,771</td>\n",
       "      <td>9289,304</td>\n",
       "      <td>310,959</td>\n",
       "      <td>117,5</td>\n",
       "      <td>14276,043</td>\n",
       "      <td>126,494</td>\n",
       "      <td>5642,209</td>\n",
       "      <td>4036,447</td>\n",
       "      <td>3035,119</td>\n",
       "      <td>1429,637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2012</td>\n",
       "      <td>1195,69</td>\n",
       "      <td>5609,83</td>\n",
       "      <td>1849,13</td>\n",
       "      <td>1998,61</td>\n",
       "      <td>7663,55</td>\n",
       "      <td>311,82</td>\n",
       "      <td>74,9</td>\n",
       "      <td>12202,38</td>\n",
       "      <td>110,89</td>\n",
       "      <td>5044,07</td>\n",
       "      <td>3415,99</td>\n",
       "      <td>2015,12</td>\n",
       "      <td>1372,531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2013</td>\n",
       "      <td>993,298</td>\n",
       "      <td>3964,662</td>\n",
       "      <td>2055,637</td>\n",
       "      <td>2854,463</td>\n",
       "      <td>7876,901</td>\n",
       "      <td>347,66</td>\n",
       "      <td>58,293</td>\n",
       "      <td>13579,982</td>\n",
       "      <td>101,941</td>\n",
       "      <td>4675,3</td>\n",
       "      <td>2615,418</td>\n",
       "      <td>2630,461</td>\n",
       "      <td>1158,455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2014</td>\n",
       "      <td>1030,9233</td>\n",
       "      <td>5642,354</td>\n",
       "      <td>2493,013</td>\n",
       "      <td>2886,295</td>\n",
       "      <td>8598,123</td>\n",
       "      <td>328,856</td>\n",
       "      <td>75,6</td>\n",
       "      <td>12707,8</td>\n",
       "      <td>106,864</td>\n",
       "      <td>6125,698</td>\n",
       "      <td>3423</td>\n",
       "      <td>2738,443</td>\n",
       "      <td>1217,5233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2015</td>\n",
       "      <td>1018,283</td>\n",
       "      <td>5672,26</td>\n",
       "      <td>2284,321</td>\n",
       "      <td>2500,2</td>\n",
       "      <td>9635,244</td>\n",
       "      <td>353,26</td>\n",
       "      <td>80,115</td>\n",
       "      <td>13649,75</td>\n",
       "      <td>111</td>\n",
       "      <td>5594</td>\n",
       "      <td>3526,97</td>\n",
       "      <td>2779</td>\n",
       "      <td>1217,67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2016</td>\n",
       "      <td>1230,18</td>\n",
       "      <td>6700,23</td>\n",
       "      <td>2065,773</td>\n",
       "      <td>2076,73</td>\n",
       "      <td>7820,91</td>\n",
       "      <td>350,618</td>\n",
       "      <td>94,012</td>\n",
       "      <td>12362,235</td>\n",
       "      <td>119,272</td>\n",
       "      <td>5797,492</td>\n",
       "      <td>4060,339</td>\n",
       "      <td>2173,538</td>\n",
       "      <td>1421,653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2017</td>\n",
       "      <td>919,044</td>\n",
       "      <td>3638,261</td>\n",
       "      <td>2212,899</td>\n",
       "      <td>2237,789</td>\n",
       "      <td>6862,562</td>\n",
       "      <td>287,517</td>\n",
       "      <td>46,592</td>\n",
       "      <td>10429,142</td>\n",
       "      <td>97,461</td>\n",
       "      <td>4267,969</td>\n",
       "      <td>2957,179</td>\n",
       "      <td>2224,02</td>\n",
       "      <td>1099,056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2018</td>\n",
       "      <td>1293,487</td>\n",
       "      <td>5533,976</td>\n",
       "      <td>29660,12</td>\n",
       "      <td>3416,789</td>\n",
       "      <td>9933,363</td>\n",
       "      <td>337,822</td>\n",
       "      <td>131,516</td>\n",
       "      <td>12658,994</td>\n",
       "      <td>133,42</td>\n",
       "      <td>5083,678</td>\n",
       "      <td>3865,66</td>\n",
       "      <td>3376,05</td>\n",
       "      <td>1529,333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2019</td>\n",
       "      <td>1052</td>\n",
       "      <td>5296,31</td>\n",
       "      <td>1904</td>\n",
       "      <td>2411,1</td>\n",
       "      <td>7871,23</td>\n",
       "      <td>285,59</td>\n",
       "      <td>54,03</td>\n",
       "      <td>12148,82</td>\n",
       "      <td>104,33</td>\n",
       "      <td>5171,117</td>\n",
       "      <td>3347,5</td>\n",
       "      <td>2201,85</td>\n",
       "      <td>1221,41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2020</td>\n",
       "      <td>1013,35</td>\n",
       "      <td>5000</td>\n",
       "      <td>2359,92</td>\n",
       "      <td>2010,23</td>\n",
       "      <td>10882,53</td>\n",
       "      <td>352,4</td>\n",
       "      <td>91,77</td>\n",
       "      <td>12553,117</td>\n",
       "      <td>115,38</td>\n",
       "      <td>5414,82</td>\n",
       "      <td>3572,779</td>\n",
       "      <td>2852,49</td>\n",
       "      <td>1183,347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2021</td>\n",
       "      <td>818,9</td>\n",
       "      <td>4127,93</td>\n",
       "      <td>1584,17</td>\n",
       "      <td>1579,65</td>\n",
       "      <td>9631,26</td>\n",
       "      <td>318,96</td>\n",
       "      <td>33,431</td>\n",
       "      <td>9680,409</td>\n",
       "      <td>78,227</td>\n",
       "      <td>4793,2</td>\n",
       "      <td>2834,588</td>\n",
       "      <td>1827,75</td>\n",
       "      <td>955,864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2022</td>\n",
       "      <td>907,243</td>\n",
       "      <td>4503,6</td>\n",
       "      <td>2554,29</td>\n",
       "      <td>3135,9</td>\n",
       "      <td>9565,2</td>\n",
       "      <td>343</td>\n",
       "      <td>116,522</td>\n",
       "      <td>13182,891</td>\n",
       "      <td>103,533</td>\n",
       "      <td>5260,721</td>\n",
       "      <td>2612,25</td>\n",
       "      <td>2427,523</td>\n",
       "      <td>1023,765</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0     Alsace Bordelais Bourgogne Beaujolais Champagne Charentes   \n",
       "0         2000    1268,81   7227,31              3211,82   2742,79   8211,92  \\\n",
       "1         2001    1265,22   6913,59              3035,74   2338,92   8508,88   \n",
       "2         2002    1264,14   5903,98              3007,01   2538,24   8486,97   \n",
       "3         2003    1015,31   5974,61              1983,34   1642,83   7731,28   \n",
       "4         2004    1353,94   7968,91              3194,11   3278,85  10102,93   \n",
       "5         2005    1195,26   6481,59              2792,99   3002,12   9459,55   \n",
       "6         2006     1109,2    6081,5              2752,95   3080,24   9335,26   \n",
       "7         2007    1205,22   5932,02              2625,39    2994,1   6482,06   \n",
       "8         2008    1169,27   4898,15              2279,44   3026,11   6977,02   \n",
       "9         2009    1214,82   6069,09              2550,19   2893,92    8656,6   \n",
       "10        2010     921,67   5983,02              2305,89   2478,76   8719,14   \n",
       "11        2011   1201,247  6187,886             2591,061  3028,771  9289,304   \n",
       "12        2012    1195,69   5609,83              1849,13   1998,61   7663,55   \n",
       "13        2013    993,298  3964,662             2055,637  2854,463  7876,901   \n",
       "14        2014  1030,9233  5642,354             2493,013  2886,295  8598,123   \n",
       "15        2015   1018,283   5672,26             2284,321    2500,2  9635,244   \n",
       "16        2016    1230,18   6700,23             2065,773   2076,73   7820,91   \n",
       "17        2017    919,044  3638,261             2212,899  2237,789  6862,562   \n",
       "18        2018   1293,487  5533,976             29660,12  3416,789  9933,363   \n",
       "19        2019       1052   5296,31                 1904    2411,1   7871,23   \n",
       "20        2020    1013,35      5000              2359,92   2010,23  10882,53   \n",
       "21        2021      818,9   4127,93              1584,17   1579,65   9631,26   \n",
       "22        2022    907,243    4503,6              2554,29    3135,9    9565,2   \n",
       "\n",
       "      Corse     Jura Languedoc Roussillon   Savoie   Sud est Sud ouest   \n",
       "0       378   110,12             19958,96   148,43   7370,87   4545,93  \\\n",
       "1       340    92,97             18156,23    135,9   6201,98   4183,71   \n",
       "2     75,69    89,26             16578,97   132,79   5950,69   3499,03   \n",
       "3    299,26    72,18             15276,84   116,52   6169,92   3284,55   \n",
       "4    364,12   123,56             17781,25   151,38   6037,22   4251,09   \n",
       "5    351,15    96,88             15955,94   139,05   5765,12   3977,23   \n",
       "6    347,41    85,89             16313,25   131,76   5974,05   3858,65   \n",
       "7    311,73      102             14656,93   117,64   5718,29   3063,79   \n",
       "8    292,13     93,4             12984,83   108,58   5137,99   2814,93   \n",
       "9    291,93     82,8             12135,64   120,56   5130,18   3735,27   \n",
       "10   335,75    89,68              11966,2   121,34   5090,39   3464,76   \n",
       "11  310,959    117,5            14276,043  126,494  5642,209  4036,447   \n",
       "12   311,82     74,9             12202,38   110,89   5044,07   3415,99   \n",
       "13   347,66   58,293            13579,982  101,941    4675,3  2615,418   \n",
       "14  328,856     75,6              12707,8  106,864  6125,698      3423   \n",
       "15   353,26   80,115             13649,75      111      5594   3526,97   \n",
       "16  350,618   94,012            12362,235  119,272  5797,492  4060,339   \n",
       "17  287,517   46,592            10429,142   97,461  4267,969  2957,179   \n",
       "18  337,822  131,516            12658,994   133,42  5083,678   3865,66   \n",
       "19   285,59    54,03             12148,82   104,33  5171,117    3347,5   \n",
       "20    352,4    91,77            12553,117   115,38   5414,82  3572,779   \n",
       "21   318,96   33,431             9680,409   78,227    4793,2  2834,588   \n",
       "22      343  116,522            13182,891  103,533  5260,721   2612,25   \n",
       "\n",
       "   Val de Loire       BBJS  \n",
       "0       3586,81    1514,83  \n",
       "1       3272,45    1490,98  \n",
       "2       3376,16    1469,92  \n",
       "3       3019,23    1238,87  \n",
       "4       3752,56    1616,55  \n",
       "5       3338,56     1423,9  \n",
       "6       3183,57    1312,73  \n",
       "7       2890,39     1415,8  \n",
       "8       2261,31    1383,23  \n",
       "9       3159,11    1418,96  \n",
       "10      3152,11   1137,844  \n",
       "11     3035,119   1429,637  \n",
       "12      2015,12   1372,531  \n",
       "13     2630,461   1158,455  \n",
       "14     2738,443  1217,5233  \n",
       "15         2779    1217,67  \n",
       "16     2173,538   1421,653  \n",
       "17      2224,02   1099,056  \n",
       "18      3376,05   1529,333  \n",
       "19      2201,85    1221,41  \n",
       "20      2852,49   1183,347  \n",
       "21      1827,75    955,864  \n",
       "22     2427,523   1023,765  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "production=pd.read_csv('Production_viticole_france_2000_2022.csv')\n",
    "production "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7bb1b7-443b-4a62-83fc-bcd6ef7108cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_labels(years_data,production,regions):\n",
    "    \n",
    "    #Méthode classification binaire\n",
    "    binary_labels=[]\n",
    "    \n",
    "    for region in regions:\n",
    "        \n",
    "        prod_region=production[region]\n",
    "        for value in range(len(prod_region)):\n",
    "            \n",
    "            prod_region[value]=prod_region[value].replace(',','.')\n",
    "        prod_region=scaler(np.asarray(prod_region).astype(float))\n",
    "        \n",
    "        #Valeur de référence pour la définition des classes\n",
    "        valeur_olympique=np.mean(prod_region)  \n",
    "        \n",
    "        for year in years_data:\n",
    "            \n",
    "            year_ref=np.where(production['Unnamed: 0']==year)\n",
    "            \n",
    "            if prod_region[year_ref]<valeur_olympique:\n",
    "                binary_labels.append(int(1))\n",
    "            if prod_region[year_ref]>valeur_olympique:\n",
    "                binary_labels.append(int(0))\n",
    "            \n",
    "    # Méthode multi classification\n",
    "                             \n",
    "    return binary_labels           \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b5a617-54a3-4455-abd6-77e6dc13aff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_labels2(years_data,production,regions,portion):\n",
    "    \n",
    "    #Méthode classification binaire\n",
    "    binary_labels=[]\n",
    "    \n",
    "    for region in regions:\n",
    "        \n",
    "        prod_region=production[region]\n",
    "        for value in range(len(prod_region)):\n",
    "            \n",
    "            prod_region[value]=prod_region[value].replace(',','.')\n",
    "            \n",
    "        prod_region=scaler(np.asarray(prod_region).astype(float))\n",
    "        prod_ready_region=np.delete(prod_region,np.argmax(prod_region))\n",
    "        prod_ready_region=np.delete(prod_ready_region,np.argmin(prod_ready_region))\n",
    "        \n",
    "        #Valeur de référence pour la définition des classes\n",
    "        valeur_olympique=np.mean(prod_ready_region)*portion  \n",
    "        \n",
    "        for year in years_data:\n",
    "            \n",
    "            year_ref=np.where(production['Unnamed: 0']==year)\n",
    "            \n",
    "            if prod_region[year_ref]<valeur_olympique:\n",
    "                binary_labels.append(int(1))\n",
    "            if prod_region[year_ref]>valeur_olympique:\n",
    "                binary_labels.append(int(0))\n",
    "            \n",
    "    # Méthode multi classification\n",
    "                             \n",
    "    return binary_labels  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6992e2e-0bab-46ac-9f80-6828677a879d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering dataset by region\n",
    "\n",
    "\n",
    "\n",
    "def region_filter(region_choice,nb_years,data_matrix,labels,day_nbs):\n",
    "    \n",
    "    nb_examples=(len(region_choice)*(nb_years))\n",
    "    new_data_matrix=np.zeros((nb_examples,int(min(year_size_data)/day_nbs),len(data_features)))\n",
    "    new_labels = []                            \n",
    "\n",
    "    region_idx_start=0\n",
    "    region_idx_end=nb_years-1\n",
    "    \n",
    "    \n",
    "    end=nb_years-1\n",
    "    start=0\n",
    "    for region in region_choice:\n",
    "        \n",
    "        idx_start=(region*nb_years)\n",
    "        idx_end=(region*nb_years)+nb_years-1\n",
    "        new_data_matrix[region_idx_start:region_idx_end+1]=data_matrix[idx_start:idx_end+1]\n",
    "        new_labels.append(labels[idx_start:idx_end+1])\n",
    "        \n",
    "\n",
    "        region_idx_start+=nb_years\n",
    "        region_idx_end+=nb_years   \n",
    "        print(idx_start,idx_end)\n",
    "    return new_data_matrix,np.hstack(new_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91a14c3-deec-492c-81bb-b73d2c1df971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def period_means(arr,year_size_data,choice):\n",
    "    \n",
    "    if choice=='day':\n",
    "    \n",
    "        day_means=[]\n",
    "        _iter=0\n",
    "        while _iter < year_size_data :\n",
    "            day_actual=[]\n",
    "            for hour in range(24):\n",
    "                day_actual.append(arr[_iter])\n",
    "                _iter+=1\n",
    "            day_means.append(np.mean(day_actual))\n",
    "                \n",
    "#     if choice=='month':\n",
    "#          month_means=[]\n",
    "#         _iter=0\n",
    "#         while _iter < month_size_data:\n",
    "#             month_actual=[]\n",
    "            \n",
    "#             for hour in range(24):\n",
    "                \n",
    "#                 _iter+=1\n",
    "#                 month_actual.append(arr[_iter])\n",
    "#             month_means.append(np.mean(month_actual))\n",
    "        \n",
    "        return day_means       \n",
    "   # return day_means,month_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0943dd-2340-46a2-94bc-538cac0dfe95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is ordered well\n",
      "Region : ALSACE ET EST\n",
      "Region : BOURGOGNE BEAUJOLAIS SAVOIE JURA\n",
      "Region : CHAMPAGNE\n",
      "Region : LANGUEDOC-ROUSSILLON\n",
      "Region : SUD-OUEST\n",
      "Region : VAL DE LOIRE\n",
      "Region : Gironde\n",
      "Region : Charente\n",
      "(184, 14, 365)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "year_start=2000\n",
    "year_finish=2022\n",
    "\n",
    "\n",
    "#Picking all the regions related to the regions of agreste productivity data\n",
    "\n",
    "\n",
    "French_wine_region_choice=[3,4,5,6,8,10]\n",
    "# ALSACE ET EST,BOURGOGNE BEAUJOLAIS SAVOIE JURA,CHAMPAGNE,LANGUEDOC-ROUSSILLON,Sud Ouest, Val de Loire\n",
    "\n",
    "French_departments_choice=[54,71]\n",
    "# Gironde(pour bordeaux), Charentes\n",
    "\n",
    "years_data=np.arange(year_start,year_finish+1,1)\n",
    "nb_years=len(years_data)\n",
    "nb_features=len(data_features)\n",
    "nb_examples= (len(French_wine_region_choice)+len(French_departments_choice))*nb_years\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "idx_match=[1,-1,4,8,11,12,2,5]\n",
    "\n",
    "region_choice_match_production=np.asarray(production.columns)[idx_match]\n",
    "\n",
    "binary_labels_pdp=gen_labels(years_data,production,region_choice_match_production)  \n",
    "\n",
    "path_picked_files,_,_,year_size_data=data_period_pick('era1/era5-land',year_start,year_finish,1,12)\n",
    "\n",
    "\n",
    "#Checking if meta_data is ordered\n",
    "time_=[]\n",
    "for file in path_picked_files:\n",
    "    \n",
    "    main_data= netCDF4.Dataset(file)\n",
    "    data=np.asarray(main_data.variables['time'])\n",
    "    time_.append(data)\n",
    "    \n",
    "time_=np.hstack(time_)\n",
    "if all(b >= a for a, b in zip(time_, time_[1:]))==True:\n",
    "    print('Data is ordered well')\n",
    "    \n",
    "    \n",
    "data_matrix_conv = np.zeros((nb_examples,nb_features,int(min(year_size_data)/24)))\n",
    "\n",
    "idx_region_year_data=0                       \n",
    "\n",
    "for region in French_wine_region_choice:\n",
    "    print('Region :', French_wine_regions['Bassin'][region])\n",
    "\n",
    "    \n",
    "    data_region,_,_,year_size_data=data_gen_region(data_features,French_wine_regions['geometry'],region,False,year_start,year_finish)\n",
    "    \n",
    "    for year in range(nb_years):\n",
    "        \n",
    "        for variable in range(len(data_features)):\n",
    "        \n",
    "            data_matrix_conv[idx_region_year_data][variable]=period_means(data_region[variable][year],min(year_size_data),'day')\n",
    "            \n",
    "        idx_region_year_data+=1 \n",
    "            \n",
    "\n",
    "for region in French_departments_choice:\n",
    "    \n",
    "    print('Region :', French_departments['nom'][region])\n",
    "\n",
    "    \n",
    "    data_region,_,_,year_size_data=data_gen_region(data_features,French_departments['geometry'],region,False,year_start,year_finish)\n",
    "    \n",
    "    for year in range(nb_years):\n",
    "        \n",
    "        for variable in range(len(data_features)):\n",
    "        \n",
    "            data_matrix_conv[idx_region_year_data][variable]=period_means(data_region[variable][year],min(year_size_data),'day')\n",
    "            \n",
    "        idx_region_year_data+=1 \n",
    "\n",
    "    \n",
    "print(data_matrix_conv.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81aa036e-4c56-40d2-bc59-1dde408ab5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce9ae75-ff61-497c-be88-ac82325f250d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Saving and retrieving processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8566cc60-817b-4fe3-9454-350f95dadda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving data matrix for later use\n",
    "\n",
    "arr_reshaped = data_matrix_conv.reshape(data_matrix_conv.shape[0], -1)\n",
    "# saving reshaped array to file.\n",
    "np.savetxt(\"Conv_matrix_8reg_22y_unscaled.txt\", arr_reshaped)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8c5d6d-b0bf-458e-9336-7847b1334c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieving data from file.\n",
    "data_matrix_conv2D = np.loadtxt(\"Conv_matrix_8reg_22y_scaled.txt\")\n",
    "  \n",
    "# This loadedArr is a 2D array, therefore\n",
    "# we need to convert it to the original\n",
    "# array shape.reshaping to get original\n",
    "# matrice with original shape.\n",
    "data_matrix_conv= data_matrix_conv2D.reshape(\n",
    "    data_matrix_conv2D.shape[0], data_matrix_conv2D.shape[1] // data_matrix_conv.shape[2], data_matrix_conv.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b83e44b-47c8-4279-b2fb-14ccce763c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(184, 365, 14)\n"
     ]
    }
   ],
   "source": [
    "print(data_matrix_conv.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580a6892-aa20-461e-b2fa-67f1008fccc9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Calculating of the weights of the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de05191-8482-4731-81c7-7b5e6ca0260e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating initial weights for each class\n",
    "\n",
    "\n",
    "zero_class_nb=0\n",
    "one_class_nb=0\n",
    "for i in range(len(binary_labels_pdp)):\n",
    "    if binary_labels_pdp[i]==0:\n",
    "        \n",
    "        zero_class_nb+=1\n",
    "    else:\n",
    "        one_class_nb+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01630d89-57c9-4f25-9ba3-72abd37176e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 1.0\n"
     ]
    }
   ],
   "source": [
    "weight_for_0 = (1 / zero_class_nb) * (len(binary_labels_pdp) / 2.0)\n",
    "weight_for_1 = (1 / one_class_nb) * (len(binary_labels_pdp) / 2.0)\n",
    "print(weight_for_0,weight_for_1)\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f836d53-eb21-4c1d-91aa-50852d1834aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Modèles et scores de classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74964913-f16c-4f5b-b395-72db4fd33a52",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Réseau de neurones Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd42bcc-7458-4ec9-854f-7b2787fa2229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_matrix_conv_saved_unscaled=data_matrix_conv_saved_unscaled.reshape((nb_examples,int(min(year_size_data)/24),len(data_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497e8c3b-7b66-4f58-af0b-b823d5860bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_matrix_conv=data_matrix_conv.reshape((nb_examples,int(min(year_size_data)/24),len(data_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2b4a96-07f5-4c7c-94ad-24bf20fc1881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from keras.layers import Conv1D, Conv2D,Dense, Dropout, Input, Concatenate, GlobalMaxPooling1D, BatchNormalization\n",
    "from keras.layers.convolutional import MaxPooling1D,AveragePooling1D,MaxPooling2D\n",
    "import tensorflow.keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Activation, Concatenate\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import h5py\n",
    "from keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe1e5df-ebcf-45f5-8204-2049efa6dfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25db8ae1-86d1-4087-b126-7ae8e9f43b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_state=np.random.randint(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375f735d-bde5-4b46-8977-8a717c088c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 22\n",
      "23 45\n",
      "92 114\n",
      "115 137\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(92, 365, 12)"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Filtering dataset by features and regions\n",
    "\n",
    "year_start=2000\n",
    "year_finish=2022\n",
    "\n",
    "years_data=np.arange(year_start,year_finish+1,1)\n",
    "\n",
    "\n",
    "idx_match=[1,-1,4,8,11,12,2,5]\n",
    "\n",
    "#best regions filtered\n",
    "region_choice=[0,1,4,5]\n",
    "\n",
    "# region_choice=[0,1,2,3,4,5,6,7]\n",
    "# ALSACE ET EST,BOURGOGNE BEAUJOLAIS SAVOIE JURA,CHAMPAGNE,LANGUEDOC-ROUSSILLON,Sud Ouest, Val de Loire,Gironde(pour bordeaux), Charentes\n",
    "\n",
    "\n",
    "region_choice_match_production=np.asarray(production.columns)[idx_match]\n",
    "nb_years=23\n",
    "binary_labels_pdp=gen_labels(years_data,production,region_choice_match_production)\n",
    "\n",
    "\n",
    "data_matrix,labels=region_filter(region_choice,nb_years,data_matrix_conv,binary_labels_pdp,24)\n",
    "\n",
    "\n",
    "\n",
    "# ['u10','v10','t2m', 'evabs', 'evatc', 'evavt', 'lai_hv', 'lai_lv', 'src', 'stl1', 'sp', 'e', 'tp', 'swvl1']\n",
    "# feature_choice=[0,1,2,3,4,5,6,7,8,9,10,11,12,13]\n",
    "feature_choice=[0,1,2,3,4,5,8,9,10,11,12,13]\n",
    "\n",
    "# feature_choice=[0,1,2,3,4,5,6,7,8,9,10,11,12,13]\n",
    "\n",
    "\n",
    "# feature_choice=[0,2,10,11,12,13]\n",
    "\n",
    "data_matrix=data_matrix.T[feature_choice]\n",
    "data_matrix=data_matrix.T\n",
    "data_matrix.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dc431d-161a-45c5-9bf1-d9c346184020",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "17/17 [==============================] - 2s 78ms/step - loss: 0.8235 - binary_accuracy: 0.4853 - val_loss: 0.6913 - val_binary_accuracy: 0.5217\n",
      "Epoch 2/50\n",
      "17/17 [==============================] - 1s 68ms/step - loss: 0.7388 - binary_accuracy: 0.4853 - val_loss: 0.7024 - val_binary_accuracy: 0.4783\n",
      "Epoch 3/50\n",
      "17/17 [==============================] - 1s 67ms/step - loss: 0.7131 - binary_accuracy: 0.5294 - val_loss: 0.6936 - val_binary_accuracy: 0.4783\n",
      "Epoch 4/50\n",
      "17/17 [==============================] - 1s 58ms/step - loss: 0.7010 - binary_accuracy: 0.4706 - val_loss: 0.6945 - val_binary_accuracy: 0.4783\n",
      "Epoch 5/50\n",
      "17/17 [==============================] - 1s 65ms/step - loss: 0.6989 - binary_accuracy: 0.4853 - val_loss: 0.6960 - val_binary_accuracy: 0.4783\n",
      "Epoch 6/50\n",
      "17/17 [==============================] - 1s 67ms/step - loss: 0.7161 - binary_accuracy: 0.4853 - val_loss: 0.6930 - val_binary_accuracy: 0.4783\n",
      "Epoch 7/50\n",
      "17/17 [==============================] - 1s 64ms/step - loss: 0.7015 - binary_accuracy: 0.5147 - val_loss: 0.7026 - val_binary_accuracy: 0.4783\n",
      "Epoch 8/50\n",
      "17/17 [==============================] - 1s 64ms/step - loss: 0.6998 - binary_accuracy: 0.4412 - val_loss: 0.6940 - val_binary_accuracy: 0.4783\n",
      "Epoch 9/50\n",
      "17/17 [==============================] - 1s 62ms/step - loss: 0.7067 - binary_accuracy: 0.5147 - val_loss: 0.6927 - val_binary_accuracy: 0.4783\n",
      "Epoch 10/50\n",
      "17/17 [==============================] - 1s 62ms/step - loss: 0.6968 - binary_accuracy: 0.5294 - val_loss: 0.7043 - val_binary_accuracy: 0.4783\n",
      "Epoch 11/50\n",
      "17/17 [==============================] - 1s 63ms/step - loss: 0.7020 - binary_accuracy: 0.5441 - val_loss: 0.7220 - val_binary_accuracy: 0.4783\n",
      "Epoch 12/50\n",
      "17/17 [==============================] - 1s 62ms/step - loss: 0.6933 - binary_accuracy: 0.5882 - val_loss: 0.6994 - val_binary_accuracy: 0.4783\n",
      "Epoch 13/50\n",
      "17/17 [==============================] - 1s 63ms/step - loss: 0.6808 - binary_accuracy: 0.6912 - val_loss: 0.6844 - val_binary_accuracy: 0.6522\n",
      "Epoch 14/50\n",
      "17/17 [==============================] - 1s 69ms/step - loss: 0.6851 - binary_accuracy: 0.6471 - val_loss: 0.6734 - val_binary_accuracy: 0.9130\n",
      "Epoch 15/50\n",
      "17/17 [==============================] - 1s 68ms/step - loss: 0.6246 - binary_accuracy: 0.7647 - val_loss: 0.6186 - val_binary_accuracy: 0.6522\n",
      "Epoch 16/50\n",
      "17/17 [==============================] - 1s 69ms/step - loss: 0.5089 - binary_accuracy: 0.8088 - val_loss: 0.4813 - val_binary_accuracy: 0.7826\n",
      "Epoch 17/50\n",
      "17/17 [==============================] - 1s 70ms/step - loss: 0.2409 - binary_accuracy: 0.9706 - val_loss: 0.3156 - val_binary_accuracy: 0.9565\n",
      "Epoch 18/50\n",
      "17/17 [==============================] - 1s 62ms/step - loss: 0.0728 - binary_accuracy: 1.0000 - val_loss: 0.1965 - val_binary_accuracy: 0.9565\n",
      "Epoch 19/50\n",
      "17/17 [==============================] - 1s 64ms/step - loss: 0.0215 - binary_accuracy: 1.0000 - val_loss: 0.2949 - val_binary_accuracy: 0.9130\n",
      "Epoch 20/50\n",
      "17/17 [==============================] - 1s 62ms/step - loss: 0.0160 - binary_accuracy: 1.0000 - val_loss: 0.2224 - val_binary_accuracy: 0.8696\n",
      "Epoch 21/50\n",
      "17/17 [==============================] - 1s 63ms/step - loss: 0.0020 - binary_accuracy: 1.0000 - val_loss: 0.3498 - val_binary_accuracy: 0.8696\n",
      "Epoch 22/50\n",
      "17/17 [==============================] - 1s 68ms/step - loss: 6.4139e-04 - binary_accuracy: 1.0000 - val_loss: 0.1957 - val_binary_accuracy: 0.9130\n",
      "Epoch 23/50\n",
      "17/17 [==============================] - 1s 62ms/step - loss: 8.5522e-04 - binary_accuracy: 1.0000 - val_loss: 0.4739 - val_binary_accuracy: 0.8696\n",
      "Epoch 24/50\n",
      "17/17 [==============================] - 1s 64ms/step - loss: 3.2856e-04 - binary_accuracy: 1.0000 - val_loss: 0.4299 - val_binary_accuracy: 0.8696\n",
      "Epoch 25/50\n",
      "17/17 [==============================] - 1s 69ms/step - loss: 1.0424e-04 - binary_accuracy: 1.0000 - val_loss: 0.4025 - val_binary_accuracy: 0.8696\n",
      "Epoch 26/50\n",
      "17/17 [==============================] - 1s 67ms/step - loss: 8.7435e-05 - binary_accuracy: 1.0000 - val_loss: 0.3208 - val_binary_accuracy: 0.8696\n",
      "Epoch 27/50\n",
      "17/17 [==============================] - 1s 61ms/step - loss: 3.9029e-05 - binary_accuracy: 1.0000 - val_loss: 0.3451 - val_binary_accuracy: 0.8696\n",
      "Epoch 28/50\n",
      "17/17 [==============================] - 1s 67ms/step - loss: 2.2257e-05 - binary_accuracy: 1.0000 - val_loss: 0.4032 - val_binary_accuracy: 0.8696\n",
      "Epoch 29/50\n",
      "17/17 [==============================] - 1s 67ms/step - loss: 1.8092e-05 - binary_accuracy: 1.0000 - val_loss: 0.3868 - val_binary_accuracy: 0.8696\n",
      "Epoch 30/50\n",
      "17/17 [==============================] - 1s 67ms/step - loss: 1.7321e-05 - binary_accuracy: 1.0000 - val_loss: 0.3952 - val_binary_accuracy: 0.8696\n",
      "Epoch 31/50\n",
      "17/17 [==============================] - 1s 67ms/step - loss: 4.2060e-05 - binary_accuracy: 1.0000 - val_loss: 0.2476 - val_binary_accuracy: 0.9565\n",
      "Epoch 32/50\n",
      "17/17 [==============================] - 1s 62ms/step - loss: 1.5187e-05 - binary_accuracy: 1.0000 - val_loss: 0.3081 - val_binary_accuracy: 0.9130\n",
      "Epoch 33/50\n",
      "17/17 [==============================] - 1s 63ms/step - loss: 1.6106e-05 - binary_accuracy: 1.0000 - val_loss: 0.3677 - val_binary_accuracy: 0.9130\n",
      "Epoch 34/50\n",
      "17/17 [==============================] - 1s 64ms/step - loss: 2.6943e-06 - binary_accuracy: 1.0000 - val_loss: 0.3699 - val_binary_accuracy: 0.9130\n",
      "Epoch 35/50\n",
      "17/17 [==============================] - 1s 63ms/step - loss: 2.8631e-06 - binary_accuracy: 1.0000 - val_loss: 0.3669 - val_binary_accuracy: 0.9130\n",
      "Epoch 36/50\n",
      "17/17 [==============================] - 1s 58ms/step - loss: 1.5130e-05 - binary_accuracy: 1.0000 - val_loss: 0.3768 - val_binary_accuracy: 0.8696\n",
      "Epoch 37/50\n",
      "17/17 [==============================] - 1s 63ms/step - loss: 4.2947e-06 - binary_accuracy: 1.0000 - val_loss: 0.3907 - val_binary_accuracy: 0.8696\n",
      "Epoch 38/50\n",
      "17/17 [==============================] - 1s 64ms/step - loss: 2.4973e-06 - binary_accuracy: 1.0000 - val_loss: 0.3936 - val_binary_accuracy: 0.8696\n",
      "Epoch 39/50\n",
      "17/17 [==============================] - 1s 62ms/step - loss: 2.1289e-05 - binary_accuracy: 1.0000 - val_loss: 0.3096 - val_binary_accuracy: 0.9130\n",
      "Epoch 40/50\n",
      "17/17 [==============================] - 1s 63ms/step - loss: 1.8518e-05 - binary_accuracy: 1.0000 - val_loss: 0.4012 - val_binary_accuracy: 0.8696\n",
      "Epoch 41/50\n",
      "17/17 [==============================] - 1s 63ms/step - loss: 5.3754e-06 - binary_accuracy: 1.0000 - val_loss: 0.4241 - val_binary_accuracy: 0.8696\n",
      "Epoch 42/50\n",
      "17/17 [==============================] - 1s 63ms/step - loss: 2.9333e-06 - binary_accuracy: 1.0000 - val_loss: 0.4204 - val_binary_accuracy: 0.8696\n",
      "Epoch 43/50\n",
      "17/17 [==============================] - 1s 62ms/step - loss: 6.3949e-06 - binary_accuracy: 1.0000 - val_loss: 0.4488 - val_binary_accuracy: 0.8696\n",
      "Epoch 44/50\n",
      "17/17 [==============================] - 1s 67ms/step - loss: 1.6964e-06 - binary_accuracy: 1.0000 - val_loss: 0.4470 - val_binary_accuracy: 0.8696\n",
      "Epoch 45/50\n",
      "17/17 [==============================] - 1s 59ms/step - loss: 1.9911e-06 - binary_accuracy: 1.0000 - val_loss: 0.4496 - val_binary_accuracy: 0.8696\n",
      "Epoch 46/50\n",
      "17/17 [==============================] - 1s 58ms/step - loss: 4.2975e-06 - binary_accuracy: 1.0000 - val_loss: 0.4291 - val_binary_accuracy: 0.8696\n",
      "Epoch 47/50\n",
      "17/17 [==============================] - 1s 64ms/step - loss: 1.0008e-04 - binary_accuracy: 1.0000 - val_loss: 0.4135 - val_binary_accuracy: 0.8696\n",
      "Epoch 48/50\n",
      "17/17 [==============================] - 1s 62ms/step - loss: 2.4385e-06 - binary_accuracy: 1.0000 - val_loss: 0.4113 - val_binary_accuracy: 0.8696\n",
      "Epoch 49/50\n",
      "17/17 [==============================] - 1s 63ms/step - loss: 4.1496e-06 - binary_accuracy: 1.0000 - val_loss: 0.3994 - val_binary_accuracy: 0.8696\n",
      "Epoch 50/50\n",
      "17/17 [==============================] - 1s 56ms/step - loss: 2.4823e-06 - binary_accuracy: 1.0000 - val_loss: 0.3986 - val_binary_accuracy: 0.8696\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "Epoch 1/50\n",
      "17/17 [==============================] - 2s 81ms/step - loss: 0.9208 - binary_accuracy: 0.4118 - val_loss: 0.6941 - val_binary_accuracy: 0.4783\n",
      "Epoch 2/50\n",
      "17/17 [==============================] - 1s 70ms/step - loss: 0.6985 - binary_accuracy: 0.5000 - val_loss: 0.6926 - val_binary_accuracy: 0.5217\n",
      "Epoch 3/50\n",
      "17/17 [==============================] - 1s 63ms/step - loss: 0.6979 - binary_accuracy: 0.4853 - val_loss: 0.6954 - val_binary_accuracy: 0.4783\n",
      "Epoch 4/50\n",
      "17/17 [==============================] - 1s 59ms/step - loss: 0.7015 - binary_accuracy: 0.5000 - val_loss: 0.6986 - val_binary_accuracy: 0.4783\n",
      "Epoch 5/50\n",
      "17/17 [==============================] - 1s 68ms/step - loss: 0.6988 - binary_accuracy: 0.4412 - val_loss: 0.6950 - val_binary_accuracy: 0.4783\n",
      "Epoch 6/50\n",
      "17/17 [==============================] - 1s 68ms/step - loss: 0.7072 - binary_accuracy: 0.5147 - val_loss: 0.6928 - val_binary_accuracy: 0.5217\n",
      "Epoch 7/50\n",
      "17/17 [==============================] - 1s 69ms/step - loss: 0.7146 - binary_accuracy: 0.4412 - val_loss: 0.6936 - val_binary_accuracy: 0.4783\n",
      "Epoch 8/50\n",
      "17/17 [==============================] - 1s 62ms/step - loss: 0.7100 - binary_accuracy: 0.5441 - val_loss: 0.6983 - val_binary_accuracy: 0.4783\n",
      "Epoch 9/50\n",
      "17/17 [==============================] - 1s 63ms/step - loss: 0.6960 - binary_accuracy: 0.5294 - val_loss: 0.6929 - val_binary_accuracy: 0.5652\n",
      "Epoch 10/50\n",
      "17/17 [==============================] - 1s 67ms/step - loss: 0.7068 - binary_accuracy: 0.5735 - val_loss: 0.6993 - val_binary_accuracy: 0.4783\n",
      "Epoch 11/50\n",
      "17/17 [==============================] - 1s 63ms/step - loss: 0.6985 - binary_accuracy: 0.4853 - val_loss: 0.6943 - val_binary_accuracy: 0.4783\n",
      "Epoch 12/50\n",
      "17/17 [==============================] - 1s 63ms/step - loss: 0.6988 - binary_accuracy: 0.5000 - val_loss: 0.6945 - val_binary_accuracy: 0.4783\n",
      "Epoch 13/50\n",
      "17/17 [==============================] - 1s 58ms/step - loss: 0.7060 - binary_accuracy: 0.5294 - val_loss: 0.6931 - val_binary_accuracy: 0.4783\n",
      "Epoch 14/50\n",
      "17/17 [==============================] - 1s 64ms/step - loss: 0.6987 - binary_accuracy: 0.4412 - val_loss: 0.6954 - val_binary_accuracy: 0.4783\n",
      "Epoch 15/50\n",
      "17/17 [==============================] - 1s 63ms/step - loss: 0.7076 - binary_accuracy: 0.5000 - val_loss: 0.6944 - val_binary_accuracy: 0.4783\n",
      "Epoch 16/50\n",
      "17/17 [==============================] - 1s 64ms/step - loss: 0.6977 - binary_accuracy: 0.5294 - val_loss: 0.6931 - val_binary_accuracy: 0.4783\n",
      "Epoch 17/50\n",
      "17/17 [==============================] - 1s 62ms/step - loss: 0.6948 - binary_accuracy: 0.6471 - val_loss: 0.6941 - val_binary_accuracy: 0.5217\n",
      "Epoch 18/50\n",
      "17/17 [==============================] - 1s 63ms/step - loss: 0.7010 - binary_accuracy: 0.4706 - val_loss: 0.6950 - val_binary_accuracy: 0.4783\n",
      "Epoch 19/50\n",
      "17/17 [==============================] - 1s 63ms/step - loss: 0.7059 - binary_accuracy: 0.5000 - val_loss: 0.6934 - val_binary_accuracy: 0.4783\n",
      "Epoch 20/50\n",
      "17/17 [==============================] - 1s 65ms/step - loss: 0.6904 - binary_accuracy: 0.5000 - val_loss: 0.6900 - val_binary_accuracy: 0.4783\n",
      "Epoch 21/50\n",
      "17/17 [==============================] - 1s 70ms/step - loss: 0.7105 - binary_accuracy: 0.5588 - val_loss: 0.6875 - val_binary_accuracy: 0.4783\n",
      "Epoch 22/50\n",
      "17/17 [==============================] - 1s 67ms/step - loss: 0.6686 - binary_accuracy: 0.6029 - val_loss: 0.6743 - val_binary_accuracy: 0.7826\n",
      "Epoch 23/50\n",
      "17/17 [==============================] - 1s 69ms/step - loss: 0.6778 - binary_accuracy: 0.6324 - val_loss: 0.6448 - val_binary_accuracy: 0.8696\n",
      "Epoch 24/50\n",
      "17/17 [==============================] - 1s 65ms/step - loss: 0.6233 - binary_accuracy: 0.7353 - val_loss: 0.6334 - val_binary_accuracy: 0.5217\n",
      "Epoch 25/50\n",
      "17/17 [==============================] - 1s 65ms/step - loss: 0.3813 - binary_accuracy: 0.9265 - val_loss: 0.5007 - val_binary_accuracy: 0.7826\n",
      "Epoch 26/50\n",
      "17/17 [==============================] - 1s 65ms/step - loss: 0.2086 - binary_accuracy: 0.9559 - val_loss: 0.3641 - val_binary_accuracy: 0.8696\n",
      "Epoch 27/50\n",
      "17/17 [==============================] - 1s 64ms/step - loss: 0.0752 - binary_accuracy: 1.0000 - val_loss: 0.5000 - val_binary_accuracy: 0.7826\n",
      "Epoch 28/50\n",
      "17/17 [==============================] - 1s 69ms/step - loss: 0.0473 - binary_accuracy: 1.0000 - val_loss: 0.2532 - val_binary_accuracy: 0.9130\n",
      "Epoch 29/50\n",
      "17/17 [==============================] - 1s 69ms/step - loss: 0.0141 - binary_accuracy: 1.0000 - val_loss: 0.1719 - val_binary_accuracy: 0.9130\n",
      "Epoch 30/50\n",
      "17/17 [==============================] - 1s 64ms/step - loss: 0.0136 - binary_accuracy: 1.0000 - val_loss: 0.3016 - val_binary_accuracy: 0.8696\n",
      "Epoch 31/50\n",
      "17/17 [==============================] - 1s 62ms/step - loss: 0.0030 - binary_accuracy: 1.0000 - val_loss: 0.4663 - val_binary_accuracy: 0.7826\n",
      "Epoch 32/50\n",
      "17/17 [==============================] - 1s 63ms/step - loss: 8.5139e-04 - binary_accuracy: 1.0000 - val_loss: 0.2911 - val_binary_accuracy: 0.9130\n",
      "Epoch 33/50\n",
      "17/17 [==============================] - 1s 67ms/step - loss: 1.7990e-04 - binary_accuracy: 1.0000 - val_loss: 0.4705 - val_binary_accuracy: 0.7826\n",
      "Epoch 34/50\n",
      "17/17 [==============================] - 1s 67ms/step - loss: 0.0023 - binary_accuracy: 1.0000 - val_loss: 0.2779 - val_binary_accuracy: 0.9130\n",
      "Epoch 35/50\n",
      "17/17 [==============================] - 1s 64ms/step - loss: 1.7682e-04 - binary_accuracy: 1.0000 - val_loss: 0.2819 - val_binary_accuracy: 0.9130\n",
      "Epoch 36/50\n",
      "17/17 [==============================] - 1s 65ms/step - loss: 7.1525e-05 - binary_accuracy: 1.0000 - val_loss: 0.2854 - val_binary_accuracy: 0.9130\n",
      "Epoch 37/50\n",
      "17/17 [==============================] - 1s 68ms/step - loss: 2.2517e-04 - binary_accuracy: 1.0000 - val_loss: 0.3911 - val_binary_accuracy: 0.9130\n",
      "Epoch 38/50\n",
      "17/17 [==============================] - 1s 68ms/step - loss: 1.2024e-04 - binary_accuracy: 1.0000 - val_loss: 0.4506 - val_binary_accuracy: 0.8696\n",
      "Epoch 39/50\n",
      "17/17 [==============================] - 1s 67ms/step - loss: 4.3529e-05 - binary_accuracy: 1.0000 - val_loss: 0.2692 - val_binary_accuracy: 0.9130\n",
      "Epoch 40/50\n",
      "17/17 [==============================] - 1s 62ms/step - loss: 2.5192e-05 - binary_accuracy: 1.0000 - val_loss: 0.2641 - val_binary_accuracy: 0.9130\n",
      "Epoch 41/50\n",
      "17/17 [==============================] - 1s 63ms/step - loss: 3.1754e-05 - binary_accuracy: 1.0000 - val_loss: 0.3137 - val_binary_accuracy: 0.9130\n",
      "Epoch 42/50\n",
      "17/17 [==============================] - 1s 62ms/step - loss: 1.7194e-05 - binary_accuracy: 1.0000 - val_loss: 0.2893 - val_binary_accuracy: 0.9130\n",
      "Epoch 43/50\n",
      "17/17 [==============================] - 1s 67ms/step - loss: 1.2016e-05 - binary_accuracy: 1.0000 - val_loss: 0.2873 - val_binary_accuracy: 0.9130\n",
      "Epoch 44/50\n",
      "17/17 [==============================] - 1s 63ms/step - loss: 7.7476e-06 - binary_accuracy: 1.0000 - val_loss: 0.3008 - val_binary_accuracy: 0.9130\n",
      "Epoch 45/50\n",
      "17/17 [==============================] - 1s 62ms/step - loss: 5.6472e-06 - binary_accuracy: 1.0000 - val_loss: 0.3075 - val_binary_accuracy: 0.9130\n",
      "Epoch 46/50\n",
      "17/17 [==============================] - 1s 67ms/step - loss: 8.3130e-06 - binary_accuracy: 1.0000 - val_loss: 0.2896 - val_binary_accuracy: 0.9130\n",
      "Epoch 47/50\n",
      "17/17 [==============================] - 1s 67ms/step - loss: 1.0324e-05 - binary_accuracy: 1.0000 - val_loss: 0.2637 - val_binary_accuracy: 0.9130\n",
      "Epoch 48/50\n",
      "17/17 [==============================] - 1s 63ms/step - loss: 3.7016e-06 - binary_accuracy: 1.0000 - val_loss: 0.2670 - val_binary_accuracy: 0.9130\n",
      "Epoch 49/50\n",
      "17/17 [==============================] - 1s 62ms/step - loss: 3.9541e-06 - binary_accuracy: 1.0000 - val_loss: 0.2699 - val_binary_accuracy: 0.9130\n",
      "Epoch 50/50\n",
      "17/17 [==============================] - 1s 63ms/step - loss: 2.3292e-06 - binary_accuracy: 1.0000 - val_loss: 0.2724 - val_binary_accuracy: 0.9130\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "Epoch 1/50\n",
      "17/17 [==============================] - 2s 78ms/step - loss: 0.7550 - binary_accuracy: 0.5441 - val_loss: 0.6969 - val_binary_accuracy: 0.5217\n",
      "Epoch 2/50\n",
      "17/17 [==============================] - 1s 70ms/step - loss: 0.7058 - binary_accuracy: 0.4265 - val_loss: 0.6917 - val_binary_accuracy: 0.5217\n",
      "Epoch 3/50\n",
      "17/17 [==============================] - 1s 67ms/step - loss: 0.7028 - binary_accuracy: 0.4412 - val_loss: 0.6929 - val_binary_accuracy: 0.4783\n",
      "Epoch 4/50\n",
      "17/17 [==============================] - 1s 64ms/step - loss: 0.7069 - binary_accuracy: 0.4853 - val_loss: 0.6934 - val_binary_accuracy: 0.4783\n",
      "Epoch 5/50\n",
      "17/17 [==============================] - 1s 64ms/step - loss: 0.7083 - binary_accuracy: 0.4118 - val_loss: 0.6937 - val_binary_accuracy: 0.4783\n",
      "Epoch 6/50\n",
      "17/17 [==============================] - 1s 64ms/step - loss: 0.6985 - binary_accuracy: 0.3676 - val_loss: 0.6932 - val_binary_accuracy: 0.4783\n",
      "Epoch 7/50\n",
      "17/17 [==============================] - 1s 64ms/step - loss: 0.6967 - binary_accuracy: 0.5147 - val_loss: 0.6942 - val_binary_accuracy: 0.5217\n",
      "Epoch 8/50\n",
      "17/17 [==============================] - 1s 63ms/step - loss: 0.7076 - binary_accuracy: 0.4412 - val_loss: 0.6935 - val_binary_accuracy: 0.4783\n",
      "Epoch 9/50\n",
      "17/17 [==============================] - 1s 62ms/step - loss: 0.7291 - binary_accuracy: 0.4559 - val_loss: 0.6946 - val_binary_accuracy: 0.4783\n",
      "Epoch 10/50\n",
      "17/17 [==============================] - 1s 67ms/step - loss: 0.7111 - binary_accuracy: 0.5588 - val_loss: 0.6922 - val_binary_accuracy: 0.5217\n",
      "Epoch 11/50\n",
      "17/17 [==============================] - 1s 63ms/step - loss: 0.6990 - binary_accuracy: 0.4559 - val_loss: 0.6928 - val_binary_accuracy: 0.4783\n",
      "Epoch 12/50\n",
      "17/17 [==============================] - 1s 61ms/step - loss: 0.6976 - binary_accuracy: 0.4559 - val_loss: 0.6968 - val_binary_accuracy: 0.4783\n",
      "Epoch 13/50\n",
      "17/17 [==============================] - 1s 63ms/step - loss: 0.6985 - binary_accuracy: 0.5294 - val_loss: 0.6924 - val_binary_accuracy: 0.6522\n",
      "Epoch 14/50\n",
      "17/17 [==============================] - 1s 69ms/step - loss: 0.6936 - binary_accuracy: 0.5294 - val_loss: 0.6908 - val_binary_accuracy: 0.4783\n",
      "Epoch 15/50\n",
      "17/17 [==============================] - 1s 69ms/step - loss: 0.6979 - binary_accuracy: 0.5147 - val_loss: 0.6848 - val_binary_accuracy: 0.6087\n",
      "Epoch 16/50\n",
      "17/17 [==============================] - 1s 63ms/step - loss: 0.6654 - binary_accuracy: 0.7794 - val_loss: 0.7914 - val_binary_accuracy: 0.4783\n",
      "Epoch 17/50\n",
      "17/17 [==============================] - 1s 70ms/step - loss: 0.5626 - binary_accuracy: 0.8235 - val_loss: 0.6708 - val_binary_accuracy: 0.4783\n",
      "Epoch 18/50\n",
      "17/17 [==============================] - 1s 65ms/step - loss: 0.3143 - binary_accuracy: 0.9265 - val_loss: 0.3148 - val_binary_accuracy: 0.8696\n",
      "Epoch 19/50\n",
      "17/17 [==============================] - 1s 64ms/step - loss: 0.1567 - binary_accuracy: 0.9412 - val_loss: 0.4758 - val_binary_accuracy: 0.8261\n",
      "Epoch 20/50\n",
      "17/17 [==============================] - 1s 68ms/step - loss: 0.0248 - binary_accuracy: 1.0000 - val_loss: 0.1880 - val_binary_accuracy: 0.9565\n",
      "Epoch 21/50\n",
      "17/17 [==============================] - 1s 62ms/step - loss: 0.0062 - binary_accuracy: 1.0000 - val_loss: 0.2820 - val_binary_accuracy: 0.9130\n",
      "Epoch 22/50\n",
      "17/17 [==============================] - 1s 63ms/step - loss: 0.0183 - binary_accuracy: 1.0000 - val_loss: 0.4479 - val_binary_accuracy: 0.8696\n",
      "Epoch 23/50\n",
      "17/17 [==============================] - 1s 68ms/step - loss: 0.0094 - binary_accuracy: 1.0000 - val_loss: 0.1664 - val_binary_accuracy: 0.9565\n",
      "Epoch 24/50\n",
      "17/17 [==============================] - 1s 66ms/step - loss: 0.0023 - binary_accuracy: 1.0000 - val_loss: 0.3112 - val_binary_accuracy: 0.9130\n",
      "Epoch 25/50\n",
      "17/17 [==============================] - 1s 64ms/step - loss: 0.0011 - binary_accuracy: 1.0000 - val_loss: 0.2427 - val_binary_accuracy: 0.9565\n",
      "Epoch 26/50\n",
      "17/17 [==============================] - 1s 63ms/step - loss: 1.3913e-04 - binary_accuracy: 1.0000 - val_loss: 0.2900 - val_binary_accuracy: 0.9565\n",
      "Epoch 27/50\n",
      "17/17 [==============================] - 1s 62ms/step - loss: 2.3024e-04 - binary_accuracy: 1.0000 - val_loss: 0.4054 - val_binary_accuracy: 0.9130\n",
      "Epoch 28/50\n",
      "17/17 [==============================] - 1s 63ms/step - loss: 0.0047 - binary_accuracy: 1.0000 - val_loss: 0.3261 - val_binary_accuracy: 0.9565\n",
      "Epoch 29/50\n",
      "17/17 [==============================] - 1s 62ms/step - loss: 2.9612e-04 - binary_accuracy: 1.0000 - val_loss: 0.3142 - val_binary_accuracy: 0.9565\n",
      "Epoch 30/50\n",
      "17/17 [==============================] - 1s 62ms/step - loss: 2.1125e-04 - binary_accuracy: 1.0000 - val_loss: 0.4101 - val_binary_accuracy: 0.9565\n",
      "Epoch 31/50\n",
      "17/17 [==============================] - 1s 64ms/step - loss: 2.9774e-05 - binary_accuracy: 1.0000 - val_loss: 0.4101 - val_binary_accuracy: 0.9565\n",
      "Epoch 32/50\n",
      "17/17 [==============================] - 1s 62ms/step - loss: 3.8372e-05 - binary_accuracy: 1.0000 - val_loss: 0.4151 - val_binary_accuracy: 0.9565\n",
      "Epoch 33/50\n",
      "17/17 [==============================] - 1s 62ms/step - loss: 1.5808e-05 - binary_accuracy: 1.0000 - val_loss: 0.4094 - val_binary_accuracy: 0.9565\n",
      "Epoch 34/50\n",
      "17/17 [==============================] - 1s 67ms/step - loss: 1.7057e-05 - binary_accuracy: 1.0000 - val_loss: 0.3813 - val_binary_accuracy: 0.9565\n",
      "Epoch 35/50\n",
      "17/17 [==============================] - 1s 61ms/step - loss: 1.8957e-05 - binary_accuracy: 1.0000 - val_loss: 0.4536 - val_binary_accuracy: 0.9130\n",
      "Epoch 36/50\n",
      "17/17 [==============================] - 1s 63ms/step - loss: 1.3358e-05 - binary_accuracy: 1.0000 - val_loss: 0.4976 - val_binary_accuracy: 0.9130\n",
      "Epoch 37/50\n",
      "17/17 [==============================] - 1s 62ms/step - loss: 5.6330e-06 - binary_accuracy: 1.0000 - val_loss: 0.4832 - val_binary_accuracy: 0.9130\n",
      "Epoch 38/50\n",
      "17/17 [==============================] - 1s 62ms/step - loss: 1.7422e-05 - binary_accuracy: 1.0000 - val_loss: 0.4201 - val_binary_accuracy: 0.9130\n",
      "Epoch 39/50\n",
      "17/17 [==============================] - 1s 63ms/step - loss: 3.1475e-06 - binary_accuracy: 1.0000 - val_loss: 0.4170 - val_binary_accuracy: 0.9130\n",
      "Epoch 40/50\n",
      "17/17 [==============================] - 1s 62ms/step - loss: 2.8881e-06 - binary_accuracy: 1.0000 - val_loss: 0.4186 - val_binary_accuracy: 0.9565\n",
      "Epoch 41/50\n",
      "17/17 [==============================] - 1s 62ms/step - loss: 9.0208e-06 - binary_accuracy: 1.0000 - val_loss: 0.4219 - val_binary_accuracy: 0.9130\n",
      "Epoch 42/50\n",
      "17/17 [==============================] - 1s 62ms/step - loss: 5.6477e-06 - binary_accuracy: 1.0000 - val_loss: 0.4384 - val_binary_accuracy: 0.9130\n",
      "Epoch 43/50\n",
      "17/17 [==============================] - 1s 67ms/step - loss: 3.1686e-06 - binary_accuracy: 1.0000 - val_loss: 0.4374 - val_binary_accuracy: 0.9130\n",
      "Epoch 44/50\n",
      "17/17 [==============================] - 1s 62ms/step - loss: 1.8938e-06 - binary_accuracy: 1.0000 - val_loss: 0.4404 - val_binary_accuracy: 0.9130\n",
      "Epoch 45/50\n",
      "17/17 [==============================] - 1s 63ms/step - loss: 2.8301e-06 - binary_accuracy: 1.0000 - val_loss: 0.4299 - val_binary_accuracy: 0.9130\n",
      "Epoch 46/50\n",
      "17/17 [==============================] - 1s 63ms/step - loss: 2.3493e-06 - binary_accuracy: 1.0000 - val_loss: 0.4330 - val_binary_accuracy: 0.9130\n",
      "Epoch 47/50\n",
      "17/17 [==============================] - 1s 63ms/step - loss: 1.2242e-06 - binary_accuracy: 1.0000 - val_loss: 0.4326 - val_binary_accuracy: 0.9130\n",
      "Epoch 48/50\n",
      "17/17 [==============================] - 1s 67ms/step - loss: 3.7637e-06 - binary_accuracy: 1.0000 - val_loss: 0.4281 - val_binary_accuracy: 0.9130\n",
      "Epoch 49/50\n",
      "17/17 [==============================] - 1s 61ms/step - loss: 2.2738e-06 - binary_accuracy: 1.0000 - val_loss: 0.4355 - val_binary_accuracy: 0.9130\n",
      "Epoch 50/50\n",
      "17/17 [==============================] - 1s 67ms/step - loss: 1.5197e-06 - binary_accuracy: 1.0000 - val_loss: 0.4336 - val_binary_accuracy: 0.9130\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "Epoch 1/50\n",
      "17/17 [==============================] - 2s 79ms/step - loss: 0.8934 - binary_accuracy: 0.4853 - val_loss: 0.6921 - val_binary_accuracy: 0.5217\n",
      "Epoch 2/50\n",
      "17/17 [==============================] - 1s 62ms/step - loss: 0.7034 - binary_accuracy: 0.5294 - val_loss: 0.6923 - val_binary_accuracy: 0.5217\n",
      "Epoch 3/50\n",
      "17/17 [==============================] - 1s 67ms/step - loss: 0.7024 - binary_accuracy: 0.4412 - val_loss: 0.6933 - val_binary_accuracy: 0.4783\n",
      "Epoch 4/50\n",
      "17/17 [==============================] - 1s 67ms/step - loss: 0.7368 - binary_accuracy: 0.4853 - val_loss: 0.6938 - val_binary_accuracy: 0.5217\n",
      "Epoch 5/50\n",
      "17/17 [==============================] - 1s 67ms/step - loss: 0.7091 - binary_accuracy: 0.3971 - val_loss: 0.6935 - val_binary_accuracy: 0.4783\n",
      "Epoch 6/50\n",
      "17/17 [==============================] - 1s 67ms/step - loss: 0.6986 - binary_accuracy: 0.4265 - val_loss: 0.6944 - val_binary_accuracy: 0.4783\n",
      "Epoch 7/50\n",
      "17/17 [==============================] - 1s 62ms/step - loss: 0.6976 - binary_accuracy: 0.4118 - val_loss: 0.6929 - val_binary_accuracy: 0.5217\n",
      "Epoch 8/50\n",
      "17/17 [==============================] - 1s 64ms/step - loss: 0.6962 - binary_accuracy: 0.3382 - val_loss: 0.6932 - val_binary_accuracy: 0.4783\n",
      "Epoch 9/50\n",
      "17/17 [==============================] - 1s 64ms/step - loss: 0.6992 - binary_accuracy: 0.4853 - val_loss: 0.6942 - val_binary_accuracy: 0.4783\n",
      "Epoch 10/50\n",
      "17/17 [==============================] - 1s 64ms/step - loss: 0.7058 - binary_accuracy: 0.5441 - val_loss: 0.6950 - val_binary_accuracy: 0.4783\n",
      "Epoch 11/50\n",
      "17/17 [==============================] - 1s 64ms/step - loss: 0.6960 - binary_accuracy: 0.5294 - val_loss: 0.6941 - val_binary_accuracy: 0.4783\n",
      "Epoch 12/50\n",
      "17/17 [==============================] - 1s 59ms/step - loss: 0.7001 - binary_accuracy: 0.5147 - val_loss: 0.6953 - val_binary_accuracy: 0.4783\n",
      "Epoch 13/50\n",
      "17/17 [==============================] - 1s 63ms/step - loss: 0.6959 - binary_accuracy: 0.4706 - val_loss: 0.6933 - val_binary_accuracy: 0.4783\n",
      "Epoch 14/50\n",
      "17/17 [==============================] - 1s 68ms/step - loss: 0.7035 - binary_accuracy: 0.5294 - val_loss: 0.6949 - val_binary_accuracy: 0.4783\n",
      "Epoch 15/50\n",
      "17/17 [==============================] - 1s 67ms/step - loss: 0.6917 - binary_accuracy: 0.5588 - val_loss: 0.7029 - val_binary_accuracy: 0.4783\n",
      "Epoch 16/50\n",
      "17/17 [==============================] - 1s 63ms/step - loss: 0.7242 - binary_accuracy: 0.4853 - val_loss: 0.6955 - val_binary_accuracy: 0.4783\n",
      "Epoch 17/50\n",
      "17/17 [==============================] - 1s 64ms/step - loss: 0.6975 - binary_accuracy: 0.5147 - val_loss: 0.6945 - val_binary_accuracy: 0.4783\n",
      "Epoch 18/50\n",
      "17/17 [==============================] - 1s 63ms/step - loss: 0.6943 - binary_accuracy: 0.5294 - val_loss: 0.6929 - val_binary_accuracy: 0.5217\n",
      "Epoch 19/50\n",
      "17/17 [==============================] - 1s 68ms/step - loss: 0.6963 - binary_accuracy: 0.4706 - val_loss: 0.6953 - val_binary_accuracy: 0.4783\n",
      "Epoch 20/50\n",
      "17/17 [==============================] - 1s 68ms/step - loss: 0.6972 - binary_accuracy: 0.5147 - val_loss: 0.6933 - val_binary_accuracy: 0.4783\n",
      "Epoch 21/50\n",
      "17/17 [==============================] - 1s 70ms/step - loss: 0.6979 - binary_accuracy: 0.5294 - val_loss: 0.6919 - val_binary_accuracy: 0.5217\n",
      "Epoch 22/50\n",
      "17/17 [==============================] - 1s 65ms/step - loss: 0.7217 - binary_accuracy: 0.5147 - val_loss: 0.6933 - val_binary_accuracy: 0.4783\n",
      "Epoch 23/50\n",
      "17/17 [==============================] - 1s 67ms/step - loss: 0.6943 - binary_accuracy: 0.5000 - val_loss: 0.6924 - val_binary_accuracy: 0.4783\n",
      "Epoch 24/50\n",
      "17/17 [==============================] - 1s 67ms/step - loss: 0.6936 - binary_accuracy: 0.5294 - val_loss: 0.6997 - val_binary_accuracy: 0.4783\n",
      "Epoch 25/50\n",
      "17/17 [==============================] - 1s 64ms/step - loss: 0.6928 - binary_accuracy: 0.5294 - val_loss: 0.6862 - val_binary_accuracy: 0.4783\n",
      "Epoch 26/50\n",
      "17/17 [==============================] - 1s 65ms/step - loss: 0.6756 - binary_accuracy: 0.5735 - val_loss: 0.6822 - val_binary_accuracy: 0.4783\n",
      "Epoch 27/50\n",
      "17/17 [==============================] - 1s 68ms/step - loss: 0.6494 - binary_accuracy: 0.6912 - val_loss: 0.6311 - val_binary_accuracy: 0.7826\n",
      "Epoch 28/50\n",
      "17/17 [==============================] - 1s 71ms/step - loss: 0.5523 - binary_accuracy: 0.7647 - val_loss: 0.5504 - val_binary_accuracy: 0.8261\n",
      "Epoch 29/50\n",
      "17/17 [==============================] - 1s 70ms/step - loss: 0.3714 - binary_accuracy: 0.9265 - val_loss: 0.4861 - val_binary_accuracy: 0.7391\n",
      "Epoch 30/50\n",
      "17/17 [==============================] - 1s 67ms/step - loss: 0.1895 - binary_accuracy: 0.9706 - val_loss: 0.3974 - val_binary_accuracy: 0.8261\n",
      "Epoch 31/50\n",
      "17/17 [==============================] - 1s 70ms/step - loss: 0.0815 - binary_accuracy: 1.0000 - val_loss: 0.3198 - val_binary_accuracy: 0.8261\n",
      "Epoch 32/50\n",
      "17/17 [==============================] - 1s 66ms/step - loss: 0.0881 - binary_accuracy: 0.9853 - val_loss: 0.2180 - val_binary_accuracy: 0.9565\n",
      "Epoch 33/50\n",
      "17/17 [==============================] - 1s 62ms/step - loss: 0.0201 - binary_accuracy: 1.0000 - val_loss: 0.2855 - val_binary_accuracy: 0.8261\n",
      "Epoch 34/50\n",
      "17/17 [==============================] - 1s 64ms/step - loss: 0.0048 - binary_accuracy: 1.0000 - val_loss: 0.1502 - val_binary_accuracy: 0.9130\n",
      "Epoch 35/50\n",
      "17/17 [==============================] - 1s 66ms/step - loss: 0.0108 - binary_accuracy: 1.0000 - val_loss: 0.4452 - val_binary_accuracy: 0.8261\n",
      "Epoch 36/50\n",
      "17/17 [==============================] - 1s 62ms/step - loss: 7.8899e-04 - binary_accuracy: 1.0000 - val_loss: 0.3048 - val_binary_accuracy: 0.8261\n",
      "Epoch 37/50\n",
      "17/17 [==============================] - 1s 67ms/step - loss: 0.0014 - binary_accuracy: 1.0000 - val_loss: 0.5387 - val_binary_accuracy: 0.8261\n",
      "Epoch 38/50\n",
      "17/17 [==============================] - 1s 67ms/step - loss: 3.0342e-04 - binary_accuracy: 1.0000 - val_loss: 0.4817 - val_binary_accuracy: 0.8261\n",
      "Epoch 39/50\n",
      "17/17 [==============================] - 1s 61ms/step - loss: 1.5765e-04 - binary_accuracy: 1.0000 - val_loss: 0.4947 - val_binary_accuracy: 0.8261\n",
      "Epoch 40/50\n",
      "17/17 [==============================] - 1s 63ms/step - loss: 2.5258e-04 - binary_accuracy: 1.0000 - val_loss: 0.4635 - val_binary_accuracy: 0.8261\n",
      "Epoch 41/50\n",
      "17/17 [==============================] - 1s 67ms/step - loss: 1.7494e-04 - binary_accuracy: 1.0000 - val_loss: 0.5355 - val_binary_accuracy: 0.8261\n",
      "Epoch 42/50\n",
      "17/17 [==============================] - 1s 63ms/step - loss: 4.8396e-05 - binary_accuracy: 1.0000 - val_loss: 0.3846 - val_binary_accuracy: 0.8261\n",
      "Epoch 43/50\n",
      "17/17 [==============================] - 1s 58ms/step - loss: 4.0955e-05 - binary_accuracy: 1.0000 - val_loss: 0.2885 - val_binary_accuracy: 0.8261\n",
      "Epoch 44/50\n",
      "17/17 [==============================] - 1s 62ms/step - loss: 4.4697e-05 - binary_accuracy: 1.0000 - val_loss: 0.4756 - val_binary_accuracy: 0.8261\n",
      "Epoch 45/50\n",
      "17/17 [==============================] - 1s 62ms/step - loss: 2.2449e-05 - binary_accuracy: 1.0000 - val_loss: 0.3490 - val_binary_accuracy: 0.8261\n",
      "Epoch 46/50\n",
      "17/17 [==============================] - 1s 58ms/step - loss: 1.3494e-05 - binary_accuracy: 1.0000 - val_loss: 0.4144 - val_binary_accuracy: 0.8261\n",
      "Epoch 47/50\n",
      "17/17 [==============================] - 1s 62ms/step - loss: 4.3994e-05 - binary_accuracy: 1.0000 - val_loss: 0.4136 - val_binary_accuracy: 0.8261\n",
      "Epoch 48/50\n",
      "17/17 [==============================] - 1s 62ms/step - loss: 9.3976e-06 - binary_accuracy: 1.0000 - val_loss: 0.4129 - val_binary_accuracy: 0.8261\n",
      "Epoch 49/50\n",
      "17/17 [==============================] - 1s 59ms/step - loss: 7.2096e-06 - binary_accuracy: 1.0000 - val_loss: 0.4115 - val_binary_accuracy: 0.8261\n",
      "Epoch 50/50\n",
      "17/17 [==============================] - 1s 62ms/step - loss: 2.0779e-05 - binary_accuracy: 1.0000 - val_loss: 0.4305 - val_binary_accuracy: 0.8261\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "Optimiseur : RMSprop\n",
      "Score de classification\n",
      "0.0 +- 0.0 %\n",
      "[0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "drop_out_rate=0.1\n",
    "outlayer_neurons=128\n",
    "\n",
    "optimizer = keras.optimizers.Adam(lr=0.005)\n",
    "optimizers_list=['RMSprop']\n",
    "# optimizers_list=['Adam']\n",
    "rand_state=np.random.randint(50)                  \n",
    "\n",
    "\n",
    "for opti in optimizers_list:\n",
    "    f1=[]\n",
    "    # rand_state=np.random.randint(50)\n",
    "    for i in range(4):\n",
    "\n",
    "        \n",
    "                #Construction et définition du réseau de neurones \n",
    "            \n",
    "        model = Sequential([\n",
    "\n",
    "              layers.Conv1D(256, 14, padding='same', activation='relu', input_shape=(int(min(year_size_data)/24),len(feature_choice))),\n",
    "              layers.MaxPooling1D(pool_size=8),\n",
    "              layers.Dropout(drop_out_rate),\n",
    "              layers.Conv1D(128, 4, padding='same', activation='relu'),\n",
    "              layers.Dropout(drop_out_rate),\n",
    "              layers.Conv1D(64, 2, padding='same', activation='relu'),\n",
    "              layers.Dropout(drop_out_rate),\n",
    "              layers.Flatten(),\n",
    "              layers.Dense(outlayer_neurons, activation='relu'),\n",
    "              layers.Dense(2,activation='softmax')\n",
    "            ])\n",
    "            \n",
    "\n",
    "        # Model 1 -  Variant\n",
    "#         model = Sequential([\n",
    "\n",
    "#           layers.Conv1D(256,12, padding='same', activation='relu', input_shape=(int(min(year_size_data)/24),nb_features)),\n",
    "#           # layers.AveragePooling1D(pool_size=8),\n",
    "#           layers.MaxPooling1D(pool_size=8),\n",
    "#           layers.Dropout(drop_out_rate),\n",
    "#           layers.Conv1D(128, 4, padding='same', activation='relu'),\n",
    "#           layers.MaxPooling1D(pool_size=4),\n",
    "#           layers.Dropout(drop_out_rate),\n",
    "#           # layers.Conv1D(64, 2, padding='same', activation='relu'),\n",
    "#           # layers.MaxPooling1D(pool_size=2),\n",
    "#           # layers.Dropout(drop_out_rate),\n",
    "#           layers.Flatten(),\n",
    "#           # layers.Dense(outlayer_neurons*2, activation='relu'),\n",
    "#           layers.Dense(outlayer_neurons, activation='relu'),\n",
    "#           layers.Dropout(drop_out_rate),\n",
    "#           layers.Dense(2,activation='softmax')\n",
    "#             ])            \n",
    "             \n",
    "\n",
    "          # Second Model : \n",
    "#         model = Sequential([\n",
    "\n",
    "#           layers.Conv1D(16, 14, padding='same', activation='relu', input_shape=(int(min(year_size_data)/24),nb_features)),\n",
    "#           layers.MaxPooling1D(pool_size=2),\n",
    "#           layers.Dropout(drop_out_rate),\n",
    "\n",
    "#           layers.Conv1D(32, 10, padding='same', activation='relu', input_shape=(int(min(year_size_data)/24),nb_features)),\n",
    "#           layers.MaxPooling1D(pool_size=2),\n",
    "#           layers.Dropout(drop_out_rate),\n",
    "#           layers.Conv1D(64, 5, padding='same', activation='relu'),\n",
    "#           layers.MaxPooling1D(pool_size=2),\n",
    "#           layers.Dropout(drop_out_rate),\n",
    "#           layers.Conv1D(128, 3, padding='same', activation='relu'),\n",
    "#           layers.MaxPooling1D(pool_size=2),\n",
    "#           layers.Dropout(drop_out_rate),\n",
    "#           layers.Flatten(),\n",
    "#           layers.Dense(outlayer_neurons*2, activation='relu'),\n",
    "#           layers.Dense(outlayer_neurons, activation='relu'),\n",
    "#           layers.Dropout(drop_out_rate),\n",
    "#           layers.Dense(2,activation='softmax')\n",
    "#             ])\n",
    "\n",
    "\n",
    "        #Compilation du modèle\n",
    "\n",
    "\n",
    "        model.compile(optimizer=opti,loss='binary_crossentropy',metrics='binary_accuracy' )\n",
    "\n",
    "\n",
    "        #Séparation des données test et entraînement\n",
    "        x_train, x_test, y_train, y_test = train_test_split(data_matrix,to_categorical(labels), test_size=0.01,random_state=rand_state)\n",
    "\n",
    "        # callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=20,restore_best_weights=True,start_from_epoch=15)\n",
    "        # callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5,restore_best_weights=True,start_from_epoch=10)\n",
    "\n",
    "       \n",
    "        mc = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=0,save_best_only=True,start_from_epoch=20)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "#         scalers = {}\n",
    "#         # print(x_train.shape)\n",
    "#         # print(x_train[0].shape)\n",
    "#         print(x_train[0][0])\n",
    "#         for i in range(x_train.shape[2]):\n",
    "#             scalers[i] = MinMaxScaler()\n",
    "#             x_train[:, :, i] = scalers[i].fit_transform(x_train[:, :, i]) \n",
    "\n",
    "#         for i in range(x_test.shape[2]):\n",
    "#             x_test[:, :, i] = scalers[i].transform(x_test[:, :, i]) \n",
    "#         # print(x_train.shape)\n",
    "#         # print(x_train[0].shape)\n",
    "#         print(x_train[0][0])\n",
    "\n",
    "        history=model.fit(x_train, y_train,\n",
    "                            epochs=50,\n",
    "                            batch_size=4,verbose=1,class_weight=class_weight,validation_split=0.25,callbacks=[mc])\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "#         history=model.fit(x_train, y_train,\n",
    "#                             epochs=40,\n",
    "#                             batch_size=4,verbose=1,class_weight=class_weight,callbacks=[mc])\n",
    "        \n",
    "        saved_model = load_model('best_model.h5')\n",
    "        y_true=[]\n",
    "        pred_labels=[]\n",
    "        # prob_class=model.predict(x_test)\n",
    "        prob_class=saved_model.predict(x_test)\n",
    "\n",
    "        \n",
    "        for prob in prob_class:\n",
    "\n",
    "            pred_labels.append(np.argmax(prob))\n",
    "\n",
    "        for example in range(len(y_test)):\n",
    "            y_true.append(np.argmax(y_test[example]))\n",
    "\n",
    "        f1.append(f1_score(y_true,pred_labels))\n",
    "    print('Optimiseur :', opti)\n",
    "    print('Score de classification')\n",
    "    print(np.round(np.mean(f1)*100,2),'+-',np.round(np.std(f1)*100,2),'%')\n",
    "    print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08f6b37-65a7-4ac0-ae42-2fba7b485879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main : 0.7838\n",
    "\n",
    "# no 7 : 0.8750+\n",
    "# no 6 : 0.8750-\n",
    "# no 5: 0.75-\n",
    "# no 4: 0.75-\n",
    "# no 3 : 0.8125\n",
    "# no 2 : 0.7812+\n",
    "# no 1 : 0.718--\n",
    "# no 0 : 0.718--\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508c953f-9f84-43dc-a5e4-933daff05ffa",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (537220151.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[228], line 4\u001b[0;36m\u001b[0m\n\u001b[0;31m    Score de classification\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# First Model \n",
    "\n",
    "# Optimiseur : Adam\n",
    "# Score de classification\n",
    "# 77.72 +- 5.29 %\n",
    "# [0.7027027027027027, 0.8085106382978724, 0.8205128205128205]\n",
    "\n",
    "# First Model - Variant\n",
    "\n",
    "# Optimiseur : Adam\n",
    "# Score de classification\n",
    "# 66.61 +- 2.79 %\n",
    "# [0.7000000000000001, 0.631578947368421, 0.6666666666666666]\n",
    "\n",
    "\n",
    "# Second Model\n",
    "\n",
    "# Optimiseur : Adam\n",
    "# Score de classification\n",
    "# 48.61 +- 8.56 %\n",
    "# [0.375, 0.5833333333333334, 0.5]\n",
    "\n",
    "# Optimiseur : RMSprop\n",
    "# Score de classification\n",
    "# 45.83 +- 32.81 %\n",
    "# [0.0, 0.625, 0.75]\n",
    "\n",
    "# Third_Model : (furthers tests needed)\n",
    "\n",
    "# Optimiseur : RMSprop\n",
    "# # Score de classification\n",
    "# # 52.31 +- 9.92 %\n",
    "# # [0.4, 0.5263157894736842, 0.6428571428571429]\n",
    "\n",
    "# Optimiseur : Adam\n",
    "# Score de classification\n",
    "# 67.7 +- 9.44 %\n",
    "\n",
    "# Optimiseur : RMSprop\n",
    "# Score de classification\n",
    "# 43.97 +- 32.75 %\n",
    "# [0.7857142857142858, 0.5333333333333333, 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539ec27d-d95d-4df9-83d6-4c5e1e0e88f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 365, 14)\n",
      "Epoch 1/80\n",
      "35/35 [==============================] - 2s 38ms/step - loss: 0.7017 - binary_accuracy: 0.5036 - val_loss: 0.6934 - val_binary_accuracy: 0.4286\n",
      "Epoch 2/80\n",
      "35/35 [==============================] - 1s 29ms/step - loss: 0.6925 - binary_accuracy: 0.4604 - val_loss: 0.6934 - val_binary_accuracy: 0.4571\n",
      "Epoch 3/80\n",
      "35/35 [==============================] - 1s 33ms/step - loss: 0.6934 - binary_accuracy: 0.5252 - val_loss: 0.6934 - val_binary_accuracy: 0.4571\n",
      "Epoch 4/80\n",
      "35/35 [==============================] - 1s 32ms/step - loss: 0.6995 - binary_accuracy: 0.4604 - val_loss: 0.6914 - val_binary_accuracy: 0.5429\n",
      "Epoch 5/80\n",
      "35/35 [==============================] - 1s 29ms/step - loss: 0.6970 - binary_accuracy: 0.4820 - val_loss: 0.6942 - val_binary_accuracy: 0.4857\n",
      "Epoch 6/80\n",
      "35/35 [==============================] - 1s 29ms/step - loss: 0.6966 - binary_accuracy: 0.5755 - val_loss: 0.6921 - val_binary_accuracy: 0.5143\n",
      "Epoch 7/80\n",
      "35/35 [==============================] - 1s 30ms/step - loss: 0.6900 - binary_accuracy: 0.5899 - val_loss: 0.6902 - val_binary_accuracy: 0.4571\n",
      "Epoch 8/80\n",
      "35/35 [==============================] - 1s 31ms/step - loss: 0.6902 - binary_accuracy: 0.5468 - val_loss: 0.6937 - val_binary_accuracy: 0.4571\n",
      "Epoch 9/80\n",
      "35/35 [==============================] - 1s 30ms/step - loss: 0.6809 - binary_accuracy: 0.5324 - val_loss: 0.6940 - val_binary_accuracy: 0.4857\n",
      "Epoch 10/80\n",
      "35/35 [==============================] - 1s 31ms/step - loss: 0.6763 - binary_accuracy: 0.5612 - val_loss: 0.6818 - val_binary_accuracy: 0.4857\n",
      "Epoch 11/80\n",
      "35/35 [==============================] - 1s 30ms/step - loss: 0.6351 - binary_accuracy: 0.6835 - val_loss: 0.8012 - val_binary_accuracy: 0.5429\n",
      "Epoch 12/80\n",
      "35/35 [==============================] - 1s 32ms/step - loss: 0.6573 - binary_accuracy: 0.6331 - val_loss: 0.6773 - val_binary_accuracy: 0.5714\n",
      "Epoch 13/80\n",
      "35/35 [==============================] - 1s 38ms/step - loss: 0.6067 - binary_accuracy: 0.7122 - val_loss: 0.6361 - val_binary_accuracy: 0.6286\n",
      "Epoch 14/80\n",
      "35/35 [==============================] - 1s 31ms/step - loss: 0.6100 - binary_accuracy: 0.6763 - val_loss: 0.6345 - val_binary_accuracy: 0.6000\n",
      "Epoch 15/80\n",
      "35/35 [==============================] - 1s 29ms/step - loss: 0.5377 - binary_accuracy: 0.7554 - val_loss: 0.6503 - val_binary_accuracy: 0.6571\n",
      "Epoch 16/80\n",
      "35/35 [==============================] - 1s 33ms/step - loss: 0.5482 - binary_accuracy: 0.7698 - val_loss: 0.6328 - val_binary_accuracy: 0.6571\n",
      "Epoch 17/80\n",
      "35/35 [==============================] - 1s 33ms/step - loss: 0.4844 - binary_accuracy: 0.7626 - val_loss: 0.6246 - val_binary_accuracy: 0.7143\n",
      "Epoch 18/80\n",
      "35/35 [==============================] - 1s 33ms/step - loss: 0.4987 - binary_accuracy: 0.7410 - val_loss: 0.6231 - val_binary_accuracy: 0.6857\n",
      "Epoch 19/80\n",
      "35/35 [==============================] - 1s 35ms/step - loss: 0.4681 - binary_accuracy: 0.7698 - val_loss: 0.6236 - val_binary_accuracy: 0.7143\n",
      "Epoch 20/80\n",
      "35/35 [==============================] - 1s 34ms/step - loss: 0.4142 - binary_accuracy: 0.7914 - val_loss: 1.2740 - val_binary_accuracy: 0.5429\n",
      "Epoch 21/80\n",
      "35/35 [==============================] - 1s 31ms/step - loss: 0.4335 - binary_accuracy: 0.7770 - val_loss: 0.6117 - val_binary_accuracy: 0.6857\n",
      "Epoch 22/80\n",
      "35/35 [==============================] - 1s 31ms/step - loss: 0.3704 - binary_accuracy: 0.8129 - val_loss: 0.5913 - val_binary_accuracy: 0.7429\n",
      "Epoch 23/80\n",
      "35/35 [==============================] - 1s 34ms/step - loss: 0.4096 - binary_accuracy: 0.8058 - val_loss: 0.5912 - val_binary_accuracy: 0.7143\n",
      "Epoch 24/80\n",
      "35/35 [==============================] - 1s 32ms/step - loss: 0.3681 - binary_accuracy: 0.8345 - val_loss: 0.7557 - val_binary_accuracy: 0.6857\n",
      "Epoch 25/80\n",
      "35/35 [==============================] - 1s 27ms/step - loss: 0.3747 - binary_accuracy: 0.8129 - val_loss: 0.6280 - val_binary_accuracy: 0.6000\n",
      "Epoch 26/80\n",
      "35/35 [==============================] - 1s 30ms/step - loss: 0.2944 - binary_accuracy: 0.8705 - val_loss: 0.6148 - val_binary_accuracy: 0.7429\n",
      "Epoch 27/80\n",
      "35/35 [==============================] - 1s 29ms/step - loss: 0.3507 - binary_accuracy: 0.8345 - val_loss: 0.6131 - val_binary_accuracy: 0.7714\n",
      "Epoch 28/80\n",
      "35/35 [==============================] - 1s 29ms/step - loss: 0.3364 - binary_accuracy: 0.8345 - val_loss: 0.6044 - val_binary_accuracy: 0.7429\n",
      "Epoch 29/80\n",
      "35/35 [==============================] - 1s 29ms/step - loss: 0.3213 - binary_accuracy: 0.8489 - val_loss: 0.5913 - val_binary_accuracy: 0.7143\n",
      "Epoch 30/80\n",
      "35/35 [==============================] - 1s 29ms/step - loss: 0.2522 - binary_accuracy: 0.8993 - val_loss: 0.6817 - val_binary_accuracy: 0.6857\n",
      "Epoch 31/80\n",
      "35/35 [==============================] - 1s 32ms/step - loss: 0.3423 - binary_accuracy: 0.8633 - val_loss: 0.6294 - val_binary_accuracy: 0.7143\n",
      "Epoch 32/80\n",
      "35/35 [==============================] - 1s 29ms/step - loss: 0.2453 - binary_accuracy: 0.8705 - val_loss: 0.6745 - val_binary_accuracy: 0.7429\n",
      "Epoch 33/80\n",
      "35/35 [==============================] - 1s 32ms/step - loss: 0.2663 - binary_accuracy: 0.8921 - val_loss: 0.6913 - val_binary_accuracy: 0.7429\n",
      "Epoch 34/80\n",
      "35/35 [==============================] - 1s 32ms/step - loss: 0.2739 - binary_accuracy: 0.8921 - val_loss: 0.7105 - val_binary_accuracy: 0.6000\n",
      "Epoch 35/80\n",
      "35/35 [==============================] - 1s 31ms/step - loss: 0.2017 - binary_accuracy: 0.9137 - val_loss: 0.7655 - val_binary_accuracy: 0.7143\n",
      "Epoch 36/80\n",
      "35/35 [==============================] - 1s 32ms/step - loss: 0.2338 - binary_accuracy: 0.8633 - val_loss: 0.7887 - val_binary_accuracy: 0.6857\n",
      "Epoch 37/80\n",
      "35/35 [==============================] - 1s 29ms/step - loss: 0.2255 - binary_accuracy: 0.8849 - val_loss: 0.7697 - val_binary_accuracy: 0.7143\n",
      "Epoch 38/80\n",
      "35/35 [==============================] - 1s 31ms/step - loss: 0.1922 - binary_accuracy: 0.9209 - val_loss: 0.6817 - val_binary_accuracy: 0.7429\n",
      "Epoch 39/80\n",
      "35/35 [==============================] - 1s 29ms/step - loss: 0.1896 - binary_accuracy: 0.8993 - val_loss: 0.7989 - val_binary_accuracy: 0.6857\n",
      "Epoch 40/80\n",
      "35/35 [==============================] - 1s 29ms/step - loss: 0.1579 - binary_accuracy: 0.9424 - val_loss: 0.7671 - val_binary_accuracy: 0.7143\n",
      "Epoch 41/80\n",
      "35/35 [==============================] - 1s 32ms/step - loss: 0.2864 - binary_accuracy: 0.8705 - val_loss: 0.6158 - val_binary_accuracy: 0.7143\n",
      "Epoch 42/80\n",
      "35/35 [==============================] - 1s 29ms/step - loss: 0.1595 - binary_accuracy: 0.9496 - val_loss: 0.7188 - val_binary_accuracy: 0.7714\n",
      "Epoch 43/80\n",
      "35/35 [==============================] - 1s 29ms/step - loss: 0.1807 - binary_accuracy: 0.9137 - val_loss: 0.7270 - val_binary_accuracy: 0.7429\n",
      "Epoch 44/80\n",
      "35/35 [==============================] - 1s 30ms/step - loss: 0.2507 - binary_accuracy: 0.8921 - val_loss: 0.7090 - val_binary_accuracy: 0.7429\n",
      "Epoch 45/80\n",
      "35/35 [==============================] - 1s 30ms/step - loss: 0.1803 - binary_accuracy: 0.9496 - val_loss: 0.8119 - val_binary_accuracy: 0.7143\n",
      "Epoch 46/80\n",
      "35/35 [==============================] - 1s 30ms/step - loss: 0.2306 - binary_accuracy: 0.9137 - val_loss: 0.7134 - val_binary_accuracy: 0.7143\n",
      "Epoch 47/80\n",
      "35/35 [==============================] - 1s 30ms/step - loss: 0.2005 - binary_accuracy: 0.9065 - val_loss: 0.6626 - val_binary_accuracy: 0.7143\n",
      "Epoch 48/80\n",
      "35/35 [==============================] - 1s 30ms/step - loss: 0.1617 - binary_accuracy: 0.9424 - val_loss: 0.7160 - val_binary_accuracy: 0.7143\n",
      "Epoch 49/80\n",
      "35/35 [==============================] - 1s 32ms/step - loss: 0.1166 - binary_accuracy: 0.9640 - val_loss: 0.9160 - val_binary_accuracy: 0.7429\n",
      "Epoch 50/80\n",
      "35/35 [==============================] - 1s 34ms/step - loss: 0.1292 - binary_accuracy: 0.9496 - val_loss: 0.8506 - val_binary_accuracy: 0.7429\n",
      "Epoch 51/80\n",
      "35/35 [==============================] - 1s 32ms/step - loss: 0.1652 - binary_accuracy: 0.9281 - val_loss: 0.8543 - val_binary_accuracy: 0.6857\n",
      "Epoch 52/80\n",
      "35/35 [==============================] - 1s 32ms/step - loss: 0.1777 - binary_accuracy: 0.9281 - val_loss: 0.7039 - val_binary_accuracy: 0.6857\n",
      "Epoch 53/80\n",
      "35/35 [==============================] - 1s 32ms/step - loss: 0.1513 - binary_accuracy: 0.9424 - val_loss: 0.7499 - val_binary_accuracy: 0.7143\n",
      "Epoch 54/80\n",
      "35/35 [==============================] - 1s 35ms/step - loss: 0.1515 - binary_accuracy: 0.9281 - val_loss: 0.8203 - val_binary_accuracy: 0.7714\n",
      "Epoch 55/80\n",
      "35/35 [==============================] - 1s 33ms/step - loss: 0.1974 - binary_accuracy: 0.8921 - val_loss: 0.7649 - val_binary_accuracy: 0.6571\n",
      "Epoch 56/80\n",
      "35/35 [==============================] - 1s 27ms/step - loss: 0.1328 - binary_accuracy: 0.9496 - val_loss: 0.7928 - val_binary_accuracy: 0.7429\n",
      "Epoch 57/80\n",
      "35/35 [==============================] - 1s 29ms/step - loss: 0.1662 - binary_accuracy: 0.9281 - val_loss: 1.2041 - val_binary_accuracy: 0.7714\n",
      "Epoch 58/80\n",
      "35/35 [==============================] - 1s 27ms/step - loss: 0.2384 - binary_accuracy: 0.9281 - val_loss: 0.7677 - val_binary_accuracy: 0.7714\n",
      "Epoch 59/80\n",
      "35/35 [==============================] - 1s 29ms/step - loss: 0.1690 - binary_accuracy: 0.9353 - val_loss: 0.7940 - val_binary_accuracy: 0.7143\n",
      "Epoch 60/80\n",
      "35/35 [==============================] - 1s 32ms/step - loss: 0.1251 - binary_accuracy: 0.9281 - val_loss: 0.7311 - val_binary_accuracy: 0.7429\n",
      "Epoch 61/80\n",
      "35/35 [==============================] - 1s 29ms/step - loss: 0.1198 - binary_accuracy: 0.9568 - val_loss: 0.8120 - val_binary_accuracy: 0.7429\n",
      "Epoch 62/80\n",
      "35/35 [==============================] - 1s 29ms/step - loss: 0.1482 - binary_accuracy: 0.9496 - val_loss: 0.6927 - val_binary_accuracy: 0.7143\n",
      "Epoch 63/80\n",
      "35/35 [==============================] - 1s 29ms/step - loss: 0.0899 - binary_accuracy: 0.9640 - val_loss: 0.7806 - val_binary_accuracy: 0.7429\n",
      "Epoch 64/80\n",
      "35/35 [==============================] - 1s 29ms/step - loss: 0.1404 - binary_accuracy: 0.9353 - val_loss: 0.8505 - val_binary_accuracy: 0.6857\n",
      "Epoch 65/80\n",
      "35/35 [==============================] - 1s 32ms/step - loss: 0.1706 - binary_accuracy: 0.9137 - val_loss: 0.7456 - val_binary_accuracy: 0.8000\n",
      "Epoch 66/80\n",
      "35/35 [==============================] - 1s 30ms/step - loss: 0.1437 - binary_accuracy: 0.9424 - val_loss: 0.8162 - val_binary_accuracy: 0.7143\n",
      "Epoch 67/80\n",
      "35/35 [==============================] - 1s 29ms/step - loss: 0.1431 - binary_accuracy: 0.9424 - val_loss: 0.8383 - val_binary_accuracy: 0.7714\n",
      "Epoch 68/80\n",
      "35/35 [==============================] - 1s 30ms/step - loss: 0.0640 - binary_accuracy: 0.9712 - val_loss: 0.9444 - val_binary_accuracy: 0.7143\n",
      "Epoch 69/80\n",
      "35/35 [==============================] - 1s 28ms/step - loss: 0.1455 - binary_accuracy: 0.9424 - val_loss: 0.9328 - val_binary_accuracy: 0.7143\n",
      "Epoch 70/80\n",
      "35/35 [==============================] - 1s 29ms/step - loss: 0.0950 - binary_accuracy: 0.9712 - val_loss: 1.0508 - val_binary_accuracy: 0.7429\n",
      "Epoch 71/80\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.1863 - binary_accuracy: 0.9424"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[324], line 67\u001b[0m\n\u001b[1;32m     62\u001b[0m x_train, x_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(data_filter_year,to_categorical(binary_labels_pdp), test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m,random_state\u001b[38;5;241m=\u001b[39mrand_state)\n\u001b[1;32m     65\u001b[0m mc \u001b[38;5;241m=\u001b[39m ModelCheckpoint(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model.h5\u001b[39m\u001b[38;5;124m'\u001b[39m, monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,start_from_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m---> 67\u001b[0m history\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m80\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mmc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m saved_model \u001b[38;5;241m=\u001b[39m load_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     73\u001b[0m prob_csv\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39masarray(saved_model\u001b[38;5;241m.\u001b[39mpredict(test_year_data)))\n",
      "File \u001b[0;32m~/.miniconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.miniconda3/lib/python3.9/site-packages/keras/engine/training.py:1729\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1714\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_eval_data_handler\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1715\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_data_handler \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39mget_data_handler(\n\u001b[1;32m   1716\u001b[0m         x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[1;32m   1717\u001b[0m         y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1727\u001b[0m         steps_per_execution\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution,\n\u001b[1;32m   1728\u001b[0m     )\n\u001b[0;32m-> 1729\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1731\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1732\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1734\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_use_cached_eval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1742\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   1744\u001b[0m }\n\u001b[1;32m   1745\u001b[0m epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[0;32m~/.miniconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.miniconda3/lib/python3.9/site-packages/keras/engine/training.py:2072\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   2068\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   2069\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m, step_num\u001b[38;5;241m=\u001b[39mstep, _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2070\u001b[0m ):\n\u001b[1;32m   2071\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_begin(step)\n\u001b[0;32m-> 2072\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2073\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   2074\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.miniconda3/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.miniconda3/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    891\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 894\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    896\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    897\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.miniconda3/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:933\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    931\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[1;32m    935\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    936\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.miniconda3/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    141\u001b[0m   (concrete_function,\n\u001b[1;32m    142\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.miniconda3/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1753\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1755\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1756\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1757\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1758\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1759\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1760\u001b[0m     args,\n\u001b[1;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1762\u001b[0m     executing_eagerly)\n\u001b[1;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.miniconda3/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    380\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 381\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    389\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    390\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    394\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/.miniconda3/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Extract last year from dataset to predict on it\n",
    "\n",
    "\n",
    "data_filter_year = np.delete(data_matrix_conv, np.arange(0, len(binary_labels_pdp), 23),axis=0)\n",
    "test_year_data=data_matrix_conv[np.arange(0,len(binary_labels_pdp),23)]\n",
    "# data_test =  data_matrix_conv[test_year_data] \n",
    "print(test_year_data.shape)\n",
    "\n",
    "nb_regions=8\n",
    "portions=[1,0.8,0.7,0.6,0.5]\n",
    "prob_csv=[]\n",
    "                  \n",
    "drop_out_rate=0.1\n",
    "outlayer_neurons=128\n",
    "optimizers_list=['RMSprop']\n",
    "rand_state=np.random.randint(50)                  \n",
    "for portion in portions:\n",
    "    \n",
    "        \n",
    "                #Construction et définition du réseau de neurones \n",
    "        opt = tf.keras.optimizers.experimental.RMSprop(learning_rate=0.001)\n",
    "        years_data=np.arange(2000,2022)\n",
    "        binary_labels_pdp=gen_labels2(years_data,production,region_choice_match_production,portion)    \n",
    "            \n",
    "        # First Model:    \n",
    "        model = Sequential([\n",
    "\n",
    "              layers.Conv1D(256, 14, padding='same', activation='relu', input_shape=(int(min(year_size_data)/24),nb_features)),\n",
    "              layers.MaxPooling1D(pool_size=8),\n",
    "              layers.Dropout(drop_out_rate),\n",
    "              layers.Conv1D(128, 4, padding='same', activation='relu'),\n",
    "              layers.Dropout(drop_out_rate),\n",
    "              layers.Conv1D(64, 2, padding='same', activation='relu'),\n",
    "              layers.Dropout(drop_out_rate),\n",
    "              layers.Flatten(),\n",
    "              layers.Dense(outlayer_neurons, activation='relu'),\n",
    "              layers.Dense(2,activation='softmax')\n",
    "            ])\n",
    "\n",
    "        \n",
    "#         model = Sequential([\n",
    "\n",
    "#               layers.Conv1D(14, 14, padding='same', activation='relu', input_shape=(int(min(year_size_data)/24),nb_features)),\n",
    "#               layers.MaxPooling1D(pool_size=1),\n",
    "#               layers.Dropout(drop_out_rate),\n",
    "#               layers.Conv1D(6, 4, padding='same', activation='relu'),\n",
    "#               layers.Dropout(drop_out_rate),\n",
    "#               layers.Conv1D(4, 2, padding='same', activation='relu'),\n",
    "#               layers.Dropout(drop_out_rate),\n",
    "#               layers.Flatten(),\n",
    "#               layers.Dense(outlayer_neurons, activation='relu'),\n",
    "#               layers.Dense(2,activation='softmax')\n",
    "#             ])\n",
    "        \n",
    "        \n",
    "        #Compilation du modèle\n",
    "\n",
    "        model.compile(optimizer=opt,loss='binary_crossentropy',metrics='binary_accuracy' )\n",
    "\n",
    "\n",
    "        #Séparation des données test et entraînement\n",
    "        x_train, x_test, y_train, y_test = train_test_split(data_filter_year,to_categorical(binary_labels_pdp), test_size=0.01,random_state=rand_state)\n",
    "\n",
    "       \n",
    "        mc = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=0,save_best_only=True,start_from_epoch=10)\n",
    "\n",
    "        history=model.fit(x_train, y_train,\n",
    "                            epochs=80,\n",
    "                            batch_size=4,verbose=1,class_weight=class_weight,validation_split=0.2,callbacks=[mc])\n",
    "    \n",
    "        saved_model = load_model('best_model.h5')\n",
    "         \n",
    "        prob_csv.append(np.asarray(saved_model.predict(test_year_data)))\n",
    "        print('Portion probabilities :',np.asarray(saved_model.predict(test_year_data))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f3b0a0-9775-47dc-a85f-82351ccbb2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_features=['u10', 'v10', 't2m', 'evabs', 'evatc', 'evavt', 'lai_hv', 'lai_lv', 'src', 'stl1', 'sp', 'e', 'tp', 'swvl1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c4e895-75f2-41c5-8c74-4db0fe7c8d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "58/58 [==============================] - 19s 189ms/step - loss: 0.7748 - accuracy: 0.5217 - val_loss: 0.6996 - val_accuracy: 0.6000\n",
      "Epoch 2/30\n",
      "58/58 [==============================] - 6s 98ms/step - loss: 0.7241 - accuracy: 0.4609 - val_loss: 0.6941 - val_accuracy: 0.4000\n",
      "Epoch 3/30\n",
      "58/58 [==============================] - 9s 149ms/step - loss: 0.7049 - accuracy: 0.5217 - val_loss: 0.6963 - val_accuracy: 0.4000\n",
      "Epoch 4/30\n",
      "58/58 [==============================] - 9s 137ms/step - loss: 0.6996 - accuracy: 0.5217 - val_loss: 0.7296 - val_accuracy: 0.4000\n",
      "Epoch 5/30\n",
      "58/58 [==============================] - 7s 118ms/step - loss: 0.6980 - accuracy: 0.5043 - val_loss: 0.6962 - val_accuracy: 0.4000\n",
      "Epoch 6/30\n",
      "58/58 [==============================] - 7s 108ms/step - loss: 0.6985 - accuracy: 0.5739 - val_loss: 0.7292 - val_accuracy: 0.4000\n",
      "Epoch 7/30\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.7123 - accuracy: 0.5391"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # data_occurences_in_new_period\n",
    "\n",
    "# drop_out_rate=0.1\n",
    "# outlayer_neurons=128\n",
    "\n",
    "# # callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=4,restore_best_weights=True)\n",
    "\n",
    "\n",
    "# optimizers_list=['RMSprop']\n",
    "\n",
    "\n",
    "# f1=[]\n",
    "# # rand_state=np.random.randint(50)\n",
    "# for i in range(5):\n",
    "\n",
    "#     rand_state=np.random.randint(50)\n",
    "#             #Construction et définition du réseau de neurones \n",
    "\n",
    "#     model = Sequential([\n",
    "\n",
    "#       layers.Conv1D(256, 6, padding='same', activation='relu', input_shape=(int(min(year_size_data)/24),len(feature_choice))),\n",
    "#       # layers.AveragePooling1D(pool_size=8),\n",
    "#       layers.MaxPooling1D(pool_size=8),\n",
    "#       layers.Dropout(drop_out_rate),\n",
    "#       layers.Conv1D(128, 4, padding='same', activation='relu'),\n",
    "#       # layers.MaxPooling1D(pool_size=6),\n",
    "#       # layers.AveragePooling1D(pool_size=6),\n",
    "#       layers.Dropout(drop_out_rate),\n",
    "#       layers.Conv1D(64, 2, padding='same', activation='relu'),\n",
    "\n",
    "#       # layers.AveragePooling1D(),\n",
    "#       layers.Dropout(drop_out_rate),\n",
    "#       layers.Flatten(),\n",
    "#       layers.Dense(outlayer_neurons, activation='relu'),\n",
    "#       layers.Dense(2,activation='softmax')\n",
    "#         ])\n",
    "\n",
    "\n",
    "\n",
    "#     #Compilation du modèle\n",
    "\n",
    "\n",
    "#     model.compile(optimizer='RMSprop',loss='binary_crossentropy',metrics='accuracy' )\n",
    "\n",
    "\n",
    "#     #Séparation des données test et entraînement\n",
    "#     x_train, x_test, y_train, y_test = train_test_split(data_matrix_conv_fvar,to_categorical(binary_labels_pdp), test_size=0.1,random_state=rand_state)\n",
    "\n",
    "#     # callback = tf.keras.callbacks.EarlyStopping(monitor='loss',patience=10,restore_best_weights=True,start_from_epoch=15)\n",
    "\n",
    "\n",
    "#     # history=model.fit(x_train, y_train,\n",
    "#     #                     epochs=120,\n",
    "#     #                     batch_size=16,verbose=1,class_weight=class_weight,validation_split=0.3)\n",
    "\n",
    "#     history=model.fit(x_train, y_train,\n",
    "#                         epochs=30,\n",
    "#                         batch_size=2,verbose=1,class_weight=class_weight,validation_split=0.3)\n",
    " \n",
    "#     y_true=[]\n",
    "#     pred_labels=[]\n",
    "#     prob_class=model.predict(x_test)\n",
    "\n",
    "\n",
    "#     for prob in prob_class:\n",
    "\n",
    "#         pred_labels.append(np.argmax(prob))\n",
    "\n",
    "#     for example in range(len(y_test)):\n",
    "#         y_true.append(np.argmax(y_test[example]))\n",
    "\n",
    "#     f1.append(f1_score(y_true,pred_labels))\n",
    "# print('Optimiseur :', opti)\n",
    "# print('Score de classification')\n",
    "print(np.round(np.mean(f1)*100,2),'+-',np.round(np.std(f1)*100,2),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39ac9ba-330f-4c3c-ba95-14dc35614a7d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### PYTS time series classification and time window importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fd2aaa-8d04-42c0-8849-eeaa69af7392",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[187], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m feature_test\u001b[38;5;241m=\u001b[39m\u001b[43mx_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnb_years\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnb_examples\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43myear_size_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      5\u001b[0m clf \u001b[38;5;241m=\u001b[39m TimeSeriesForest(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m43\u001b[39m)\n\u001b[1;32m      6\u001b[0m clf\u001b[38;5;241m.\u001b[39mfit(feature_test, y_train)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# feature_test=x_train.reshape((nb_years,nb_examples*0.9,min(year_size_data)))[-1]\n",
    "\n",
    "\n",
    "# clf = TimeSeriesForest(random_state=43)\n",
    "# clf.fit(feature_test, y_train)\n",
    "\n",
    "# start_idxmax, end_idxmax = clf.indices_[\n",
    "#     np.argmax(clf.feature_importances_) // 3]\n",
    "\n",
    "# plt.figure(figsize=(12, 5))\n",
    "# plt.plot(feature_test[0], label='First sample in class 1')\n",
    "# plt.plot(np.arange(start_idxmax, end_idxmax),\n",
    "#          feature_test[0,start_idxmax:end_idxmax],\n",
    "#          color='C0', lw=4)\n",
    "\n",
    "# plt.plot(feature_test[-1], label='First sample in class 0')\n",
    "# plt.plot(np.arange(start_idxmax, end_idxmax),\n",
    "#          feature_test[-1, start_idxmax:end_idxmax],\n",
    "#          color='C1', lw=4)\n",
    "\n",
    "# plt.legend(loc='best', fontsize=14)\n",
    "# plt.title('The most important window according to the feature importance '\n",
    "#           'scores', fontsize=16)\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa625a91-d0de-4b14-abbf-30ecd609bc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.3636363636363636, 'u10'], [0.4615384615384615, 'v10'], [0.20689655172413793, 't2m'], [0.4827586206896552, 'evabs'], [0.3846153846153846, 'evatc'], [0.20689655172413796, 'evavt'], [0.3703703703703704, 'lai_hv'], [0.2608695652173913, 'lai_lv'], [0.23076923076923075, 'src'], [0.24, 'stl1'], [0.29629629629629634, 'sp'], [0.4166666666666667, 'e'], [0.4666666666666667, 'tp'], [0.17391304347826086, 'swvl1']]\n"
     ]
    }
   ],
   "source": [
    "# import pyts\n",
    "# from pyts.classification import BOSSVS\n",
    "# from pyts.multivariate.classification import MultivariateClassifier\n",
    "# from pyts.classification import TimeSeriesForest\n",
    "# features_scores=[]\n",
    "# for i in range(len(data_features)):\n",
    "    \n",
    "#     x_train, x_test, y_train, y_test = train_test_split(data_matrix_conv,binary_labels_pdp, test_size=0.3, random_state=np.random.randint(100))\n",
    "#     feature_train=x_train.reshape((14,len(y_train),365))[i]\n",
    "#     feature_test=x_test.reshape((14,len(y_test),365))[i]\n",
    "#     f1=[]\n",
    "#     for exp in range(1):\n",
    "        \n",
    "\n",
    "\n",
    "#         clf = TimeSeriesForest(random_state=np.random.randint(100))\n",
    "#         clf.fit(feature_train, y_train)\n",
    "        \n",
    "#         prediction= clf.predict(feature_test)\n",
    "\n",
    "#         #Evaluation de la précision de classification du modèle\n",
    "#         f1.append(f1_score(y_test,prediction))\n",
    "#     features_scores.append([np.mean(f1),data_features[i]])\n",
    "# print(features_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f67931-8c8d-4322-9e0e-59d9d76b0d4e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### BOSSVS Multivariate time series classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d4ee3f-618b-4da2-b5e7-98c234678b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score moyen de prédiction de classes pour  184  examples de résultats de production\n",
      "0.4864864864864865 +- 0.0\n"
     ]
    }
   ],
   "source": [
    "# import pyts\n",
    "# from pyts.classification import BOSSVS\n",
    "# from pyts.multivariate.classification import MultivariateClassifier\n",
    "# from pyts.classification import TimeSeriesForest\n",
    "\n",
    "# f1=[]\n",
    "# for i in range(1):\n",
    "    \n",
    "#     x_train, x_test, y_train, y_test = train_test_split(data_matrix_conv,binary_labels_pdp, test_size=0.5, random_state=np.random.randint(100))\n",
    "#     clf = MultivariateClassifier(BOSSVS(n_bins=2,anova=False,word_size=2,drop_sum=False))\n",
    "    \n",
    "#     clf.fit(x_train, y_train)\n",
    "#     MultivariateClassifier(...)\n",
    "#     clf.predict(x_test)\n",
    "\n",
    "#     clf.fit(x_train, y_train)\n",
    "\n",
    "#     #Prédiction sur les données de test avec le modèle entraîné \n",
    "\n",
    "#     prediction= clf.predict(x_test)\n",
    "\n",
    "#     #Evaluation de la précision de classification du modèle\n",
    "#     f1.append(f1_score(y_test,prediction))\n",
    "# print('Score moyen de prédiction de classes pour ', nb_examples,' examples de résultats de production')\n",
    "\n",
    "# print(np.mean(f1),'+-',np.std(f1))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
